{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 4: Sensor Technologies & Fusion\n\n## Module II: Perception & Localization\n\n### Topics Covered\n\n- Camera (Image Processing, Homography)\n- LiDAR (Point Clouds, Range Data)\n- Radar (Doppler Effects)\n- Sensor Synchronization and Calibration\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. Understand the physics and characteristics of automotive sensors (Camera, LiDAR, Radar)\n2. Process camera images and apply homography transformations\n3. Work with LiDAR point clouds and range data\n4. Understand radar principles and Doppler effect\n5. Perform sensor calibration and extrinsic/intrinsic parameter estimation\n6. Implement multi-sensor fusion techniques\n7. Synchronize data from multiple sensors with different frequencies\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nImport required libraries for sensor data processing and visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, Rectangle, Wedge\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.spatial.transform import Rotation\n\n# Set random seed\nnp.random.seed(42)\n\n# Plotting configuration\nplt.rcParams['figure.figsize'] = (14, 8)\nplt.rcParams['font.size'] = 10\n\nprint(\"Libraries loaded successfully!\")\nprint(\"NumPy version:\", np.__version__)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Camera Sensors\n\n**Cameras** are the most cost-effective sensors for autonomous vehicles, providing rich visual information about the environment including colors, textures, lane markings, traffic signs, and lights.\n\n### Camera Basics\n\n#### **Pinhole Camera Model**\n\nThe fundamental model relating 3D world points to 2D image points:\n\n$$\\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = \\frac{1}{Z} \\mathbf{K} \\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}$$\n\nWhere:\n- **(X, Y, Z)**: 3D point in camera frame\n- **(u, v)**: 2D pixel coordinates\n- **K**: Intrinsic camera matrix\n- **Z**: Depth (distance from camera)\n\n#### **Intrinsic Parameters (K Matrix)**\n\n$$\\mathbf{K} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n\n- **fx, fy**: Focal lengths (in pixels)\n- **cx, cy**: Principal point (optical center)\n\n**Typical values** for automotive cameras:\n- Focal length: 700-1200 pixels\n- Resolution: 1920×1080 (Full HD) or 1280×720 (HD)\n- Field of View (FoV): 50-120 degrees\n- Frame rate: 30-60 FPS\n\n---\n\n### Camera Types in Autonomous Vehicles\n\n| Camera Type | FoV | Use Case | Position |\n|-------------|-----|----------|----------|\n| **Front Wide** | 120° | Lane keeping, traffic lights | Windshield |\n| **Front Narrow** | 50° | Long-range detection (200m+) | Windshield |\n| **Side** | 90° | Lane changes, blind spots | Side mirrors |\n| **Rear** | 120° | Parking, rear monitoring | Rear bumper |\n| **Fisheye** | 180°+ | 360° surround view | Roof/corners |\n\n---\n\n### Image Processing Pipeline\n\n```\nRaw Image → Undistortion → Color Space → Feature Extraction → Object Detection\n                           Conversion\n```\n\n#### **1. Lens Distortion Correction**\n\nReal lenses have radial and tangential distortion. Undistortion formula:\n\n$$\\begin{align}\nx_{distorted} &= x(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) \\\\\ny_{distorted} &= y(1 + k_1 r^2 + k_2 r^4 + k_3 r^6)\n\\end{align}$$\n\nWhere **r² = x² + y²** and **k₁, k₂, k₃** are radial distortion coefficients.\n\n#### **2. Color Spaces**\n- **RGB**: Raw camera output\n- **HSV**: Better for color-based segmentation (e.g., lane lines)\n- **Grayscale**: Reduces data, used for edge detection\n\n#### **3. Edge Detection**\n- **Canny Edge Detector**: Multi-stage algorithm for edge detection\n- **Sobel Filter**: Gradient-based edge detection\n\n---\n\n### Homography: Ground Plane Projection\n\n**Homography** transforms a planar surface from one view to another. Critical for **bird's-eye view** (BEV) transformation.\n\n#### **Homography Matrix (3×3)**\n\n$$\\mathbf{H} = \\begin{bmatrix} h_{11} & h_{12} & h_{13} \\\\ h_{21} & h_{22} & h_{23} \\\\ h_{31} & h_{32} & h_{33} \\end{bmatrix}$$\n\nMaps image point **(u, v)** to ground plane point **(x, y)**:\n\n$$\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\sim \\mathbf{H} \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}$$\n\n#### **Computing Homography**\n\nGiven 4+ corresponding points between image and ground plane, solve for H using:\n- **Direct Linear Transform (DLT)**\n- **RANSAC** for robustness to outliers\n\n**Application**: Convert front camera view → top-down view for lane detection\n\n---\n\n### Advantages & Limitations\n\n**Advantages**:\n- ✅ Low cost ($50-200 per camera)\n- ✅ Rich semantic information (colors, textures, text)\n- ✅ High resolution (megapixels)\n- ✅ Passive sensor (no emissions)\n\n**Limitations**:\n- ❌ No direct depth information (monocular)\n- ❌ Performance degrades in poor lighting, rain, fog\n- ❌ Sensitive to sun glare\n- ❌ Requires heavy computation for deep learning\n\n**Solutions**:\n- Stereo cameras for depth\n- Multi-sensor fusion (camera + LiDAR)\n- HDR imaging for varying light conditions"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Camera Homography Demonstration\n\nclass CameraModel:\n    \"\"\"Simple pinhole camera model.\"\"\"\n    \n    def __init__(self, fx=800, fy=800, cx=640, cy=360, width=1280, height=720):\n        \"\"\"\n        Args:\n            fx, fy: Focal lengths in pixels\n            cx, cy: Principal point (optical center)\n            width, height: Image resolution\n        \"\"\"\n        self.K = np.array([\n            [fx, 0, cx],\n            [0, fy, cy],\n            [0, 0, 1]\n        ])\n        self.width = width\n        self.height = height\n        \n    def project_3d_to_2d(self, points_3d):\n        \"\"\"\n        Project 3D points (in camera frame) to 2D image coordinates.\n        \n        Args:\n            points_3d: Nx3 array of 3D points [X, Y, Z]\n        \n        Returns:\n            points_2d: Nx2 array of pixel coordinates [u, v]\n        \"\"\"\n        # Homogeneous coordinates\n        points_2d_hom = (self.K @ points_3d.T).T\n        \n        # Normalize by Z (depth)\n        points_2d = points_2d_hom[:, :2] / points_2d_hom[:, 2:3]\n        \n        return points_2d\n\n\n# Simulate camera viewing a road scene\ncamera = CameraModel()\n\n# Define 3D points on ground plane (Z=0, camera at height 1.5m)\n# Road lanes in front of vehicle\ncamera_height = 1.5  # meters above ground\n\n# Create grid of points on ground plane\nx_range = np.linspace(0, 50, 20)  # 0-50m in front\ny_range = np.linspace(-10, 10, 10)  # ±10m width\n\nground_points = []\nfor x in x_range:\n    for y in y_range:\n        # Points in camera frame: X=forward, Y=left, Z=up\n        # Ground plane: Z = -camera_height (below camera)\n        ground_points.append([x, y, -camera_height])\n\nground_points = np.array(ground_points)\n\n# Only keep points with positive depth (in front of camera)\nvalid_mask = ground_points[:, 0] > 0\nground_points_valid = ground_points[valid_mask]\n\n# Project to image\nimage_points = camera.project_3d_to_2d(ground_points_valid)\n\n# Filter points within image bounds\nin_image = (image_points[:, 0] >= 0) & (image_points[:, 0] < camera.width) & \\\n           (image_points[:, 1] >= 0) & (image_points[:, 1] < camera.height)\n\nimage_points_valid = image_points[in_image]\nground_points_final = ground_points_valid[in_image]\n\n# Visualization\nfig = plt.figure(figsize=(16, 7))\n\n# Left: Front camera view\nax1 = fig.add_subplot(1, 2, 1)\nax1.scatter(image_points_valid[:, 0], image_points_valid[:, 1], c=ground_points_final[:, 0], \n           cmap='viridis', s=50, alpha=0.6)\nax1.set_xlim([0, camera.width])\nax1.set_ylim([camera.height, 0])  # Flip Y for image coordinates\nax1.set_xlabel('u (pixels)', fontweight='bold')\nax1.set_ylabel('v (pixels)', fontweight='bold')\nax1.set_title('Front Camera View (Perspective)', fontweight='bold', fontsize=14)\nax1.set_aspect('equal')\nax1.grid(True, alpha=0.3)\n\n# Add lane markings\nlane_left_3d = np.array([[i, 3.5, -camera_height] for i in range(5, 50, 2)])\nlane_right_3d = np.array([[i, -3.5, -camera_height] for i in range(5, 50, 2)])\n\nlane_left_2d = camera.project_3d_to_2d(lane_left_3d)\nlane_right_2d = camera.project_3d_to_2d(lane_right_3d)\n\nax1.plot(lane_left_2d[:, 0], lane_left_2d[:, 1], 'y-', linewidth=3, label='Left Lane')\nax1.plot(lane_right_2d[:, 0], lane_right_2d[:, 1], 'y-', linewidth=3, label='Right Lane')\nax1.legend()\n\n# Right: Bird's Eye View (top-down)\nax2 = fig.add_subplot(1, 2, 2)\nax2.scatter(ground_points_final[:, 0], ground_points_final[:, 1], \n           c=ground_points_final[:, 0], cmap='viridis', s=50, alpha=0.6)\nax2.set_xlabel('X (m) - Forward', fontweight='bold')\nax2.set_ylabel('Y (m) - Left', fontweight='bold')\nax2.set_title('Bird\\'s Eye View (Top-Down)', fontweight='bold', fontsize=14)\nax2.set_aspect('equal')\nax2.grid(True, alpha=0.3)\n\n# Add lane markings\nax2.plot(lane_left_3d[:, 0], lane_left_3d[:, 1], 'y-', linewidth=3, label='Left Lane')\nax2.plot(lane_right_3d[:, 0], lane_right_3d[:, 1], 'y-', linewidth=3, label='Right Lane')\n\n# Add vehicle position\nax2.plot(0, 0, 'ro', markersize=15, label='Vehicle')\nvehicle_rect = Rectangle((-2, -1), 4, 2, fill=False, edgecolor='red', linewidth=2)\nax2.add_patch(vehicle_rect)\n\nax2.legend()\nax2.set_xlim([-5, 50])\nax2.set_ylim([-15, 15])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"=\" * 70)\nprint(\"Camera Parameters:\")\nprint(\"=\" * 70)\nprint(f\"Intrinsic Matrix K:\\n{camera.K}\\n\")\nprint(f\"Focal lengths: fx={camera.K[0,0]:.1f} px, fy={camera.K[1,1]:.1f} px\")\nprint(f\"Principal point: cx={camera.K[0,2]:.1f} px, cy={camera.K[1,2]:.1f} px\")\nprint(f\"Resolution: {camera.width}×{camera.height}\")\nprint(f\"Camera height: {camera_height} m\")\nprint(\"=\" * 70)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. LiDAR Sensors\n\n**LiDAR (Light Detection and Ranging)** uses laser pulses to measure distances and create precise 3D maps of the environment. It's the cornerstone of most autonomous vehicle sensor suites.\n\n### LiDAR Basics\n\n#### **Operating Principle**\n\n1. **Emit**: Laser pulse sent out\n2. **Reflect**: Pulse bounces off object\n3. **Receive**: Sensor detects reflected pulse\n4. **Calculate**: Distance = (Time of Flight × Speed of Light) / 2\n\n$$d = \\frac{c \\cdot \\Delta t}{2}$$\n\nWhere:\n- **d**: Distance to object\n- **c**: Speed of light (≈ 3 × 10⁸ m/s)\n- **Δt**: Time of flight (round trip)\n\n**Example**: For Δt = 200 nanoseconds:\n- d = (3 × 10⁸ m/s × 200 × 10⁻⁹ s) / 2 = 30 meters\n\n---\n\n### LiDAR Types\n\n| Type | Mechanism | Range | Points/sec | Cost | Use Case |\n|------|-----------|-------|------------|------|----------|\n| **Mechanical** | Rotating mirror/head | 100-200m | 1-2M | $$$$$ | Long-range, 360° |\n| **Solid-State** | MEMS mirrors, no rotation | 50-100m | 100k-500k | $$$ | Compact, reliable |\n| **Flash** | Illuminates entire scene | 30-50m | 10k-100k | $$ | Short-range |\n\n**Common Automotive LiDAR**:\n- **Velodyne HDL-64E**: 64 channels, 360°, 2.2M points/sec, ~200m range\n- **Ouster OS1**: 64/128 channels, 360°, 1.3M points/sec\n- **Luminar Iris**: 1550nm wavelength, 250m+ range\n\n---\n\n### Point Cloud Representation\n\n**Point Cloud**: Collection of 3D points, each with (x, y, z, intensity)\n\n```\nPoint = [x, y, z, intensity, timestamp, laser_id]\n```\n\n- **x, y, z**: 3D coordinates in LiDAR frame\n- **intensity**: Reflectivity (0-255)\n- **timestamp**: When point was captured\n- **laser_id**: Which laser channel (for multi-channel LiDAR)\n\n**Typical point cloud**:\n- **Size**: 100,000 - 2,000,000 points per scan\n- **Frequency**: 10-20 Hz\n- **Data rate**: ~10-40 MB/s\n\n---\n\n### Coordinate Systems\n\n**LiDAR Frame** (standard convention):\n- **X**: Forward\n- **Y**: Left\n- **Z**: Up\n\n**Spherical Coordinates** → Cartesian:\n\n$$\\begin{align}\nx &= r \\cos(\\theta) \\cos(\\phi) \\\\\ny &= r \\cos(\\theta) \\sin(\\phi) \\\\\nz &= r \\sin(\\theta)\n\\end{align}$$\n\nWhere:\n- **r**: Range (distance)\n- **θ**: Elevation angle (vertical)\n- **φ**: Azimuth angle (horizontal)\n\n---\n\n### Point Cloud Processing Pipeline\n\n```\nRaw Points → Ground Removal → Clustering → Object Detection → Tracking\n```\n\n#### **1. Ground Plane Removal**\n- **RANSAC**: Fit plane to ground points\n- **Height threshold**: Remove points below z < -1.5m\n\n#### **2. Clustering**\n- **Euclidean clustering**: Group nearby points\n- **DBSCAN**: Density-based clustering\n\n#### **3. Bounding Box Fitting**\n- **Oriented bounding box (OBB)**: Fit box to cluster\n- **Extract**: Position, size, orientation\n\n---\n\n### Advantages & Limitations\n\n**Advantages**:\n- ✅ Accurate 3D measurements (±2cm precision)\n- ✅ Direct range/depth information\n- ✅ Works in darkness (active sensor)\n- ✅ 360° field of view (mechanical)\n- ✅ Not affected by texture/color\n\n**Limitations**:\n- ❌ Expensive ($1,000 - $75,000)\n- ❌ Performance degrades in rain, fog, snow\n- ❌ No color information\n- ❌ Lower resolution than cameras\n- ❌ Moving parts can fail (mechanical)\n- ❌ Difficulty detecting dark/absorbing surfaces\n\n**Solutions**:\n- Multi-wavelength LiDAR (905nm + 1550nm)\n- Sensor fusion with cameras\n- Solid-state for reliability",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# LiDAR Sensor Model and Point Cloud Processing\n\nclass LiDARSensor:\n    \"\"\"\n    Simulates a 3D LiDAR sensor with configurable resolution and range.\n    \"\"\"\n    def __init__(self, \n                 horizontal_resolution=0.2,  # degrees\n                 vertical_resolution=2.0,    # degrees\n                 horizontal_fov=360,         # degrees\n                 vertical_fov=(-25, 15),     # (min, max) in degrees\n                 max_range=100,              # meters\n                 min_range=1):               # meters\n        self.h_res = np.deg2rad(horizontal_resolution)\n        self.v_res = np.deg2rad(vertical_resolution)\n        self.h_fov = np.deg2rad(horizontal_fov)\n        self.v_fov = (np.deg2rad(vertical_fov[0]), np.deg2rad(vertical_fov[1]))\n        self.max_range = max_range\n        self.min_range = min_range\n        \n        # Generate scan pattern\n        self.azimuth_angles = np.arange(0, self.h_fov, self.h_res)\n        self.elevation_angles = np.arange(self.v_fov[0], self.v_fov[1], self.v_res)\n        \n    def spherical_to_cartesian(self, ranges, azimuth, elevation):\n        \"\"\"\n        Convert spherical coordinates (r, θ, φ) to Cartesian (x, y, z).\n        \n        Args:\n            ranges: Distance measurements (m)\n            azimuth: Horizontal angle φ (radians)\n            elevation: Vertical angle θ (radians)\n        \n        Returns:\n            Point cloud in Cartesian coordinates (N x 3)\n        \"\"\"\n        x = ranges * np.cos(elevation) * np.cos(azimuth)\n        y = ranges * np.cos(elevation) * np.sin(azimuth)\n        z = ranges * np.sin(elevation)\n        return np.column_stack([x, y, z])\n    \n    def scan_environment(self, objects):\n        \"\"\"\n        Simulate LiDAR scan of environment with objects.\n        \n        Args:\n            objects: List of (center, size, shape_type) tuples\n        \n        Returns:\n            Point cloud (N x 4): [x, y, z, intensity]\n        \"\"\"\n        points = []\n        intensities = []\n        \n        for elevation in self.elevation_angles:\n            for azimuth in self.azimuth_angles:\n                # Ray direction\n                ray_dir = np.array([\n                    np.cos(elevation) * np.cos(azimuth),\n                    np.cos(elevation) * np.sin(azimuth),\n                    np.sin(elevation)\n                ])\n                \n                # Find intersection with objects or ground\n                min_range = self.max_range\n                hit_intensity = 0.3  # Default ground intensity\n                \n                # Check ground plane (z = 0)\n                if ray_dir[2] < 0:\n                    t = -0.0 / ray_dir[2]  # sensor at z=0\n                    if self.min_range < t < min_range:\n                        min_range = t\n                        hit_intensity = 0.4\n                \n                # Check objects\n                for obj_center, obj_size, shape_type in objects:\n                    if shape_type == 'box':\n                        # Simplified box intersection\n                        t = np.linalg.norm(obj_center[:2])  # Distance to center\n                        if abs(np.arctan2(obj_center[1], obj_center[0]) - azimuth) < 0.1:\n                            if self.min_range < t < min_range:\n                                min_range = t\n                                hit_intensity = 0.8\n                    elif shape_type == 'sphere':\n                        # Sphere intersection\n                        oc = -obj_center\n                        b = np.dot(oc, ray_dir)\n                        c = np.dot(oc, oc) - obj_size**2\n                        discriminant = b**2 - c\n                        if discriminant > 0:\n                            t = b - np.sqrt(discriminant)\n                            if self.min_range < t < min_range:\n                                min_range = t\n                                hit_intensity = 0.9\n                \n                if min_range < self.max_range:\n                    point = min_range * ray_dir\n                    points.append(point)\n                    intensities.append(hit_intensity)\n        \n        points = np.array(points)\n        intensities = np.array(intensities).reshape(-1, 1)\n        return np.hstack([points, intensities])\n    \n    def remove_ground_plane(self, point_cloud, ground_threshold=-0.2):\n        \"\"\"\n        Remove ground points using height threshold.\n        \n        Args:\n            point_cloud: (N x 4) array [x, y, z, intensity]\n            ground_threshold: Maximum z-value for ground points\n        \n        Returns:\n            Point cloud with ground removed\n        \"\"\"\n        return point_cloud[point_cloud[:, 2] > ground_threshold]\n    \n    def cluster_points(self, point_cloud, distance_threshold=0.5):\n        \"\"\"\n        Simple distance-based clustering of points.\n        \n        Args:\n            point_cloud: (N x 4) array [x, y, z, intensity]\n            distance_threshold: Maximum distance for same cluster\n        \n        Returns:\n            Cluster labels (N,)\n        \"\"\"\n        from sklearn.cluster import DBSCAN\n        clustering = DBSCAN(eps=distance_threshold, min_samples=5)\n        labels = clustering.fit_predict(point_cloud[:, :3])\n        return labels\n\n# Create LiDAR sensor\nlidar = LiDARSensor(\n    horizontal_resolution=0.4,\n    vertical_resolution=2.0,\n    horizontal_fov=360,\n    max_range=50\n)\n\n# Define environment: (center, size, shape_type)\nobjects = [\n    (np.array([15, 0, 0.75]), 2.0, 'box'),      # Vehicle ahead\n    (np.array([20, -3, 0.75]), 2.0, 'box'),     # Vehicle left lane\n    (np.array([8, 2, 0.9]), 0.3, 'sphere'),     # Pedestrian right\n    (np.array([25, 5, 0.5]), 0.3, 'sphere'),    # Pedestrian far right\n]\n\n# Perform LiDAR scan\npoint_cloud = lidar.scan_environment(objects)\nprint(f\"Total points in scan: {len(point_cloud)}\")\n\n# Remove ground plane\nnon_ground_points = lidar.remove_ground_plane(point_cloud, ground_threshold=-0.1)\nprint(f\"Non-ground points: {len(non_ground_points)}\")\n\n# Cluster objects\nif len(non_ground_points) > 0:\n    cluster_labels = lidar.cluster_points(non_ground_points, distance_threshold=1.5)\n    num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n    print(f\"Detected clusters: {num_clusters}\")\n\n# Visualization\nfig = plt.figure(figsize=(16, 6))\n\n# 1. Full point cloud (top view)\nax1 = fig.add_subplot(131)\nscatter1 = ax1.scatter(point_cloud[:, 0], point_cloud[:, 1], \n                       c=point_cloud[:, 3], cmap='viridis', s=1, alpha=0.6)\nax1.scatter(0, 0, c='red', s=200, marker='^', label='LiDAR sensor', edgecolors='black', linewidths=2)\nax1.set_xlabel('X (m)', fontsize=11)\nax1.set_ylabel('Y (m)', fontsize=11)\nax1.set_title('LiDAR Point Cloud - Top View', fontsize=13, fontweight='bold')\nax1.set_aspect('equal')\nax1.grid(True, alpha=0.3)\nax1.legend()\ncbar1 = plt.colorbar(scatter1, ax=ax1)\ncbar1.set_label('Intensity', fontsize=10)\nax1.set_xlim(-10, 40)\nax1.set_ylim(-20, 20)\n\n# 2. 3D point cloud\nax2 = fig.add_subplot(132, projection='3d')\nscatter2 = ax2.scatter(point_cloud[:, 0], point_cloud[:, 1], point_cloud[:, 2],\n                       c=point_cloud[:, 2], cmap='terrain', s=1, alpha=0.5)\nax2.scatter(0, 0, 0, c='red', s=200, marker='^', label='LiDAR sensor')\nax2.set_xlabel('X (m)', fontsize=10)\nax2.set_ylabel('Y (m)', fontsize=10)\nax2.set_zlabel('Z (m)', fontsize=10)\nax2.set_title('LiDAR Point Cloud - 3D View', fontsize=13, fontweight='bold')\nax2.legend()\ncbar2 = plt.colorbar(scatter2, ax=ax2, pad=0.1, shrink=0.8)\ncbar2.set_label('Height (m)', fontsize=10)\nax2.view_init(elev=25, azim=45)\n\n# 3. Clustered objects (ground removed)\nax3 = fig.add_subplot(133)\nif len(non_ground_points) > 0:\n    scatter3 = ax3.scatter(non_ground_points[:, 0], non_ground_points[:, 1],\n                          c=cluster_labels, cmap='tab10', s=3, alpha=0.7)\n    ax3.scatter(0, 0, c='red', s=200, marker='^', label='LiDAR sensor', edgecolors='black', linewidths=2)\n    \n    # Draw bounding boxes for each cluster\n    for cluster_id in set(cluster_labels):\n        if cluster_id == -1:  # Skip noise\n            continue\n        cluster_points = non_ground_points[cluster_labels == cluster_id]\n        x_min, x_max = cluster_points[:, 0].min(), cluster_points[:, 0].max()\n        y_min, y_max = cluster_points[:, 1].min(), cluster_points[:, 1].max()\n        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n                            fill=False, edgecolor='red', linewidth=2)\n        ax3.add_patch(rect)\n    \nax3.set_xlabel('X (m)', fontsize=11)\nax3.set_ylabel('Y (m)', fontsize=11)\nax3.set_title('Object Detection (Ground Removed)', fontsize=13, fontweight='bold')\nax3.set_aspect('equal')\nax3.grid(True, alpha=0.3)\nax3.legend()\nax3.set_xlim(-5, 35)\nax3.set_ylim(-10, 10)\n\nplt.tight_layout()\nplt.show()\n\n# Print cluster statistics\nprint(\"\\n=== Detected Objects ===\")\nfor cluster_id in set(cluster_labels):\n    if cluster_id == -1:\n        continue\n    cluster_points = non_ground_points[cluster_labels == cluster_id]\n    center = cluster_points[:, :3].mean(axis=0)\n    size = np.max(np.ptp(cluster_points[:, :3], axis=0))\n    distance = np.linalg.norm(center[:2])\n    print(f\"Cluster {cluster_id}: Center=({center[0]:.1f}, {center[1]:.1f}, {center[2]:.1f})m, \"\n          f\"Size={size:.1f}m, Distance={distance:.1f}m, Points={len(cluster_points)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Radar Sensors\n\n**Radar (Radio Detection and Ranging)** uses radio waves to detect objects and measure their velocity. It's essential for adaptive cruise control (ACC) and collision avoidance systems.\n\n### Radar Basics\n\n#### **Operating Principle**\n\n1. **Transmit**: Radio wave (typically 24 GHz, 77 GHz, or 79 GHz)\n2. **Reflect**: Wave bounces off target\n3. **Receive**: Reflected wave with frequency shift\n4. **Measure**: Range and velocity simultaneously\n\n**Key advantage**: Direct velocity measurement via Doppler effect\n\n---\n\n### Frequency Bands for Automotive Radar\n\n| Band | Frequency | Wavelength | Range | Use Case |\n|------|-----------|------------|-------|----------|\n| **24 GHz** | 24.05-24.25 GHz | 12.5 mm | Short (0.2-30m) | Blind spot, parking |\n| **77 GHz** | 76-77 GHz | 3.9 mm | Long (1-250m) | ACC, AEB, highway |\n| **79 GHz** | 77-81 GHz | 3.8 mm | Medium-long | High resolution |\n\n**Trend**: Shifting from 24 GHz → 77/79 GHz for better resolution and longer range\n\n---\n\n### Doppler Effect\n\nThe **Doppler shift** is the change in frequency due to relative motion between radar and target.\n\n#### **Doppler Frequency Shift**\n\n$$f_d = \\frac{2 v_r f_0}{c}$$\n\nWhere:\n- **f_d**: Doppler shift (Hz)\n- **v_r**: Radial velocity (m/s) - positive = approaching, negative = receding\n- **f_0**: Transmitted frequency (Hz)\n- **c**: Speed of light (3 × 10⁸ m/s)\n\n**Example** (77 GHz radar):\n- Target approaching at 30 m/s (108 km/h)\n- f_d = (2 × 30 × 77×10⁹) / (3×10⁸) = **15.4 kHz**\n\n**Key insight**: Doppler shift is proportional to velocity, so radar directly measures speed!\n\n---\n\n### FMCW Radar (Frequency-Modulated Continuous Wave)\n\nMost automotive radars use **FMCW** to measure both range and velocity.\n\n#### **Principle**\n\n1. **Transmit**: Frequency sweeps linearly (chirp)\n   - Start frequency: f₀\n   - Bandwidth: B (e.g., 4 GHz)\n   - Chirp duration: T (e.g., 100 μs)\n\n2. **Receive**: Delayed and Doppler-shifted signal\n\n3. **Mix**: Transmitted and received signals → beat frequency\n\n4. **FFT**: Extract range and velocity from beat frequency\n\n#### **Range Calculation**\n\n$$R = \\frac{c \\cdot f_b \\cdot T}{2B}$$\n\nWhere:\n- **f_b**: Beat frequency (Hz)\n- **B**: Bandwidth (Hz)\n- **T**: Chirp duration (s)\n\n**Resolution**:\n- **Range resolution**: Δr = c / (2B)\n  - For B = 4 GHz: Δr = 3.75 cm\n- **Velocity resolution**: Δv = λ / (2 T_frame)\n  - For 77 GHz, T_frame = 50 ms: Δv ≈ 0.1 m/s\n\n---\n\n### Radar Characteristics\n\n| Parameter | Typical Value | Notes |\n|-----------|---------------|-------|\n| **Range** | 0.2 - 250 m | Long-range: 250m, Short-range: 30m |\n| **Accuracy** | ±0.1 m (range), ±0.1 m/s (velocity) | Very accurate |\n| **FoV (Azimuth)** | ±60° (short), ±15° (long) | Narrow beam for long-range |\n| **FoV (Elevation)** | ±10° | Limited vertical coverage |\n| **Update rate** | 10-50 Hz | Fast updates |\n| **Angular resolution** | 1-15° | Coarse compared to LiDAR |\n\n---\n\n### Radar Processing Pipeline\n\n```\nRaw IF Signal → FFT (Range) → FFT (Doppler) → CFAR Detection → Clustering → Tracking\n```\n\n#### **1. Range-Doppler Map**\n- 2D FFT of FMCW signal\n- Rows: Range bins\n- Columns: Doppler (velocity) bins\n\n#### **2. CFAR (Constant False Alarm Rate)**\n- Adaptive thresholding to detect targets\n- Distinguishes real targets from noise/clutter\n\n#### **3. Angle Estimation**\n- Multiple antennas (MIMO array)\n- Digital beamforming to estimate angle of arrival (AoA)\n\n---\n\n### Advantages & Limitations\n\n**Advantages**:\n- ✅ Direct velocity measurement (Doppler)\n- ✅ Long range (250m+)\n- ✅ Works in all weather (rain, fog, snow)\n- ✅ Works in darkness\n- ✅ Penetrates dust, smoke\n- ✅ Low cost ($50-200)\n- ✅ Small size, low power\n\n**Limitations**:\n- ❌ Low angular resolution (1-15°)\n- ❌ No height information (2D)\n- ❌ Difficulty detecting stationary objects (ground clutter)\n- ❌ Multipath reflections (metallic environments)\n- ❌ Interference from other radars\n\n**Solutions**:\n- High-resolution MIMO radar (4D imaging radar)\n- Sensor fusion with camera/LiDAR for angular precision\n- Advanced clutter suppression algorithms",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# FMCW Radar Simulation with Doppler Effect\n\nclass FMCWRadar:\n    \"\"\"\n    Simulates an FMCW (Frequency-Modulated Continuous Wave) radar.\n    \"\"\"\n    def __init__(self, \n                 frequency=77e9,        # Hz (77 GHz)\n                 bandwidth=4e9,         # Hz (4 GHz)\n                 chirp_duration=100e-6, # seconds (100 μs)\n                 max_range=250,         # meters\n                 angular_fov=30,        # degrees (±15°)\n                 angular_resolution=2): # degrees\n        self.f0 = frequency\n        self.B = bandwidth\n        self.T = chirp_duration\n        self.max_range = max_range\n        self.c = 3e8  # Speed of light (m/s)\n        self.wavelength = self.c / self.f0\n        \n        # Angular parameters\n        self.angular_fov = np.deg2rad(angular_fov)\n        self.angular_res = np.deg2rad(angular_resolution)\n        \n        # Range and velocity resolution\n        self.range_resolution = self.c / (2 * self.B)\n        self.velocity_resolution = self.wavelength / (2 * self.T * 10)  # Assuming 10 chirps per frame\n        \n    def calculate_beat_frequency(self, range_m):\n        \"\"\"Calculate beat frequency for a given range.\"\"\"\n        return (2 * self.B * range_m) / (self.c * self.T)\n    \n    def calculate_doppler_shift(self, velocity_mps):\n        \"\"\"\n        Calculate Doppler frequency shift for a given radial velocity.\n        Positive velocity = approaching, negative = receding.\n        \"\"\"\n        return (2 * velocity_mps * self.f0) / self.c\n    \n    def detect_targets(self, objects, ego_velocity=0):\n        \"\"\"\n        Detect targets and compute their range, velocity, and angle.\n        \n        Args:\n            objects: List of (position, velocity, rcs) tuples\n                - position: [x, y] in radar frame\n                - velocity: [vx, vy] velocity vector\n                - rcs: Radar cross-section (m²)\n            ego_velocity: Ego vehicle velocity (m/s, forward positive)\n        \n        Returns:\n            detections: List of (range, radial_velocity, angle, rcs) tuples\n        \"\"\"\n        detections = []\n        \n        for pos, vel, rcs in objects:\n            # Calculate range\n            range_m = np.linalg.norm(pos)\n            \n            if range_m > self.max_range:\n                continue\n            \n            # Calculate angle\n            angle = np.arctan2(pos[1], pos[0])\n            \n            # Check if within FoV\n            if abs(angle) > self.angular_fov / 2:\n                continue\n            \n            # Calculate radial velocity (projection onto line of sight)\n            # Relative velocity = target velocity - ego velocity\n            rel_velocity = vel - np.array([ego_velocity, 0])\n            unit_vector = pos / range_m\n            radial_velocity = np.dot(rel_velocity, unit_vector)\n            \n            detections.append((range_m, radial_velocity, angle, rcs))\n        \n        return detections\n    \n    def generate_range_doppler_map(self, detections, num_range_bins=256, num_doppler_bins=128):\n        \"\"\"\n        Generate a Range-Doppler map from detections.\n        \n        Args:\n            detections: List of (range, radial_velocity, angle, rcs) tuples\n            num_range_bins: Number of range bins\n            num_doppler_bins: Number of Doppler bins\n        \n        Returns:\n            rd_map: Range-Doppler map (2D array)\n            range_axis: Range values for each bin\n            velocity_axis: Velocity values for each bin\n        \"\"\"\n        # Create empty map\n        rd_map = np.zeros((num_range_bins, num_doppler_bins))\n        \n        # Range and velocity axes\n        range_axis = np.linspace(0, self.max_range, num_range_bins)\n        max_velocity = 50  # m/s\n        velocity_axis = np.linspace(-max_velocity, max_velocity, num_doppler_bins)\n        \n        # Populate map with detections\n        for range_m, radial_velocity, angle, rcs in detections:\n            # Find closest bins\n            range_bin = np.argmin(np.abs(range_axis - range_m))\n            velocity_bin = np.argmin(np.abs(velocity_axis - radial_velocity))\n            \n            # Add detection with RCS-based intensity\n            rd_map[range_bin, velocity_bin] += rcs\n        \n        return rd_map, range_axis, velocity_axis\n\n\n# Create 77 GHz FMCW Radar\nradar = FMCWRadar(\n    frequency=77e9,\n    bandwidth=4e9,\n    chirp_duration=100e-6,\n    max_range=150,\n    angular_fov=30,\n    angular_resolution=2\n)\n\nprint(\"=\" * 70)\nprint(\"Radar Specifications:\")\nprint(\"=\" * 70)\nprint(f\"Frequency: {radar.f0/1e9:.1f} GHz\")\nprint(f\"Bandwidth: {radar.B/1e9:.1f} GHz\")\nprint(f\"Wavelength: {radar.wavelength*1000:.2f} mm\")\nprint(f\"Range resolution: {radar.range_resolution:.3f} m\")\nprint(f\"Velocity resolution: {radar.velocity_resolution:.3f} m/s\")\nprint(f\"Max range: {radar.max_range} m\")\nprint(f\"Field of View: ±{np.rad2deg(radar.angular_fov/2):.1f}°\")\nprint(\"=\" * 70)\n\n# Define scenario: Highway driving\nego_velocity = 25  # m/s (90 km/h)\n\n# Objects: (position [x, y], velocity [vx, vy], RCS)\nobjects = [\n    (np.array([50, 0]), np.array([20, 0]), 10),      # Vehicle ahead, slower (72 km/h)\n    (np.array([80, -3.5]), np.array([30, 0]), 10),   # Vehicle in left lane, same speed\n    (np.array([100, 3.5]), np.array([15, 0]), 10),   # Vehicle in right lane, slower\n    (np.array([30, 2]), np.array([25, 0]), 5),       # Motorcycle nearby\n    (np.array([120, 0]), np.array([25, 0]), 15),     # Truck ahead, same speed\n]\n\n# Detect targets\ndetections = radar.detect_targets(objects, ego_velocity=ego_velocity)\n\nprint(f\"\\n=== Detected Targets: {len(detections)} ===\")\nfor i, (r, v_r, theta, rcs) in enumerate(detections):\n    doppler_shift = radar.calculate_doppler_shift(v_r)\n    print(f\"Target {i+1}: Range={r:.1f}m, Radial Vel={v_r:.1f}m/s ({v_r*3.6:.1f}km/h), \"\n          f\"Angle={np.rad2deg(theta):.1f}°, Doppler={doppler_shift/1000:.2f}kHz, RCS={rcs}m²\")\n\n# Generate Range-Doppler map\nrd_map, range_axis, velocity_axis = radar.generate_range_doppler_map(detections)\n\n# Visualization\nfig = plt.figure(figsize=(16, 10))\n\n# 1. Range-Doppler Map\nax1 = fig.add_subplot(2, 2, 1)\nim1 = ax1.imshow(rd_map, aspect='auto', origin='lower', cmap='hot', \n                 extent=[velocity_axis[0], velocity_axis[-1], range_axis[0], range_axis[-1]])\nax1.set_xlabel('Radial Velocity (m/s)', fontsize=11, fontweight='bold')\nax1.set_ylabel('Range (m)', fontsize=11, fontweight='bold')\nax1.set_title('Range-Doppler Map (FMCW Radar)', fontsize=13, fontweight='bold')\nax1.grid(True, alpha=0.3, color='white')\nax1.axvline(x=0, color='cyan', linestyle='--', linewidth=1.5, label='Zero velocity')\nplt.colorbar(im1, ax=ax1, label='Intensity (RCS)')\nax1.legend()\n\n# 2. Top-down view with radar FoV\nax2 = fig.add_subplot(2, 2, 2)\n\n# Draw radar FoV\nfov_angle = radar.angular_fov / 2\nfov_arc = Wedge((0, 0), radar.max_range, np.rad2deg(-fov_angle), np.rad2deg(fov_angle), \n                facecolor='blue', alpha=0.1, edgecolor='blue', linewidth=2)\nax2.add_patch(fov_arc)\n\n# Draw radar position\nax2.plot(0, 0, 'bs', markersize=15, label='Ego (Radar)', markeredgecolor='black', linewidth=2)\n\n# Draw detected objects\nfor i, (r, v_r, theta, rcs) in enumerate(detections):\n    x = r * np.cos(theta)\n    y = r * np.sin(theta)\n    \n    # Color based on velocity (approaching=red, receding=green, stationary=yellow)\n    if v_r > 1:\n        color = 'red'\n        marker = '^'\n    elif v_r < -1:\n        color = 'green'\n        marker = 'v'\n    else:\n        color = 'yellow'\n        marker = 'o'\n    \n    ax2.plot(x, y, marker=marker, markersize=10+rcs/2, color=color, \n            markeredgecolor='black', linewidth=1.5)\n    ax2.text(x+5, y+3, f\"{v_r:.1f}m/s\", fontsize=9, color='black')\n\nax2.set_xlabel('X (m) - Forward', fontsize=11, fontweight='bold')\nax2.set_ylabel('Y (m) - Lateral', fontsize=11, fontweight='bold')\nax2.set_title('Radar Detection - Top View', fontsize=13, fontweight='bold')\nax2.set_aspect('equal')\nax2.grid(True, alpha=0.3)\nax2.set_xlim([-20, radar.max_range + 20])\nax2.set_ylim([-50, 50])\nax2.legend()\n\n# Add custom legend for velocity\nfrom matplotlib.lines import Line2D\nlegend_elements = [\n    Line2D([0], [0], marker='^', color='w', markerfacecolor='red', markersize=10, \n           label='Approaching', markeredgecolor='black'),\n    Line2D([0], [0], marker='v', color='w', markerfacecolor='green', markersize=10, \n           label='Receding', markeredgecolor='black'),\n    Line2D([0], [0], marker='o', color='w', markerfacecolor='yellow', markersize=10, \n           label='Stationary', markeredgecolor='black')\n]\nax2.legend(handles=legend_elements, loc='upper right')\n\n# 3. Range vs Radial Velocity scatter\nax3 = fig.add_subplot(2, 2, 3)\nfor i, (r, v_r, theta, rcs) in enumerate(detections):\n    ax3.scatter(r, v_r, s=rcs*20, alpha=0.7, edgecolors='black', linewidth=1.5)\n    ax3.text(r+2, v_r+0.5, f\"T{i+1}\", fontsize=9)\n\nax3.set_xlabel('Range (m)', fontsize=11, fontweight='bold')\nax3.set_ylabel('Radial Velocity (m/s)', fontsize=11, fontweight='bold')\nax3.set_title('Range vs Radial Velocity', fontsize=13, fontweight='bold')\nax3.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.7)\nax3.grid(True, alpha=0.3)\nax3.set_xlim([0, radar.max_range])\nax3.set_ylim([-30, 30])\n\n# 4. Doppler Shift Demonstration\nax4 = fig.add_subplot(2, 2, 4)\n\n# Generate Doppler shift curve\nvelocities = np.linspace(-50, 50, 100)\ndoppler_shifts = radar.calculate_doppler_shift(velocities)\n\nax4.plot(velocities, doppler_shifts/1000, 'b-', linewidth=2, label='Doppler Shift')\nax4.axhline(y=0, color='gray', linestyle='--', linewidth=1)\nax4.axvline(x=0, color='gray', linestyle='--', linewidth=1)\n\n# Mark detected targets\nfor i, (r, v_r, theta, rcs) in enumerate(detections):\n    doppler = radar.calculate_doppler_shift(v_r)\n    ax4.plot(v_r, doppler/1000, 'ro', markersize=10, markeredgecolor='black', linewidth=1.5)\n\nax4.set_xlabel('Radial Velocity (m/s)', fontsize=11, fontweight='bold')\nax4.set_ylabel('Doppler Shift (kHz)', fontsize=11, fontweight='bold')\nax4.set_title('Doppler Effect (77 GHz Radar)', fontsize=13, fontweight='bold')\nax4.grid(True, alpha=0.3)\nax4.legend()\nax4.set_xlim([-50, 50])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Doppler Shift Examples:\")\nprint(\"=\" * 70)\ntest_velocities = [30, 10, 0, -10, -30]  # m/s\nfor v in test_velocities:\n    doppler = radar.calculate_doppler_shift(v)\n    print(f\"Velocity: {v:+.1f} m/s ({v*3.6:+.1f} km/h) → Doppler shift: {doppler/1000:+.2f} kHz\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Sensor Fusion & Calibration\n\n**Sensor fusion** combines data from multiple sensors to create a more accurate and robust perception of the environment than any single sensor could provide.\n\n### Why Sensor Fusion?\n\nEach sensor has complementary strengths and weaknesses:\n\n| Sensor | Strengths | Weaknesses |\n|--------|-----------|------------|\n| **Camera** | High resolution, color, semantic info | No depth, weather-sensitive |\n| **LiDAR** | Accurate 3D, works in dark | Expensive, no color, weather-sensitive |\n| **Radar** | Direct velocity, all-weather, long range | Low angular resolution, no height |\n\n**Synergy**: Camera provides semantic understanding, LiDAR provides accurate localization, Radar provides velocity and all-weather robustness.\n\n---\n\n### Sensor Fusion Architectures\n\n#### **1. Early Fusion (Low-Level)**\n- Combine raw sensor data before processing\n- Example: Fuse LiDAR point cloud with camera pixels\n- **Advantage**: Maximum information retention\n- **Disadvantage**: Computationally expensive, requires precise calibration\n\n#### **2. Late Fusion (High-Level)**\n- Each sensor processes independently, combine detections\n- Example: Merge object lists from camera, LiDAR, and radar\n- **Advantage**: Modular, fault-tolerant\n- **Disadvantage**: Information loss from independent processing\n\n#### **3. Mid-Level Fusion (Feature-Level)**\n- Fuse intermediate representations (features)\n- Example: Combine camera features with LiDAR features\n- **Advantage**: Balance between information and efficiency\n- **Disadvantage**: Requires careful feature design\n\n**Most common in production**: Late fusion with track-level association\n\n---\n\n### Sensor Calibration\n\n**Calibration** determines the transformation between sensor frames to enable fusion.\n\n#### **Intrinsic Calibration**\n- Determines sensor's internal parameters\n- **Camera**: Focal length, principal point, distortion coefficients\n- **LiDAR**: Beam angles, range offsets\n- **Radar**: Antenna phase offsets\n\n#### **Extrinsic Calibration**\n- Determines sensor position and orientation relative to vehicle frame\n- Represented as homogeneous transformation matrix (4×4):\n\n$$\\mathbf{T}_{sensor}^{vehicle} = \\begin{bmatrix} \\mathbf{R} & \\mathbf{t} \\\\ \\mathbf{0}^T & 1 \\end{bmatrix}$$\n\nWhere:\n- **R**: 3×3 rotation matrix\n- **t**: 3×1 translation vector (x, y, z position)\n\n**Calibration methods**:\n- **Manual**: Physical measurements\n- **Target-based**: Use calibration targets (checkerboard, retroreflector)\n- **Automatic**: Online calibration using scene structure\n\n---\n\n### Coordinate Frame Transformations\n\n**Standard vehicle frame** (ISO 8855):\n- **X**: Forward (longitudinal)\n- **Y**: Left (lateral)\n- **Z**: Up (vertical)\n- **Origin**: Typically rear axle center\n\n**Transformation pipeline**:\n```\nSensor Frame → Vehicle Frame → World Frame\n```\n\n#### **Example: LiDAR to Camera Projection**\n\n1. **Transform** LiDAR point from LiDAR frame to camera frame:\n   $$\\mathbf{p}_{cam} = \\mathbf{T}_{lidar}^{cam} \\mathbf{p}_{lidar}$$\n\n2. **Project** 3D point to 2D image:\n   $$\\mathbf{p}_{image} = \\mathbf{K} \\cdot \\mathbf{p}_{cam}$$\n\n3. **Normalize** by depth and get pixel coordinates\n\n---\n\n### Time Synchronization\n\nSensors operate at different frequencies:\n- **Camera**: 30-60 Hz\n- **LiDAR**: 10-20 Hz  \n- **Radar**: 10-50 Hz\n- **IMU**: 100-1000 Hz\n- **GNSS**: 1-10 Hz\n\n**Synchronization strategies**:\n\n#### **1. Hardware Synchronization**\n- Use PTP (Precision Time Protocol) or GPS time\n- All sensors trigger from common clock\n- **Accuracy**: Sub-millisecond\n- **Challenge**: Requires hardware support\n\n#### **2. Software Synchronization**\n- Timestamp each measurement\n- Interpolate/extrapolate to common time\n- **Advantage**: Works with any sensors\n- **Disadvantage**: Introduces errors for fast-moving objects\n\n#### **3. Buffering & Alignment**\n- Buffer measurements within time window (e.g., 50ms)\n- Align closest timestamps for fusion\n\n**Critical for moving vehicles**: At 30 m/s, a 50ms delay = 1.5m position error!\n\n---\n\n### Sensor Fusion Algorithms\n\n#### **1. Kalman Filter**\n- Optimal for linear systems with Gaussian noise\n- Used for sensor fusion and state estimation\n- **Variants**: Extended Kalman Filter (EKF), Unscented Kalman Filter (UKF)\n\n**State update**:\n$$\\hat{x}_{k} = \\hat{x}_{k-1} + K_k (z_k - H \\hat{x}_{k-1})$$\n\nWhere:\n- **K_k**: Kalman gain (weighs prediction vs measurement)\n- **z_k**: Measurement from sensor\n- **H**: Measurement model\n\n#### **2. Bayesian Occupancy Grid**\n- Discretize space into grid cells\n- Each cell has occupancy probability\n- Fuse measurements from multiple sensors probabilistically\n\n#### **3. Deep Learning Fusion**\n- Neural networks learn optimal fusion strategy\n- **Architectures**: BEVFusion, TransFusion, PointPainting\n- **Input**: Multi-modal data (images, point clouds, radar)\n- **Output**: Unified object detections or occupancy\n\n---\n\n### Data Association\n\n**Problem**: Match detections from different sensors to same real-world object.\n\n#### **Gating**\n- Only consider associations within distance threshold\n- **Mahalanobis distance**: Accounts for uncertainty\n\n$$d_M = \\sqrt{(\\mathbf{z}_1 - \\mathbf{z}_2)^T \\mathbf{S}^{-1} (\\mathbf{z}_1 - \\mathbf{z}_2)}$$\n\n#### **Association Algorithms**\n- **Nearest Neighbor**: Match closest detections\n- **Global Nearest Neighbor (GNN)**: Solve assignment problem\n- **Joint Probabilistic Data Association (JPDA)**: Probabilistic matching\n- **Multi-Hypothesis Tracking (MHT)**: Maintain multiple association hypotheses\n\n---\n\n### Sensor Fusion Pipeline\n\n```\n┌─────────┐  ┌─────────┐  ┌─────────┐\n│ Camera  │  │ LiDAR   │  │ Radar   │\n└────┬────┘  └────┬────┘  └────┬────┘\n     │            │            │\n     ▼            ▼            ▼\n┌─────────────────────────────────┐\n│   Calibration & Synchronization │\n└────────────┬────────────────────┘\n             ▼\n┌──────────────────────────────────┐\n│    Feature Extraction            │\n│  (Objects, Lane Lines, etc.)     │\n└────────────┬─────────────────────┘\n             ▼\n┌──────────────────────────────────┐\n│     Data Association             │\n│  (Match detections across sensors)│\n└────────────┬─────────────────────┘\n             ▼\n┌──────────────────────────────────┐\n│    State Estimation              │\n│  (Kalman Filter, Tracking)       │\n└────────────┬─────────────────────┘\n             ▼\n┌──────────────────────────────────┐\n│   Fused Perception Output        │\n│ (Object list: position, velocity,│\n│  classification, uncertainty)    │\n└──────────────────────────────────┘\n```\n\n---\n\n### Advantages of Sensor Fusion\n\n✅ **Robustness**: Redundancy handles sensor failures  \n✅ **Accuracy**: Complementary info reduces uncertainty  \n✅ **Completeness**: Better coverage of environment  \n✅ **Reduced false positives**: Cross-validation between sensors  \n✅ **All-weather operation**: Radar compensates when camera/LiDAR fail  \n\n---\n\n### Challenges\n\n❌ **Calibration drift**: Sensors shift over time (vibration, temperature)  \n❌ **Synchronization**: Temporal misalignment causes errors  \n❌ **Computational cost**: Processing multiple sensors in real-time  \n❌ **Conflicting measurements**: Resolving disagreements between sensors  \n❌ **Scalability**: Adding more sensors increases complexity  \n\n---\n\n### Industry Examples\n\n| Company | Sensor Suite | Fusion Strategy |\n|---------|--------------|-----------------|\n| **Waymo** | 5 LiDARs, 29 cameras, radars | Early + mid-level fusion |\n| **Tesla** | 8 cameras, 12 ultrasonic, radar* | Vision-first, camera fusion |\n| **Cruise** | 5 LiDARs, 21 cameras, 5 radars | Multi-modal late fusion |\n| **Mobileye** | Cameras, radar, (LiDAR optional) | Camera-centric with radar |\n\n*Tesla removed radar from newer vehicles (2023+) for vision-only approach\n\n---\n\n### Best Practices\n\n1. **Calibrate regularly**: Check calibration every maintenance cycle\n2. **Validate fusion output**: Compare against ground truth data\n3. **Handle graceful degradation**: System should work with sensor failures\n4. **Monitor sensor health**: Detect blockage, malfunction, degradation\n5. **Use uncertainty estimates**: Track confidence in fused measurements\n6. **Test in diverse conditions**: Rain, fog, night, tunnels, etc.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Simple Sensor Fusion Demonstration\n# Late fusion: Combine object detections from camera, LiDAR, and radar\n\nclass SimpleSensorFusion:\n    \"\"\"\n    Demonstrates late fusion of multi-sensor object detections.\n    \"\"\"\n    def __init__(self, association_threshold=3.0):\n        \"\"\"\n        Args:\n            association_threshold: Maximum distance (m) for associating detections\n        \"\"\"\n        self.threshold = association_threshold\n    \n    def associate_detections(self, camera_dets, lidar_dets, radar_dets):\n        \"\"\"\n        Associate detections from different sensors using nearest neighbor.\n        \n        Args:\n            camera_dets: List of (x, y, confidence, type) from camera\n            lidar_dets: List of (x, y, z, confidence) from LiDAR\n            radar_dets: List of (x, y, velocity, confidence) from radar\n        \n        Returns:\n            fused_objects: List of fused object detections\n        \"\"\"\n        fused_objects = []\n        used_lidar = set()\n        used_radar = set()\n        \n        # For each camera detection, find closest LiDAR and radar matches\n        for cam_det in camera_dets:\n            cam_x, cam_y, cam_conf, obj_type = cam_det\n            cam_pos = np.array([cam_x, cam_y])\n            \n            # Find closest LiDAR detection\n            best_lidar_idx = None\n            best_lidar_dist = float('inf')\n            for i, lidar_det in enumerate(lidar_dets):\n                if i in used_lidar:\n                    continue\n                lidar_pos = np.array([lidar_det[0], lidar_det[1]])\n                dist = np.linalg.norm(cam_pos - lidar_pos)\n                if dist < best_lidar_dist and dist < self.threshold:\n                    best_lidar_dist = dist\n                    best_lidar_idx = i\n            \n            # Find closest radar detection\n            best_radar_idx = None\n            best_radar_dist = float('inf')\n            for i, radar_det in enumerate(radar_dets):\n                if i in used_radar:\n                    continue\n                radar_pos = np.array([radar_det[0], radar_det[1]])\n                dist = np.linalg.norm(cam_pos - radar_pos)\n                if dist < best_radar_dist and dist < self.threshold:\n                    best_radar_dist = dist\n                    best_radar_idx = i\n            \n            # Fuse detections\n            fused_obj = self.fuse_object(cam_det, \n                                         lidar_dets[best_lidar_idx] if best_lidar_idx is not None else None,\n                                         radar_dets[best_radar_idx] if best_radar_idx is not None else None)\n            fused_objects.append(fused_obj)\n            \n            if best_lidar_idx is not None:\n                used_lidar.add(best_lidar_idx)\n            if best_radar_idx is not None:\n                used_radar.add(best_radar_idx)\n        \n        return fused_objects\n    \n    def fuse_object(self, cam_det, lidar_det, radar_det):\n        \"\"\"\n        Fuse information from camera, LiDAR, and radar for a single object.\n        \n        Returns:\n            dict with fused properties: position, velocity, type, confidence\n        \"\"\"\n        # Initialize with camera detection\n        cam_x, cam_y, cam_conf, obj_type = cam_det\n        \n        # Weighted average of positions (higher weight for more confident sensors)\n        positions = [(cam_x, cam_y, cam_conf)]\n        confidences = [cam_conf]\n        \n        if lidar_det is not None:\n            lidar_x, lidar_y, lidar_z, lidar_conf = lidar_det\n            positions.append((lidar_x, lidar_y, lidar_conf * 1.2))  # LiDAR more reliable for position\n            confidences.append(lidar_conf)\n        \n        if radar_det is not None:\n            radar_x, radar_y, radar_vel, radar_conf = radar_det\n            positions.append((radar_x, radar_y, radar_conf))\n            confidences.append(radar_conf)\n        \n        # Weighted average position\n        total_weight = sum(p[2] for p in positions)\n        fused_x = sum(p[0] * p[2] for p in positions) / total_weight\n        fused_y = sum(p[1] * p[2] for p in positions) / total_weight\n        \n        # Velocity from radar (only sensor that directly measures it)\n        velocity = radar_det[2] if radar_det is not None else 0.0\n        \n        # Overall confidence (average with boost for multi-sensor)\n        avg_conf = np.mean(confidences)\n        num_sensors = 1 + (lidar_det is not None) + (radar_det is not None)\n        fused_conf = min(1.0, avg_conf * (1 + 0.2 * (num_sensors - 1)))\n        \n        return {\n            'position': (fused_x, fused_y),\n            'velocity': velocity,\n            'type': obj_type,\n            'confidence': fused_conf,\n            'num_sensors': num_sensors,\n            'has_lidar': lidar_det is not None,\n            'has_radar': radar_det is not None\n        }\n\n\n# Create fusion system\nfusion = SimpleSensorFusion(association_threshold=3.0)\n\n# Simulate detections from 3 sensors for a highway scene\n# Camera detections: (x, y, confidence, type)\ncamera_dets = [\n    (50, 0, 0.9, 'vehicle'),\n    (80, -3.5, 0.85, 'vehicle'),\n    (30, 2, 0.7, 'motorcycle'),\n    (100, 3.5, 0.8, 'vehicle'),\n]\n\n# LiDAR detections: (x, y, z, confidence)\nlidar_dets = [\n    (51, 0.2, 0.75, 0.95),  # Matches camera detection 1\n    (81, -3.3, 0.8, 0.9),   # Matches camera detection 2\n    (99, 3.7, 0.9, 0.85),   # Matches camera detection 4\n]\n\n# Radar detections: (x, y, velocity_radial, confidence)\nradar_dets = [\n    (50.5, -0.1, -5, 0.9),    # Matches camera detection 1 (approaching)\n    (79, -3.6, 0, 0.85),      # Matches camera detection 2 (same speed)\n    (29, 2.2, -0.5, 0.75),    # Matches camera detection 3\n]\n\n# Perform sensor fusion\nfused_objects = fusion.associate_detections(camera_dets, lidar_dets, radar_dets)\n\n# Visualization\nfig = plt.figure(figsize=(16, 12))\n\n# 1. Camera-only detections\nax1 = fig.add_subplot(2, 2, 1)\nax1.plot(0, 0, 'bs', markersize=15, label='Ego vehicle', markeredgecolor='black', linewidth=2)\nfor i, (x, y, conf, obj_type) in enumerate(camera_dets):\n    color = 'red' if obj_type == 'vehicle' else 'orange'\n    ax1.plot(x, y, 'o', color=color, markersize=15, alpha=0.6, markeredgecolor='black', linewidth=2)\n    ax1.text(x+3, y+1, f\"Cam{i+1}\\n{obj_type}\\n{conf:.2f}\", fontsize=9)\nax1.set_xlabel('X (m) - Forward', fontsize=11, fontweight='bold')\nax1.set_ylabel('Y (m) - Lateral', fontsize=11, fontweight='bold')\nax1.set_title('Camera Detections Only', fontsize=13, fontweight='bold')\nax1.set_aspect('equal')\nax1.grid(True, alpha=0.3)\nax1.set_xlim([-10, 110])\nax1.set_ylim([-10, 10])\nax1.legend()\n\n# 2. LiDAR-only detections\nax2 = fig.add_subplot(2, 2, 2)\nax2.plot(0, 0, 'bs', markersize=15, label='Ego vehicle', markeredgecolor='black', linewidth=2)\nfor i, (x, y, z, conf) in enumerate(lidar_dets):\n    ax2.plot(x, y, '^', color='green', markersize=15, alpha=0.6, markeredgecolor='black', linewidth=2)\n    ax2.text(x+3, y+1, f\"L{i+1}\\n{conf:.2f}\", fontsize=9)\nax2.set_xlabel('X (m) - Forward', fontsize=11, fontweight='bold')\nax2.set_ylabel('Y (m) - Lateral', fontsize=11, fontweight='bold')\nax2.set_title('LiDAR Detections Only', fontsize=13, fontweight='bold')\nax2.set_aspect('equal')\nax2.grid(True, alpha=0.3)\nax2.set_xlim([-10, 110])\nax2.set_ylim([-10, 10])\nax2.legend()\n\n# 3. Radar-only detections\nax3 = fig.add_subplot(2, 2, 3)\nax3.plot(0, 0, 'bs', markersize=15, label='Ego vehicle', markeredgecolor='black', linewidth=2)\nfor i, (x, y, vel, conf) in enumerate(radar_dets):\n    color = 'red' if vel < -1 else ('yellow' if abs(vel) <= 1 else 'green')\n    ax3.plot(x, y, 's', color=color, markersize=15, alpha=0.6, markeredgecolor='black', linewidth=2)\n    ax3.text(x+3, y+1, f\"R{i+1}\\nv={vel:.1f}m/s\\n{conf:.2f}\", fontsize=8)\nax3.set_xlabel('X (m) - Forward', fontsize=11, fontweight='bold')\nax3.set_ylabel('Y (m) - Lateral', fontsize=11, fontweight='bold')\nax3.set_title('Radar Detections Only', fontsize=13, fontweight='bold')\nax3.set_aspect('equal')\nax3.grid(True, alpha=0.3)\nax3.set_xlim([-10, 110])\nax3.set_ylim([-10, 10])\nax3.legend()\n\n# 4. Fused detections\nax4 = fig.add_subplot(2, 2, 4)\nax4.plot(0, 0, 'bs', markersize=15, label='Ego vehicle', markeredgecolor='black', linewidth=2)\n\nfor i, obj in enumerate(fused_objects):\n    x, y = obj['position']\n    \n    # Size based on confidence\n    size = 20 + 10 * obj['confidence']\n    \n    # Color based on number of sensors\n    if obj['num_sensors'] == 3:\n        color = 'purple'\n        edge_color = 'gold'\n        edge_width = 3\n    elif obj['num_sensors'] == 2:\n        color = 'blue'\n        edge_color = 'black'\n        edge_width = 2\n    else:\n        color = 'gray'\n        edge_color = 'black'\n        edge_width = 1\n    \n    ax4.plot(x, y, 'D', color=color, markersize=size, alpha=0.7, \n            markeredgecolor=edge_color, linewidth=edge_width)\n    \n    # Label with details\n    sensor_str = f\"{'C' if True else ''}{'L' if obj['has_lidar'] else ''}{'R' if obj['has_radar'] else ''}\"\n    ax4.text(x+3, y+1.5, f\"F{i+1} ({sensor_str})\\n{obj['type']}\\nv={obj['velocity']:.1f}m/s\\nconf={obj['confidence']:.2f}\", \n            fontsize=8, bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n\nax4.set_xlabel('X (m) - Forward', fontsize=11, fontweight='bold')\nax4.set_ylabel('Y (m) - Lateral', fontsize=11, fontweight='bold')\nax4.set_title('Fused Multi-Sensor Detections', fontsize=13, fontweight='bold')\nax4.set_aspect('equal')\nax4.grid(True, alpha=0.3)\nax4.set_xlim([-10, 110])\nax4.set_ylim([-10, 10])\n\n# Custom legend\nfrom matplotlib.lines import Line2D\nlegend_elements = [\n    Line2D([0], [0], marker='D', color='w', markerfacecolor='purple', markersize=12, \n           label='3 sensors', markeredgecolor='gold', linewidth=3),\n    Line2D([0], [0], marker='D', color='w', markerfacecolor='blue', markersize=12, \n           label='2 sensors', markeredgecolor='black', linewidth=2),\n    Line2D([0], [0], marker='D', color='w', markerfacecolor='gray', markersize=12, \n           label='1 sensor', markeredgecolor='black', linewidth=1)\n]\nax4.legend(handles=legend_elements, loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n# Print fusion statistics\nprint(\"=\" * 80)\nprint(\"SENSOR FUSION RESULTS\")\nprint(\"=\" * 80)\nprint(f\"Camera detections: {len(camera_dets)}\")\nprint(f\"LiDAR detections: {len(lidar_dets)}\")\nprint(f\"Radar detections: {len(radar_dets)}\")\nprint(f\"Fused objects: {len(fused_objects)}\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Fused Object Details:\")\nprint(\"=\" * 80)\n\nfor i, obj in enumerate(fused_objects):\n    sensors_used = []\n    if True:  # Camera always present in this example\n        sensors_used.append(\"Camera\")\n    if obj['has_lidar']:\n        sensors_used.append(\"LiDAR\")\n    if obj['has_radar']:\n        sensors_used.append(\"Radar\")\n    \n    print(f\"\\nObject {i+1}:\")\n    print(f\"  Position: ({obj['position'][0]:.2f}, {obj['position'][1]:.2f}) m\")\n    print(f\"  Velocity: {obj['velocity']:.2f} m/s\")\n    print(f\"  Type: {obj['type']}\")\n    print(f\"  Confidence: {obj['confidence']:.2f}\")\n    print(f\"  Sensors: {', '.join(sensors_used)} ({obj['num_sensors']} total)\")\n    print(f\"  Confidence boost from fusion: +{(obj['confidence'] - 0.8)*100:.1f}%\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Key Insights:\")\nprint(\"=\" * 80)\nprint(\"✓ Multi-sensor detections have higher confidence\")\nprint(\"✓ LiDAR provides accurate positioning\")\nprint(\"✓ Radar provides direct velocity measurement\")\nprint(\"✓ Camera provides object classification\")\nprint(\"✓ Fusion reduces uncertainty and false positives\")\nprint(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Camera Field of View\n\nGiven a camera with:\n- Focal length fx = 1000 pixels\n- Image width = 1920 pixels\n- Calculate the horizontal field of view (FoV) in degrees.\n\n**Hint**: Use the relationship: `FoV = 2 * arctan(width / (2 * fx))`\n\n---\n\n### Exercise 2: LiDAR Point Cloud Analysis\n\nUsing the LiDAR sensor defined earlier:\n1. Modify the environment to add 2 more objects (your choice of position and type)\n2. Run the LiDAR scan\n3. Report:\n   - Total number of points\n   - Number of clusters detected\n   - Distance to each cluster\n\n---\n\n### Exercise 3: Doppler Radar Calculation\n\nA 77 GHz radar detects a vehicle:\n- Range: 80 meters\n- Doppler shift: +10.3 kHz (positive = approaching)\n\nCalculate:\n1. The radial velocity of the target (m/s)\n2. The target's speed in km/h\n3. If the ego vehicle is traveling at 100 km/h, what is the target vehicle's actual speed?\n\n**Formula**: v_r = (f_d * c) / (2 * f_0)\n\n---\n\n### Exercise 4: Sensor Fusion Confidence\n\nGiven three sensor detections of the same object:\n- Camera: position = (50, 2), confidence = 0.85\n- LiDAR: position = (51, 1.8), confidence = 0.95\n- Radar: position = (49.5, 2.1), confidence = 0.90\n\n1. Calculate the fused position using weighted average (weight = confidence)\n2. Calculate the overall confidence (use the formula from the fusion example)\n3. Compare the result to using only the highest-confidence sensor\n\n---\n\n### Exercise 5: Sensor Comparison Table\n\nFill in the comparison table for a specific scenario: **Detecting a pedestrian at night in light rain at 30 meters distance**\n\n| Sensor | Can Detect? | Accuracy | Confidence | Justification |\n|--------|-------------|----------|------------|---------------|\n| Camera | ? | ? | ? | ? |\n| LiDAR | ? | ? | ? | ? |\n| Radar | ? | ? | ? | ? |\n\nRate each as: High / Medium / Low / No"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Exercise Solutions\n\nprint(\"=\" * 80)\nprint(\"EXERCISE SOLUTIONS - WEEK 4: SENSOR TECHNOLOGIES & FUSION\")\nprint(\"=\" * 80)\n\n# Exercise 1: Camera Field of View\nprint(\"\\n### Exercise 1: Camera Field of View ###\\n\")\n\nfx = 1000  # pixels\nwidth = 1920  # pixels\n\n# Calculate horizontal FoV\nfov_rad = 2 * np.arctan(width / (2 * fx))\nfov_deg = np.rad2deg(fov_rad)\n\nprint(f\"Given:\")\nprint(f\"  Focal length fx = {fx} pixels\")\nprint(f\"  Image width = {width} pixels\")\nprint(f\"\\nCalculation:\")\nprint(f\"  FoV = 2 × arctan(width / (2 × fx))\")\nprint(f\"  FoV = 2 × arctan({width} / {2*fx})\")\nprint(f\"  FoV = 2 × arctan({width/(2*fx):.4f})\")\nprint(f\"  FoV = {fov_deg:.2f}°\")\nprint(f\"\\nAnswer: The horizontal field of view is {fov_deg:.2f}°\")\n\n# Exercise 2: LiDAR Point Cloud Analysis\nprint(\"\\n\" + \"=\" * 80)\nprint(\"### Exercise 2: LiDAR Point Cloud Analysis ###\\n\")\n\n# Create LiDAR sensor\nlidar_ex2 = LiDARSensor(\n    horizontal_resolution=0.4,\n    vertical_resolution=2.0,\n    horizontal_fov=360,\n    max_range=50\n)\n\n# Modified environment with 2 additional objects\nobjects_ex2 = [\n    (np.array([15, 0, 0.75]), 2.0, 'box'),      # Original: Vehicle ahead\n    (np.array([20, -3, 0.75]), 2.0, 'box'),     # Original: Vehicle left lane\n    (np.array([8, 2, 0.9]), 0.3, 'sphere'),     # Original: Pedestrian right\n    (np.array([25, 5, 0.5]), 0.3, 'sphere'),    # Original: Pedestrian far right\n    (np.array([35, 0, 0.75]), 2.5, 'box'),      # NEW: Large truck ahead\n    (np.array([12, -4, 0.9]), 0.3, 'sphere'),   # NEW: Pedestrian left\n]\n\nprint(\"Environment objects:\")\nfor i, (center, size, shape) in enumerate(objects_ex2):\n    print(f\"  Object {i+1}: {shape} at ({center[0]:.1f}, {center[1]:.1f}, {center[2]:.1f})m, size={size}\")\n\n# Perform scan\npoint_cloud_ex2 = lidar_ex2.scan_environment(objects_ex2)\nprint(f\"\\nTotal points in scan: {len(point_cloud_ex2)}\")\n\n# Remove ground and cluster\nnon_ground_ex2 = lidar_ex2.remove_ground_plane(point_cloud_ex2, ground_threshold=-0.1)\nprint(f\"Non-ground points: {len(non_ground_ex2)}\")\n\nif len(non_ground_ex2) > 0:\n    cluster_labels_ex2 = lidar_ex2.cluster_points(non_ground_ex2, distance_threshold=1.5)\n    num_clusters_ex2 = len(set(cluster_labels_ex2)) - (1 if -1 in cluster_labels_ex2 else 0)\n    print(f\"Number of clusters detected: {num_clusters_ex2}\")\n    \n    print(\"\\nCluster details:\")\n    for cluster_id in set(cluster_labels_ex2):\n        if cluster_id == -1:\n            continue\n        cluster_points = non_ground_ex2[cluster_labels_ex2 == cluster_id]\n        center = cluster_points[:, :3].mean(axis=0)\n        distance = np.linalg.norm(center[:2])\n        print(f\"  Cluster {cluster_id}: Distance={distance:.2f}m, Points={len(cluster_points)}, \"\n              f\"Center=({center[0]:.1f}, {center[1]:.1f}, {center[2]:.1f})m\")\n\n# Exercise 3: Doppler Radar Calculation\nprint(\"\\n\" + \"=\" * 80)\nprint(\"### Exercise 3: Doppler Radar Calculation ###\\n\")\n\nf_0 = 77e9  # Hz (77 GHz)\nc = 3e8  # m/s (speed of light)\nf_d = 10.3e3  # Hz (10.3 kHz Doppler shift)\nego_speed_kmh = 100  # km/h\n\nprint(f\"Given:\")\nprint(f\"  Radar frequency f_0 = 77 GHz\")\nprint(f\"  Doppler shift f_d = +10.3 kHz (approaching)\")\nprint(f\"  Ego vehicle speed = 100 km/h\")\n\n# 1. Calculate radial velocity\nv_r = (f_d * c) / (2 * f_0)\nprint(f\"\\n1. Radial velocity:\")\nprint(f\"   v_r = (f_d × c) / (2 × f_0)\")\nprint(f\"   v_r = ({f_d} × {c}) / (2 × {f_0})\")\nprint(f\"   v_r = {v_r:.2f} m/s\")\n\n# 2. Convert to km/h\nv_r_kmh = v_r * 3.6\nprint(f\"\\n2. Radial velocity in km/h:\")\nprint(f\"   v_r = {v_r:.2f} m/s × 3.6 = {v_r_kmh:.2f} km/h\")\n\n# 3. Calculate target's actual speed\nego_speed_mps = ego_speed_kmh / 3.6\ntarget_speed_mps = ego_speed_mps - v_r  # Approaching means closing speed\ntarget_speed_kmh = target_speed_mps * 3.6\n\nprint(f\"\\n3. Target vehicle's actual speed:\")\nprint(f\"   Ego speed = {ego_speed_kmh} km/h = {ego_speed_mps:.2f} m/s\")\nprint(f\"   Since target is approaching (positive Doppler):\")\nprint(f\"   Target speed = Ego speed - Radial velocity\")\nprint(f\"   Target speed = {ego_speed_mps:.2f} - {v_r:.2f} = {target_speed_mps:.2f} m/s\")\nprint(f\"   Target speed = {target_speed_kmh:.2f} km/h\")\nprint(f\"\\n   Answer: Target is traveling at {target_speed_kmh:.1f} km/h in the same direction\")\n\n# Exercise 4: Sensor Fusion Confidence\nprint(\"\\n\" + \"=\" * 80)\nprint(\"### Exercise 4: Sensor Fusion Confidence ###\\n\")\n\n# Sensor detections\ncamera_pos = np.array([50, 2])\ncamera_conf = 0.85\nlidar_pos = np.array([51, 1.8])\nlidar_conf = 0.95\nradar_pos = np.array([49.5, 2.1])\nradar_conf = 0.90\n\nprint(\"Given detections:\")\nprint(f\"  Camera: position = ({camera_pos[0]}, {camera_pos[1]}), confidence = {camera_conf}\")\nprint(f\"  LiDAR:  position = ({lidar_pos[0]}, {lidar_pos[1]}), confidence = {lidar_conf}\")\nprint(f\"  Radar:  position = ({radar_pos[0]}, {radar_pos[1]}), confidence = {radar_conf}\")\n\n# 1. Calculate fused position using weighted average\ntotal_weight = camera_conf + lidar_conf + radar_conf\nfused_x = (camera_pos[0] * camera_conf + lidar_pos[0] * lidar_conf + radar_pos[0] * radar_conf) / total_weight\nfused_y = (camera_pos[1] * camera_conf + lidar_pos[1] * lidar_conf + radar_pos[1] * radar_conf) / total_weight\nfused_pos = np.array([fused_x, fused_y])\n\nprint(f\"\\n1. Fused position (weighted average):\")\nprint(f\"   Total weight = {camera_conf} + {lidar_conf} + {radar_conf} = {total_weight}\")\nprint(f\"   x_fused = ({camera_pos[0]}×{camera_conf} + {lidar_pos[0]}×{lidar_conf} + {radar_pos[0]}×{radar_conf}) / {total_weight}\")\nprint(f\"   x_fused = {fused_x:.3f} m\")\nprint(f\"   y_fused = ({camera_pos[1]}×{camera_conf} + {lidar_pos[1]}×{lidar_conf} + {radar_pos[1]}×{radar_conf}) / {total_weight}\")\nprint(f\"   y_fused = {fused_y:.3f} m\")\nprint(f\"   Fused position = ({fused_x:.3f}, {fused_y:.3f}) m\")\n\n# 2. Calculate overall confidence\navg_conf = (camera_conf + lidar_conf + radar_conf) / 3\nnum_sensors = 3\nfusion_boost = 1 + 0.2 * (num_sensors - 1)\nfused_conf = min(1.0, avg_conf * fusion_boost)\n\nprint(f\"\\n2. Overall confidence:\")\nprint(f\"   Average confidence = ({camera_conf} + {lidar_conf} + {radar_conf}) / 3 = {avg_conf:.3f}\")\nprint(f\"   Fusion boost factor = 1 + 0.2 × (3 - 1) = {fusion_boost:.2f}\")\nprint(f\"   Fused confidence = {avg_conf:.3f} × {fusion_boost:.2f} = {fused_conf:.3f}\")\n\n# 3. Compare to highest-confidence sensor only\nbest_sensor = \"LiDAR\"\nbest_conf = lidar_conf\nbest_pos = lidar_pos\n\nprint(f\"\\n3. Comparison:\")\nprint(f\"   Best single sensor: {best_sensor}\")\nprint(f\"   - Position: ({best_pos[0]}, {best_pos[1]})\")\nprint(f\"   - Confidence: {best_conf:.3f}\")\nprint(f\"   \\n   Fused result:\")\nprint(f\"   - Position: ({fused_x:.3f}, {fused_y:.3f})\")\nprint(f\"   - Confidence: {fused_conf:.3f}\")\nprint(f\"   \\n   Improvement:\")\nprint(f\"   - Confidence gain: +{(fused_conf - best_conf)*100:.1f}%\")\nprint(f\"   - Position error vs LiDAR: {np.linalg.norm(fused_pos - lidar_pos):.3f} m\")\nprint(f\"   \\n   Conclusion: Fusion provides higher confidence and averages out position errors!\")\n\n# Exercise 5: Sensor Comparison Table\nprint(\"\\n\" + \"=\" * 80)\nprint(\"### Exercise 5: Sensor Comparison - Pedestrian at Night in Light Rain (30m) ###\\n\")\n\ncomparison_data = {\n    'Camera': {\n        'Can Detect': 'Low',\n        'Accuracy': 'Low',\n        'Confidence': 'Low',\n        'Justification': 'Poor lighting (night) and rain reduce visibility. May miss pedestrian or have low confidence.'\n    },\n    'LiDAR': {\n        'Can Detect': 'Medium',\n        'Accuracy': 'Medium',\n        'Confidence': 'Medium',\n        'Justification': 'Works in darkness, but light rain causes noise and reduces effective range. Can detect at 30m but with degraded accuracy.'\n    },\n    'Radar': {\n        'Can Detect': 'Medium',\n        'Accuracy': 'Low',\n        'Confidence': 'Medium',\n        'Justification': 'Unaffected by darkness or rain. However, pedestrians have small radar cross-section (RCS), making detection challenging. Low angular resolution.'\n    }\n}\n\nprint(\"Scenario: Detecting a pedestrian at night in light rain at 30 meters distance\\n\")\nprint(f\"{'Sensor':<10} {'Can Detect?':<15} {'Accuracy':<12} {'Confidence':<12} {'Justification'}\")\nprint(\"-\" * 110)\n\nfor sensor, data in comparison_data.items():\n    print(f\"{sensor:<10} {data['Can Detect']:<15} {data['Accuracy']:<12} {data['Confidence']:<12} {data['Justification']}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Key Takeaways:\")\nprint(\"=\" * 80)\nprint(\"✓ No single sensor is reliable in this challenging scenario\")\nprint(\"✓ Sensor fusion is CRITICAL for robust detection\")\nprint(\"✓ Camera + LiDAR + Radar together provide complementary information\")\nprint(\"✓ This demonstrates why autonomous vehicles need multi-sensor suites\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## References\n\n### Textbooks\n\n1. **Thrun, S., Burgard, W., & Fox, D.** (2005). *Probabilistic Robotics*. MIT Press.\n   - Chapters 6-7: Sensor models and observation models\n   - Gold standard for sensor modeling and Bayesian filtering\n\n2. **Szeliski, R.** (2022). *Computer Vision: Algorithms and Applications* (2nd ed.). Springer.\n   - Chapter 2: Image formation and camera models\n   - Chapter 11: Structure from motion\n   - Available online: https://szeliski.org/Book/\n\n3. **Levinson, J., & Thrun, S.** (2014). *Robust Vehicle Localization in Urban Environments Using Probabilistic Maps*.\n   - Sensor fusion techniques for autonomous driving\n\n### Research Papers\n\n#### Camera\n\n4. **DeTone, D., Malisiewicz, T., & Rabinovich, A.** (2018). \"SuperPoint: Self-Supervised Interest Point Detection and Description.\" *CVPR*.\n   - Modern feature detection for camera-based perception\n\n5. **Philion, J., & Fidler, S.** (2020). \"Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D.\" *ECCV*.\n   - Camera-to-BEV transformation for autonomous driving\n\n#### LiDAR\n\n6. **Qi, C. R., Yi, L., Su, H., & Guibas, L. J.** (2017). \"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space.\" *NeurIPS*.\n   - Deep learning on LiDAR point clouds\n\n7. **Geiger, A., Lenz, P., & Urtasun, R.** (2012). \"Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite.\" *CVPR*.\n   - Standard benchmark for LiDAR and camera evaluation\n\n#### Radar\n\n8. **Major, B., et al.** (2019). \"Vehicle Detection With Automotive Radar Using Deep Learning on Range-Azimuth-Doppler Tensors.\" *ICCV Workshops*.\n   - Deep learning for automotive radar\n\n9. **Bilik, I., et al.** (2019). \"Automotive MIMO Radar for Urban Environments.\" *IEEE Radar Conference*.\n   - Modern 4D imaging radar technology\n\n#### Sensor Fusion\n\n10. **Liang, M., et al.** (2019). \"Multi-Task Multi-Sensor Fusion for 3D Object Detection.\" *CVPR*.\n    - State-of-the-art multi-sensor fusion\n\n11. **Liu, Z., et al.** (2023). \"BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation.\" *ICRA*.\n    - Unified BEV representation for camera + LiDAR\n\n12. **Chen, X., et al.** (2017). \"Multi-View 3D Object Detection Network for Autonomous Driving.\" *CVPR*.\n    - MV3D: Early work on multi-sensor 3D detection\n\n### Industry Reports\n\n13. **Yole Développement** (2023). \"Automotive LiDAR Market and Technology Report.\"\n    - Market trends and technology comparison\n\n14. **IEEE Spectrum** (2022). \"Sensor Fusion: The Key to Autonomous Driving.\"\n    - Industry overview of sensor suites\n\n### Online Resources\n\n15. **Udacity Self-Driving Car Nanodegree**: https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013\n    - Sensor fusion and tracking projects\n\n16. **KITTI Dataset**: http://www.cvlibs.net/datasets/kitti/\n    - Benchmark dataset with camera, LiDAR, GPS/IMU data\n\n17. **nuScenes Dataset**: https://www.nuscenes.org/\n    - Full sensor suite dataset (6 cameras, 5 radars, 1 LiDAR)\n\n18. **Waymo Open Dataset**: https://waymo.com/open/\n    - Large-scale dataset from Waymo's autonomous vehicles\n\n19. **OpenPCDet**: https://github.com/open-mmlab/OpenPCDet\n    - Open-source toolbox for LiDAR-based 3D object detection\n\n### Standards & Regulations\n\n20. **ISO 26262**: Road vehicles - Functional safety\n    - Safety requirements for automotive sensors\n\n21. **SAE J3016**: Taxonomy and Definitions for Terms Related to Driving Automation Systems\n    - Defines sensor requirements for each autonomy level\n\n### Sensor Manufacturers (for specifications)\n\n22. **Velodyne Lidar**: https://velodynelidar.com/\n    - HDL-64E, VLS-128 specifications\n\n23. **Luminar Technologies**: https://luminartech.com/\n    - Iris LiDAR specifications\n\n24. **Continental Automotive**: https://www.continental-automotive.com/\n    - ARS540 radar specifications (77 GHz FMCW)\n\n25. **Mobileye**: https://www.mobileye.com/\n    - EyeQ camera-based perception systems\n\n---\n\n### Recommended Reading Order\n\n**For beginners:**\n1. Start with Thrun's *Probabilistic Robotics* Chapter 6\n2. Explore KITTI dataset to see real sensor data\n3. Read Geiger et al. (2012) for context\n\n**For practitioners:**\n1. Szeliski's *Computer Vision* for camera fundamentals\n2. Liu et al. (2023) BEVFusion for modern fusion architectures\n3. Explore nuScenes dataset for multi-sensor projects\n\n**For researchers:**\n1. Survey recent CVPR/ICCV papers on sensor fusion\n2. Study Waymo Open Dataset papers\n3. Follow arXiv for latest developments (cs.CV, cs.RO)\n\n---\n\n*Note: Some links and papers may require institutional access. Many papers are available on arXiv.org*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}