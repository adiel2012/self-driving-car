{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 4: Sensor Technologies & Fusion\n\n## Module II: Perception & Localization\n\n### Topics Covered\n\n- Camera (Image Processing, Homography)\n- LiDAR (Point Clouds, Range Data)\n- Radar (Doppler Effects)\n- Sensor Synchronization and Calibration\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. Understand the physics and characteristics of automotive sensors (Camera, LiDAR, Radar)\n2. Process camera images and apply homography transformations\n3. Work with LiDAR point clouds and range data\n4. Understand radar principles and Doppler effect\n5. Perform sensor calibration and extrinsic/intrinsic parameter estimation\n6. Implement multi-sensor fusion techniques\n7. Synchronize data from multiple sensors with different frequencies\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nImport required libraries for sensor data processing and visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, Rectangle, Wedge\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.spatial.transform import Rotation\n\n# Set random seed\nnp.random.seed(42)\n\n# Plotting configuration\nplt.rcParams['figure.figsize'] = (14, 8)\nplt.rcParams['font.size'] = 10\n\nprint(\"Libraries loaded successfully!\")\nprint(\"NumPy version:\", np.__version__)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Camera Sensors\n\n**Cameras** are the most cost-effective sensors for autonomous vehicles, providing rich visual information about the environment including colors, textures, lane markings, traffic signs, and lights.\n\n### Camera Basics\n\n#### **Pinhole Camera Model**\n\nThe fundamental model relating 3D world points to 2D image points:\n\n$$\\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = \\frac{1}{Z} \\mathbf{K} \\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}$$\n\nWhere:\n- **(X, Y, Z)**: 3D point in camera frame\n- **(u, v)**: 2D pixel coordinates\n- **K**: Intrinsic camera matrix\n- **Z**: Depth (distance from camera)\n\n#### **Intrinsic Parameters (K Matrix)**\n\n$$\\mathbf{K} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n\n- **fx, fy**: Focal lengths (in pixels)\n- **cx, cy**: Principal point (optical center)\n\n**Typical values** for automotive cameras:\n- Focal length: 700-1200 pixels\n- Resolution: 1920×1080 (Full HD) or 1280×720 (HD)\n- Field of View (FoV): 50-120 degrees\n- Frame rate: 30-60 FPS\n\n---\n\n### Camera Types in Autonomous Vehicles\n\n| Camera Type | FoV | Use Case | Position |\n|-------------|-----|----------|----------|\n| **Front Wide** | 120° | Lane keeping, traffic lights | Windshield |\n| **Front Narrow** | 50° | Long-range detection (200m+) | Windshield |\n| **Side** | 90° | Lane changes, blind spots | Side mirrors |\n| **Rear** | 120° | Parking, rear monitoring | Rear bumper |\n| **Fisheye** | 180°+ | 360° surround view | Roof/corners |\n\n---\n\n### Image Processing Pipeline\n\n```\nRaw Image → Undistortion → Color Space → Feature Extraction → Object Detection\n                           Conversion\n```\n\n#### **1. Lens Distortion Correction**\n\nReal lenses have radial and tangential distortion. Undistortion formula:\n\n$$\\begin{align}\nx_{distorted} &= x(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) \\\\\ny_{distorted} &= y(1 + k_1 r^2 + k_2 r^4 + k_3 r^6)\n\\end{align}$$\n\nWhere **r² = x² + y²** and **k₁, k₂, k₃** are radial distortion coefficients.\n\n#### **2. Color Spaces**\n- **RGB**: Raw camera output\n- **HSV**: Better for color-based segmentation (e.g., lane lines)\n- **Grayscale**: Reduces data, used for edge detection\n\n#### **3. Edge Detection**\n- **Canny Edge Detector**: Multi-stage algorithm for edge detection\n- **Sobel Filter**: Gradient-based edge detection\n\n---\n\n### Homography: Ground Plane Projection\n\n**Homography** transforms a planar surface from one view to another. Critical for **bird's-eye view** (BEV) transformation.\n\n#### **Homography Matrix (3×3)**\n\n$$\\mathbf{H} = \\begin{bmatrix} h_{11} & h_{12} & h_{13} \\\\ h_{21} & h_{22} & h_{23} \\\\ h_{31} & h_{32} & h_{33} \\end{bmatrix}$$\n\nMaps image point **(u, v)** to ground plane point **(x, y)**:\n\n$$\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\sim \\mathbf{H} \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}$$\n\n#### **Computing Homography**\n\nGiven 4+ corresponding points between image and ground plane, solve for H using:\n- **Direct Linear Transform (DLT)**\n- **RANSAC** for robustness to outliers\n\n**Application**: Convert front camera view → top-down view for lane detection\n\n---\n\n### Advantages & Limitations\n\n**Advantages**:\n- ✅ Low cost ($50-200 per camera)\n- ✅ Rich semantic information (colors, textures, text)\n- ✅ High resolution (megapixels)\n- ✅ Passive sensor (no emissions)\n\n**Limitations**:\n- ❌ No direct depth information (monocular)\n- ❌ Performance degrades in poor lighting, rain, fog\n- ❌ Sensitive to sun glare\n- ❌ Requires heavy computation for deep learning\n\n**Solutions**:\n- Stereo cameras for depth\n- Multi-sensor fusion (camera + LiDAR)\n- HDR imaging for varying light conditions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Camera Homography Demonstration\n\nclass CameraModel:\n    \"\"\"Simple pinhole camera model.\"\"\"\n    \n    def __init__(self, fx=800, fy=800, cx=640, cy=360, width=1280, height=720):\n        \"\"\"\n        Args:\n            fx, fy: Focal lengths in pixels\n            cx, cy: Principal point (optical center)\n            width, height: Image resolution\n        \"\"\"\n        self.K = np.array([\n            [fx, 0, cx],\n            [0, fy, cy],\n            [0, 0, 1]\n        ])\n        self.width = width\n        self.height = height\n        \n    def project_3d_to_2d(self, points_3d):\n        \"\"\"\n        Project 3D points (in camera frame) to 2D image coordinates.\n        \n        Args:\n            points_3d: Nx3 array of 3D points [X, Y, Z]\n        \n        Returns:\n            points_2d: Nx2 array of pixel coordinates [u, v]\n        \"\"\"\n        # Homogeneous coordinates\n        points_2d_hom = (self.K @ points_3d.T).T\n        \n        # Normalize by Z (depth)\n        points_2d = points_2d_hom[:, :2] / points_2d_hom[:, 2:3]\n        \n        return points_2d\n\n\n# Simulate camera viewing a road scene\ncamera = CameraModel()\n\n# Define 3D points on ground plane (Z=0, camera at height 1.5m)\n# Road lanes in front of vehicle\ncamera_height = 1.5  # meters above ground\n\n# Create grid of points on ground plane\nx_range = np.linspace(0, 50, 20)  # 0-50m in front\ny_range = np.linspace(-10, 10, 10)  # ±10m width\n\nground_points = []\nfor x in x_range:\n    for y in y_range:\n        # Points in camera frame: X=forward, Y=left, Z=up\n        # Ground plane: Z = -camera_height (below camera)\n        ground_points.append([x, y, -camera_height])\n\nground_points = np.array(ground_points)\n\n# Only keep points with positive depth (in front of camera)\nvalid_mask = ground_points[:, 0] > 0\nground_points_valid = ground_points[valid_mask]\n\n# Project to image\nimage_points = camera.project_3d_to_2d(ground_points_valid)\n\n# Filter points within image bounds\nin_image = (image_points[:, 0] >= 0) & (image_points[:, 0] < camera.width) & \\\n           (image_points[:, 1] >= 0) & (image_points[:, 1] < camera.height)\n\nimage_points_valid = image_points[in_image]\nground_points_final = ground_points_valid[in_image]\n\n# Visualization\nfig = plt.figure(figsize=(16, 7))\n\n# Left: Front camera view\nax1 = fig.add_subplot(1, 2, 1)\nax1.scatter(image_points_valid[:, 0], image_points_valid[:, 1], c=ground_points_final[:, 0], \n           cmap='viridis', s=50, alpha=0.6)\nax1.set_xlim([0, camera.width])\nax1.set_ylim([camera.height, 0])  # Flip Y for image coordinates\nax1.set_xlabel('u (pixels)', fontweight='bold')\nax1.set_ylabel('v (pixels)', fontweight='bold')\nax1.set_title('Front Camera View (Perspective)', fontweight='bold', fontsize=14)\nax1.set_aspect('equal')\nax1.grid(True, alpha=0.3)\n\n# Add lane markings\nlane_left_3d = np.array([[i, 3.5, -camera_height] for i in range(5, 50, 2)])\nlane_right_3d = np.array([[i, -3.5, -camera_height] for i in range(5, 50, 2)])\n\nlane_left_2d = camera.project_3d_to_2d(lane_left_3d)\nlane_right_2d = camera.project_3d_to_2d(lane_right_3d)\n\nax1.plot(lane_left_2d[:, 0], lane_left_2d[:, 1], 'y-', linewidth=3, label='Left Lane')\nax1.plot(lane_right_2d[:, 0], lane_right_2d[:, 1], 'y-', linewidth=3, label='Right Lane')\nax1.legend()\n\n# Right: Bird's Eye View (top-down)\nax2 = fig.add_subplot(1, 2, 2)\nax2.scatter(ground_points_final[:, 0], ground_points_final[:, 1], \n           c=ground_points_final[:, 0], cmap='viridis', s=50, alpha=0.6)\nax2.set_xlabel('X (m) - Forward', fontweight='bold')\nax2.set_ylabel('Y (m) - Left', fontweight='bold')\nax2.set_title('Bird\\'s Eye View (Top-Down)', fontweight='bold', fontsize=14)\nax2.set_aspect('equal')\nax2.grid(True, alpha=0.3)\n\n# Add lane markings\nax2.plot(lane_left_3d[:, 0], lane_left_3d[:, 1], 'y-', linewidth=3, label='Left Lane')\nax2.plot(lane_right_3d[:, 0], lane_right_3d[:, 1], 'y-', linewidth=3, label='Right Lane')\n\n# Add vehicle position\nax2.plot(0, 0, 'ro', markersize=15, label='Vehicle')\nvehicle_rect = Rectangle((-2, -1), 4, 2, fill=False, edgecolor='red', linewidth=2)\nax2.add_patch(vehicle_rect)\n\nax2.legend()\nax2.set_xlim([-5, 50])\nax2.set_ylim([-15, 15])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"=\" * 70)\nprint(\"Camera Parameters:\")\nprint(\"=\" * 70)\nprint(f\"Intrinsic Matrix K:\\n{camera.K}\\n\")\nprint(f\"Focal lengths: fx={camera.K[0,0]:.1f} px, fy={camera.K[1,1]:.1f} px\")\nprint(f\"Principal point: cx={camera.K[0,2]:.1f} px, cy={camera.K[1,2]:.1f} px\")\nprint(f\"Resolution: {camera.width}×{camera.height}\")\nprint(f\"Camera height: {camera_height} m\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "*Exercises to be added*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- References to be added"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}