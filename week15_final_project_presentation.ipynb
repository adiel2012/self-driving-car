{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 15: Final Project & Presentation\n\n### Topics Covered\n\n- Integrating concepts from the entire course into a comprehensive autonomous driving project\n- Project design, implementation, testing, and presentation\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. **Design** a comprehensive autonomous driving system integrating multiple course concepts\n2. **Implement** algorithms for perception, planning, control, and decision-making\n3. **Validate** your system through simulation and testing\n4. **Present** technical work clearly and professionally\n5. **Evaluate** autonomous driving systems using appropriate metrics\n\n---\n\n## Introduction\n\nThis final week brings together everything you've learned throughout the course. Your final project is an opportunity to demonstrate mastery of autonomous vehicle concepts by designing, implementing, and presenting a complete system or significant component.\n\n### Course Recap\n\nOver the past 14 weeks, you've studied:\n\n**Weeks 1-3: Foundations**\n- Introduction to autonomous vehicles and SAE levels\n- Computer vision (lane detection, object detection)\n- Deep learning for perception\n\n**Weeks 4-6: Sensing & Localization**\n- Sensor fusion (camera, LiDAR, radar)\n- Localization and mapping (SLAM, particle filters)\n- Path planning (A*, RRT, optimization)\n\n**Weeks 7-9: Control & Decision Making**\n- Vehicle dynamics and control (PID, MPC, LQR)\n- Behavioral planning (finite state machines, behavior trees)\n- Decision making under uncertainty (MDPs, POMDPs)\n\n**Weeks 10-12: Advanced Topics**\n- Motion planning in dynamic environments\n- Prediction and interaction with other agents\n- Functional safety (ISO 26262, ASIL, redundancy)\n\n**Weeks 13-14: Validation & Connectivity**\n- Testing, validation, and ethics\n- V2X communication and cooperative driving\n- Cloud computing, OTA updates, cybersecurity\n\n### Project Philosophy\n\nYour final project should:\n\n1. **Integrate Multiple Concepts**: Combine at least 2-3 major topics from the course\n2. **Demonstrate Technical Depth**: Go beyond basic implementations\n3. **Show Real-World Relevance**: Address practical challenges in autonomous driving\n4. **Be Well-Documented**: Clear code, explanations, and visualizations\n5. **Include Validation**: Test your system and analyze results\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle, Polygon, Circle\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\nimport time\n\n# Set random seed for reproducibility\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Final Project Requirements\n\n### Project Scope\n\nYour final project should be a **3-4 week effort** demonstrating integration of course concepts. You may work individually or in teams of 2-3.\n\n### Minimum Requirements\n\n**1. Technical Components** (Choose at least 2-3):\n- Perception (object detection, tracking, segmentation)\n- Localization (SLAM, particle filter, sensor fusion)\n- Planning (path planning, behavior planning, motion planning)\n- Control (trajectory tracking, speed control)\n- Prediction (trajectory forecasting, intent recognition)\n- Decision making (rule-based, learning-based, game-theoretic)\n\n**2. Implementation**:\n- Working code with clear documentation\n- Proper software architecture (modular, testable)\n- Use of appropriate data structures and algorithms\n- Version control (Git recommended)\n\n**3. Validation**:\n- Testing methodology (unit tests, integration tests, scenario tests)\n- Performance metrics appropriate to your project\n- Comparison with baselines or alternative approaches\n- Visualization of results\n\n**4. Documentation**:\n- Project report (5-10 pages)\n- Code comments and docstrings\n- README with setup instructions\n- Demo video or live demonstration\n\n**5. Presentation** (10-15 minutes):\n- Problem statement and motivation\n- Technical approach and architecture\n- Results and analysis\n- Challenges and future work\n- Q&A\n\n### Deliverables\n\n1. **Code Repository** (GitHub/GitLab)\n   - Source code\n   - README with setup and usage\n   - Requirements/dependencies\n   - Test suite (if applicable)\n\n2. **Project Report** (PDF)\n   - Introduction and motivation\n   - Related work and background\n   - System architecture\n   - Implementation details\n   - Experimental results\n   - Conclusion and future work\n   - References\n\n3. **Presentation** (Slides + Demo)\n   - 10-15 minute presentation\n   - Live demonstration or video\n   - Q&A preparation\n\n4. **Demo** (Video or Live)\n   - 2-5 minute demonstration\n   - Show key functionality\n   - Highlight technical achievements\n\n---\n\n## 2. Project Ideas\n\nBelow are suggested project topics organized by difficulty and theme. Feel free to propose your own!\n\n### 2.1 Perception Projects\n\n**P1: Multi-Object Tracking with Sensor Fusion** (Medium)\n- Fuse camera and LiDAR for object detection\n- Implement tracking algorithm (Kalman filter, Hungarian assignment)\n- Handle occlusions and track management\n- Evaluate on KITTI or nuScenes dataset\n\n**P2: Lane Detection and Road Segmentation** (Medium)\n- Implement lane detection (Hough transform or deep learning)\n- Road segmentation using semantic segmentation\n- Post-processing for lane fitting\n- Test on various lighting and weather conditions\n\n**P3: 3D Object Detection from Point Clouds** (Hard)\n- Implement PointNet or similar architecture\n- 3D bounding box estimation from LiDAR\n- Bird's eye view representation\n- Evaluate precision and recall\n\n### 2.2 Planning Projects\n\n**P4: Autonomous Parking System** (Medium)\n- Path planning for parallel/perpendicular parking\n- Hybrid A* or RRT-based planner\n- Collision checking with obstacles\n- Smooth trajectory generation\n\n**P5: Highway Merging with Behavior Planning** (Medium-Hard)\n- Finite state machine for lane change decision\n- Gap acceptance and velocity planning\n- Safety verification\n- Comfort optimization\n\n**P6: Intersection Navigation** (Hard)\n- Stop sign and traffic light detection\n- Right-of-way logic\n- Trajectory planning through intersection\n- Multi-agent interaction\n\n### 2.3 Control Projects\n\n**P7: Model Predictive Control for Autonomous Driving** (Hard)\n- Implement MPC for trajectory tracking\n- Vehicle dynamics model\n- Constraint handling (speed limits, acceleration limits)\n- Real-time performance optimization\n\n**P8: Adaptive Cruise Control with Stop-and-Go** (Medium)\n- Longitudinal control using PID or MPC\n- Car-following model\n- Smooth acceleration/deceleration\n- Safety gap maintenance\n\n**P9: Stanley Controller for Path Tracking** (Medium)\n- Implement Stanley lateral controller\n- Pure pursuit comparison\n- Tuning for different speeds and curvatures\n- Performance analysis\n\n### 2.4 Integrated Systems\n\n**P10: End-to-End Autonomous Driving Pipeline** (Hard)\n- Perception → Planning → Control integration\n- Multi-scenario support (highway, urban, parking)\n- Modular architecture\n- Comprehensive testing\n\n**P11: Cooperative Platooning System** (Hard)\n- V2V communication simulation\n- CACC implementation\n- String stability analysis\n- Energy efficiency evaluation\n\n**P12: Autonomous Valet Parking** (Medium-Hard)\n- HD map of parking lot\n- Path planning from entrance to spot\n- Obstacle avoidance\n- Speed control and stopping\n\n### 2.5 Prediction & Interaction\n\n**P13: Pedestrian Trajectory Prediction** (Medium-Hard)\n- Social LSTM or similar model\n- Context awareness (crosswalks, sidewalks)\n- Multi-modal prediction\n- Visualization of uncertainty\n\n**P14: Interactive Motion Planning** (Hard)\n- Game-theoretic planning at intersections\n- Nash equilibrium computation\n- Adversarial scenario handling\n- Safety guarantees\n\n**P15: Intent Recognition for Lane Changes** (Medium)\n- Hidden Markov Model or neural network\n- Feature engineering (lateral position, velocity, turn signal)\n- Real-time prediction\n- Validation on real driving data\n\n### 2.6 Safety & Validation\n\n**P16: Functional Safety Analysis of ACC System** (Medium)\n- HARA (Hazard Analysis and Risk Assessment)\n- ASIL determination\n- Fault tree analysis\n- Safety mechanisms design\n\n**P17: Adversarial Testing Framework** (Hard)\n- Generate challenging scenarios\n- Genetic algorithm for scenario optimization\n- Coverage metrics\n- Failure analysis\n\n**P18: Sensor Fault Detection and Diagnosis** (Medium)\n- Anomaly detection on sensor data\n- Redundancy-based fault detection\n- Graceful degradation\n- Experimental validation\n\n### 2.7 Simulation & Tools\n\n**P19: Custom Autonomous Driving Simulator** (Hard)\n- 2D or 3D environment\n- Vehicle dynamics simulation\n- Sensor simulation (camera, LiDAR)\n- Scenario editor\n\n**P20: Real-Time Visualization Dashboard** (Medium)\n- Live data visualization\n- Sensor fusion display\n- Planning trajectory overlay\n- Performance metrics\n\n---\n\n## 3. Project Timeline\n\n**Week 1: Planning & Design**\n- Define project scope and objectives\n- Literature review and related work\n- System architecture design\n- Set up development environment\n\n**Week 2: Implementation (Part 1)**\n- Implement core algorithms\n- Unit testing\n- Initial integration\n\n**Week 3: Implementation (Part 2) & Testing**\n- Complete implementation\n- Integration testing\n- Scenario-based validation\n- Performance tuning\n\n**Week 4: Documentation & Presentation**\n- Write project report\n- Create presentation slides\n- Prepare demonstration\n- Practice presentation\n\n---\n\n## 4. Technical Approach Guide\n\n### 4.1 System Architecture\n\nA well-designed autonomous driving system typically follows this architecture:\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                      SENSORS                             │\n│  Camera │ LiDAR │ Radar │ GPS/IMU │ V2X │ HD Map        │\n└────────────────────┬────────────────────────────────────┘\n                     │\n┌────────────────────▼────────────────────────────────────┐\n│                   PERCEPTION                             │\n│  Detection │ Tracking │ Segmentation │ Localization     │\n└────────────────────┬────────────────────────────────────┘\n                     │\n┌────────────────────▼────────────────────────────────────┐\n│                   PREDICTION                             │\n│  Trajectory Forecasting │ Intent Recognition            │\n└────────────────────┬────────────────────────────────────┘\n                     │\n┌────────────────────▼────────────────────────────────────┐\n│                    PLANNING                              │\n│  Mission │ Behavioral │ Motion │ Trajectory             │\n└────────────────────┬────────────────────────────────────┘\n                     │\n┌────────────────────▼────────────────────────────────────┐\n│                    CONTROL                               │\n│  Lateral │ Longitudinal │ Actuation                     │\n└────────────────────┬────────────────────────────────────┘\n                     │\n┌────────────────────▼────────────────────────────────────┐\n│                   ACTUATORS                              │\n│  Steering │ Throttle │ Brake                            │\n└─────────────────────────────────────────────────────────┘\n```\n\n**Key Principles:**\n- **Modularity**: Each component has clear interfaces\n- **Testability**: Components can be tested independently\n- **Observability**: Log and visualize internal states\n- **Safety**: Fail-safe mechanisms and redundancy\n\n### 4.2 Development Best Practices\n\n**Code Organization:**\n```\nproject/\n├── README.md\n├── requirements.txt\n├── setup.py\n├── src/\n│   ├── perception/\n│   │   ├── detection.py\n│   │   ├── tracking.py\n│   │   └── localization.py\n│   ├── planning/\n│   │   ├── behavior.py\n│   │   ├── motion.py\n│   │   └── trajectory.py\n│   ├── control/\n│   │   ├── lateral.py\n│   │   └── longitudinal.py\n│   └── utils/\n│       ├── visualization.py\n│       └── metrics.py\n├── tests/\n│   ├── test_perception.py\n│   ├── test_planning.py\n│   └── test_control.py\n├── data/\n│   ├── scenarios/\n│   └── logs/\n├── notebooks/\n│   ├── exploration.ipynb\n│   └── results.ipynb\n└── docs/\n    └── report.pdf\n```\n\n**Version Control:**\n- Use Git for version control\n- Meaningful commit messages\n- Branch for features, merge to main\n- Tag releases\n\n**Testing:**\n- Unit tests for individual functions\n- Integration tests for component interaction\n- Scenario tests for end-to-end behavior\n- Continuous integration (optional but recommended)\n\n**Documentation:**\n- Docstrings for all functions and classes\n- README with setup and usage instructions\n- Inline comments for complex logic\n- Architecture diagrams\n\n### 4.3 Validation Metrics\n\nChoose metrics appropriate to your project:\n\n**Perception:**\n- Precision, Recall, F1-score (object detection)\n- Intersection over Union (IoU)\n- Multi-Object Tracking Accuracy (MOTA)\n- Localization error (meters)\n\n**Planning:**\n- Path length and smoothness\n- Computation time\n- Collision rate\n- Comfort (jerk, lateral acceleration)\n\n**Control:**\n- Tracking error (cross-track, heading)\n- Control effort\n- Settling time\n- Overshoot percentage\n\n**End-to-End:**\n- Success rate on scenarios\n- Time to complete task\n- Safety violations\n- User comfort (if applicable)\n\n### 4.4 Common Pitfalls\n\n1. **Scope Creep**: Start simple, add complexity incrementally\n2. **No Baseline**: Always compare against a simple baseline\n3. **Overfitting to Test Cases**: Ensure diverse scenario coverage\n4. **Ignoring Edge Cases**: Test failure modes and boundary conditions\n5. **Poor Visualization**: Invest in good visualization early\n6. **Last-Minute Integration**: Integrate continuously, not at the end\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Presentation Guidelines\n\n### 5.1 Presentation Structure (10-15 minutes)\n\n**Slide 1: Title** (30 seconds)\n- Project title\n- Team members\n- Date\n\n**Slides 2-3: Introduction & Motivation** (2 minutes)\n- What problem are you solving?\n- Why is it important?\n- Real-world applications\n- Your specific contribution\n\n**Slides 4-5: Background & Related Work** (2 minutes)\n- Key concepts and terminology\n- Existing approaches\n- How your work differs/improves\n\n**Slides 6-8: Technical Approach** (4 minutes)\n- System architecture diagram\n- Key algorithms and methods\n- Design decisions and trade-offs\n- Implementation challenges\n\n**Slides 9-11: Results** (4 minutes)\n- Experimental setup\n- Quantitative results (metrics, tables, graphs)\n- Qualitative results (visualizations, demo)\n- Comparison with baselines\n\n**Slide 12: Challenges & Lessons Learned** (1 minute)\n- What didn't work initially?\n- How did you overcome obstacles?\n- What would you do differently?\n\n**Slide 13: Future Work** (1 minute)\n- Limitations of current approach\n- Potential improvements\n- Extensions and next steps\n\n**Slide 14: Conclusion** (30 seconds)\n- Summary of contributions\n- Key takeaways\n\n**Slide 15: Q&A** (5 minutes)\n- Thank you slide with contact info\n- Be prepared for questions\n\n### 5.2 Presentation Tips\n\n**Content:**\n- Start with a compelling hook (story, statistic, or problem)\n- Use clear, simple language (avoid jargon when possible)\n- One main idea per slide\n- Balance theory with practical implementation\n- Show, don't just tell (demos, videos, visualizations)\n\n**Slides:**\n- Use large, readable fonts (24pt+ for body text)\n- Limit text (5-7 bullets max per slide, <10 words per bullet)\n- High-quality figures and diagrams\n- Consistent color scheme and formatting\n- Avoid animations unless purposeful\n\n**Delivery:**\n- Practice multiple times (aim for smooth delivery)\n- Make eye contact with audience\n- Speak clearly and at moderate pace\n- Show enthusiasm for your work\n- Time yourself (don't run over)\n\n**Demo:**\n- Have a backup plan (video if live demo fails)\n- Show the most impressive features\n- Explain what's happening as you demonstrate\n- Keep it short and focused\n\n### 5.3 Common Presentation Mistakes\n\n1. **Too much text**: Slides are visual aids, not scripts\n2. **Rushing through results**: This is the most important part!\n3. **Ignoring the audience**: Engage, don't just read slides\n4. **No demo**: Show your system in action\n5. **Running over time**: Practice and time yourself\n6. **Skipping motivation**: Why should anyone care?\n7. **All implementation, no evaluation**: Results matter\n8. **Unclear figures**: Label axes, use legends, explain plots\n\n### 5.4 Q&A Preparation\n\nAnticipate questions like:\n- \"Why did you choose this approach over alternatives?\"\n- \"How does your system handle [edge case]?\"\n- \"What are the computational requirements?\"\n- \"How would this scale to real-world deployment?\"\n- \"What happens when [sensor/component] fails?\"\n- \"How did you validate your results?\"\n\n**Handling Questions:**\n- Listen carefully to the full question\n- Clarify if unsure what's being asked\n- Be honest about limitations\n- It's okay to say \"I don't know, but here's how I'd investigate...\"\n- Bridge to what you do know\n\n---\n\n## 6. Evaluation Rubric\n\nYour project will be evaluated on the following criteria:\n\n### 6.1 Technical Implementation (40 points)\n\n**Algorithm Correctness** (15 points)\n- 13-15: Algorithms implemented correctly, handle edge cases\n- 10-12: Mostly correct, minor bugs or edge case issues\n- 7-9: Functional but significant errors or limitations\n- 4-6: Partially working, major correctness issues\n- 0-3: Not functional or fundamentally flawed\n\n**Code Quality** (10 points)\n- 9-10: Well-structured, documented, follows best practices\n- 7-8: Good organization, some documentation\n- 5-6: Functional but poor organization or documentation\n- 3-4: Difficult to understand or maintain\n- 0-2: Disorganized, undocumented\n\n**Integration & Architecture** (10 points)\n- 9-10: Clean interfaces, modular, well-integrated\n- 7-8: Good integration, minor coupling issues\n- 5-6: Basic integration, some architectural issues\n- 3-4: Poor integration, tight coupling\n- 0-2: Components don't integrate properly\n\n**Technical Depth** (5 points)\n- 5: Goes significantly beyond basic implementation\n- 4: Good depth, some advanced features\n- 3: Meets requirements, limited depth\n- 2: Superficial implementation\n- 0-1: Minimal technical content\n\n### 6.2 Validation & Results (25 points)\n\n**Experimental Design** (10 points)\n- 9-10: Comprehensive testing, appropriate scenarios\n- 7-8: Good test coverage, reasonable scenarios\n- 5-6: Basic testing, limited scenarios\n- 3-4: Minimal testing\n- 0-2: Inadequate or no testing\n\n**Metrics & Analysis** (10 points)\n- 9-10: Appropriate metrics, thorough analysis, meaningful insights\n- 7-8: Good metrics and analysis\n- 5-6: Basic metrics, limited analysis\n- 3-4: Inappropriate or incomplete metrics\n- 0-2: No meaningful evaluation\n\n**Comparison & Baselines** (5 points)\n- 5: Compares with multiple baselines or alternatives\n- 4: Compares with one baseline\n- 3: Limited comparison\n- 2: Minimal comparison\n- 0-1: No comparison\n\n### 6.3 Documentation (20 points)\n\n**Project Report** (10 points)\n- 9-10: Clear, comprehensive, well-written\n- 7-8: Good content, minor issues\n- 5-6: Adequate but lacking detail\n- 3-4: Incomplete or unclear\n- 0-2: Missing or inadequate\n\n**Code Documentation** (5 points)\n- 5: Excellent comments, docstrings, README\n- 4: Good documentation\n- 3: Adequate documentation\n- 2: Minimal documentation\n- 0-1: Little or no documentation\n\n**Visualization** (5 points)\n- 5: Excellent visualizations, clear and informative\n- 4: Good visualizations\n- 3: Adequate visualizations\n- 2: Poor visualizations\n- 0-1: Missing or unhelpful visualizations\n\n### 6.4 Presentation (15 points)\n\n**Content & Clarity** (7 points)\n- 7: Clear, well-organized, covers all key points\n- 5-6: Good content, minor issues\n- 3-4: Adequate but unclear or disorganized\n- 1-2: Poor content or organization\n- 0: Inadequate presentation\n\n**Delivery** (4 points)\n- 4: Confident, engaging, good pace\n- 3: Good delivery\n- 2: Adequate delivery\n- 1: Poor delivery\n- 0: Unprepared\n\n**Demo** (4 points)\n- 4: Excellent demo, showcases key features\n- 3: Good demo\n- 2: Basic demo\n- 1: Poor or non-functional demo\n- 0: No demo\n\n### Total: 100 points\n\n**Grading Scale:**\n- A (90-100): Excellent work, exceeds expectations\n- B (80-89): Good work, meets expectations\n- C (70-79): Acceptable work, meets minimum requirements\n- D (60-69): Below expectations, significant issues\n- F (<60): Unacceptable work\n\n---\n\n## 7. Example Project Template\n\nBelow is a simplified example project to illustrate structure and approach.\n\n### Example: Autonomous Lane Keeping System"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Project Proposal\n\nWrite a 1-page project proposal including:\n\n1. **Title**: Descriptive project name\n2. **Team Members**: Names and roles\n3. **Problem Statement**: What specific challenge are you addressing?\n4. **Motivation**: Why is this important? Real-world applications?\n5. **Approach**: High-level technical approach\n6. **Components**: Which course topics will you integrate? (perception, planning, control, etc.)\n7. **Expected Outcomes**: What will you deliver?\n8. **Success Criteria**: How will you measure success?\n9. **Timeline**: Week-by-week plan\n10. **Resources Needed**: Datasets, libraries, computational resources\n\n**Template:**\n```\nProject Title: [Your Title]\n\nTeam: [Names and Roles]\n\nProblem: \n[2-3 sentences describing the problem]\n\nMotivation:\n[Why is this important? Who benefits?]\n\nTechnical Approach:\n[High-level description of your solution]\n\nCourse Integration:\n- Perception: [specific techniques]\n- Planning: [specific techniques]\n- Control: [specific techniques]\n- [Other modules as applicable]\n\nDeliverables:\n- Working implementation\n- Test results on X scenarios\n- Report and presentation\n\nSuccess Metrics:\n- Metric 1: [e.g., <5cm tracking error]\n- Metric 2: [e.g., 95% success rate]\n\nTimeline:\nWeek 1: [tasks]\nWeek 2: [tasks]\nWeek 3: [tasks]\nWeek 4: [tasks]\n\nResources:\n- KITTI dataset\n- Python libraries: numpy, scipy, matplotlib\n- [Others as needed]\n```\n\n---\n\n### Exercise 2: Architecture Design\n\nDesign the software architecture for your project:\n\n1. **Draw a system diagram** showing all components and data flow\n2. **Define interfaces** between components (inputs/outputs, data formats)\n3. **Identify key algorithms** for each component\n4. **Plan for testability** - how will you unit test each component?\n5. **Consider failure modes** - what can go wrong and how will you handle it?\n\n**Deliverable**: Architecture diagram with annotations\n\n**Example Questions to Answer:**\n- What are the main modules (perception, planning, control, etc.)?\n- What data flows between modules?\n- What is the control loop frequency for each module?\n- How will modules communicate (function calls, message passing, etc.)?\n- Where are the safety-critical components?\n- How will you handle sensor failures or bad data?\n\n---\n\n### Exercise 3: Baseline Implementation\n\nImplement a simple baseline for comparison:\n\n**For Lane Keeping:**\n- Baseline: Simple P controller based on lane center offset\n- Your approach: Stanley controller or MPC\n\n**For Object Tracking:**\n- Baseline: Nearest neighbor association without prediction\n- Your approach: Kalman filter with Hungarian algorithm\n\n**For Path Planning:**\n- Baseline: Straight line to goal\n- Your approach: A*, RRT, or optimization-based\n\n**Requirements:**\n1. Implement baseline in <50 lines of code\n2. Test on same scenarios as your main approach\n3. Compare performance using appropriate metrics\n4. Discuss why your approach is better\n\n**Why Baselines Matter:**\n- Prove your complex approach adds value\n- Understand the problem difficulty\n- Debug your system (if baseline fails, system setup is wrong)\n- Provide context for your results\n\n---"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Example Project: Autonomous Lane Keeping System\n# This is a complete mini-project demonstrating integration of perception, planning, and control\n\n@dataclass\nclass LaneMarkings:\n    \"\"\"Detected lane markings\"\"\"\n    left_coeffs: np.ndarray  # Polynomial coefficients for left lane\n    right_coeffs: np.ndarray  # Polynomial coefficients for right lane\n    confidence: float = 1.0\n    \nclass LaneDetector:\n    \"\"\"Simplified lane detection (perception module)\"\"\"\n    \n    def __init__(self):\n        # In reality, this would use computer vision on camera images\n        # Here we simulate lane detection with known lane geometry\n        pass\n    \n    def detect_lanes(self, vehicle_position, add_noise=True) -> LaneMarkings:\n        \"\"\"\n        Simulate lane detection\n        \n        In a real system, this would:\n        1. Take camera image as input\n        2. Apply edge detection or semantic segmentation\n        3. Fit polynomial curves to lane markings\n        \"\"\"\n        # Simulate curved road: left lane y = 0.001*x^2 - 2\n        # Right lane is 3.5m to the right\n        \n        x = vehicle_position[0]\n        \n        # True lane coefficients (quadratic: ax^2 + bx + c)\n        left_lane_true = np.array([0.001, 0, -2.0])\n        right_lane_true = np.array([0.001, 0, 1.5])\n        \n        # Add measurement noise\n        if add_noise:\n            noise_level = 0.1\n            left_lane = left_lane_true + np.random.randn(3) * noise_level\n            right_lane = right_lane_true + np.random.randn(3) * noise_level\n        else:\n            left_lane = left_lane_true\n            right_lane = right_lane_true\n        \n        return LaneMarkings(left_coeffs=left_lane, right_coeffs=right_lane, confidence=0.9)\n\nclass LaneKeepingPlanner:\n    \"\"\"Path planning to stay in lane center\"\"\"\n    \n    def __init__(self):\n        self.target_speed = 20.0  # m/s\n    \n    def plan_trajectory(self, vehicle_state, lane_markings: LaneMarkings, horizon=50):\n        \"\"\"\n        Plan trajectory to follow lane center\n        \n        Returns: waypoints [(x, y), ...] representing desired path\n        \"\"\"\n        x, y, heading, speed = vehicle_state\n        \n        # Compute lane center at future positions\n        waypoints = []\n        dx = 2.0  # spacing between waypoints\n        \n        for i in range(horizon):\n            future_x = x + i * dx\n            \n            # Evaluate lane polynomial at future_x\n            left_y = np.polyval(lane_markings.left_coeffs, future_x)\n            right_y = np.polyval(lane_markings.right_coeffs, future_x)\n            \n            # Lane center\n            center_y = (left_y + right_y) / 2\n            \n            waypoints.append((future_x, center_y))\n        \n        return np.array(waypoints)\n\nclass StanleyController:\n    \"\"\"Stanley lateral controller (control module)\"\"\"\n    \n    def __init__(self, k=0.5, k_soft=1.0):\n        \"\"\"\n        Stanley controller parameters\n        k: gain for cross-track error\n        k_soft: softening constant for low speeds\n        \"\"\"\n        self.k = k\n        self.k_soft = k_soft\n    \n    def compute_steering(self, vehicle_state, target_path):\n        \"\"\"\n        Compute steering angle using Stanley controller\n        \n        Stanley: δ = θ_e + arctan(k * e_fa / (k_soft + v))\n        where:\n          θ_e = heading error\n          e_fa = cross-track error at front axle\n          v = velocity\n        \"\"\"\n        x, y, heading, speed = vehicle_state\n        \n        # Find closest point on path\n        distances = np.linalg.norm(target_path - np.array([x, y]), axis=1)\n        closest_idx = np.argmin(distances)\n        \n        if closest_idx >= len(target_path) - 1:\n            closest_idx = len(target_path) - 2\n        \n        # Cross-track error (perpendicular distance to path)\n        closest_point = target_path[closest_idx]\n        next_point = target_path[closest_idx + 1]\n        \n        # Path heading at closest point\n        path_heading = np.arctan2(next_point[1] - closest_point[1],\n                                   next_point[0] - closest_point[0])\n        \n        # Heading error\n        heading_error = self.normalize_angle(path_heading - heading)\n        \n        # Cross-track error (signed distance)\n        # Vector from vehicle to closest point\n        dx = closest_point[0] - x\n        dy = closest_point[1] - y\n        \n        # Project onto path normal\n        cross_track_error = -dx * np.sin(path_heading) + dy * np.cos(path_heading)\n        \n        # Stanley control law\n        steering_angle = heading_error + np.arctan2(self.k * cross_track_error, \n                                                      self.k_soft + speed)\n        \n        # Limit steering angle\n        max_steer = np.deg2rad(30)\n        steering_angle = np.clip(steering_angle, -max_steer, max_steer)\n        \n        return steering_angle\n    \n    def normalize_angle(self, angle):\n        \"\"\"Normalize angle to [-pi, pi]\"\"\"\n        while angle > np.pi:\n            angle -= 2 * np.pi\n        while angle < -np.pi:\n            angle += 2 * np.pi\n        return angle\n\nclass VehicleDynamics:\n    \"\"\"Simple kinematic bicycle model\"\"\"\n    \n    def __init__(self, wheelbase=2.5):\n        self.wheelbase = wheelbase  # Distance between front and rear axles\n    \n    def update(self, state, steering_angle, speed, dt):\n        \"\"\"\n        Update vehicle state using kinematic bicycle model\n        \n        state: [x, y, heading, speed]\n        \"\"\"\n        x, y, heading, v = state\n        \n        # Kinematic bicycle model\n        x_new = x + v * np.cos(heading) * dt\n        y_new = y + v * np.sin(heading) * dt\n        heading_new = heading + (v / self.wheelbase) * np.tan(steering_angle) * dt\n        \n        # Normalize heading\n        while heading_new > np.pi:\n            heading_new -= 2 * np.pi\n        while heading_new < -np.pi:\n            heading_new += 2 * np.pi\n        \n        return np.array([x_new, y_new, heading_new, speed])\n\n# Integrated Simulation\ndef demonstrate_lane_keeping():\n    \"\"\"Complete lane keeping system demonstration\"\"\"\n    print(\"=== Autonomous Lane Keeping System ===\\n\")\n    \n    # Initialize modules\n    detector = LaneDetector()\n    planner = LaneKeepingPlanner()\n    controller = StanleyController(k=0.5)\n    vehicle = VehicleDynamics(wheelbase=2.5)\n    \n    # Initial state: [x, y, heading, speed]\n    # Start offset from lane center to show correction\n    state = np.array([0.0, 0.5, 0.0, 15.0])  # 0.5m offset from center\n    \n    # Simulation parameters\n    dt = 0.1\n    duration = 10.0\n    steps = int(duration / dt)\n    \n    # History for visualization\n    history = {\n        'x': [], 'y': [], 'heading': [],\n        'steering': [], 'cross_track_error': []\n    }\n    \n    print(\"Running simulation...\")\n    \n    for step in range(steps):\n        # 1. PERCEPTION: Detect lanes\n        lane_markings = detector.detect_lanes(state[:2])\n        \n        # 2. PLANNING: Compute target path\n        target_path = planner.plan_trajectory(state, lane_markings)\n        \n        # 3. CONTROL: Compute steering command\n        steering_angle = controller.compute_steering(state, target_path)\n        \n        # 4. DYNAMICS: Update vehicle state\n        state = vehicle.update(state, steering_angle, state[3], dt)\n        \n        # Record history\n        history['x'].append(state[0])\n        history['y'].append(state[1])\n        history['heading'].append(state[2])\n        history['steering'].append(steering_angle)\n        \n        # Compute cross-track error for analysis\n        x, y = state[0], state[1]\n        left_y = np.polyval(lane_markings.left_coeffs, x)\n        right_y = np.polyval(lane_markings.right_coeffs, x)\n        center_y = (left_y + right_y) / 2\n        cross_track_error = y - center_y\n        history['cross_track_error'].append(cross_track_error)\n    \n    # Analyze results\n    max_error = np.max(np.abs(history['cross_track_error']))\n    mean_error = np.mean(np.abs(history['cross_track_error']))\n    \n    print(f\"Results:\")\n    print(f\"  Max cross-track error: {max_error:.3f} m\")\n    print(f\"  Mean absolute error: {mean_error:.3f} m\")\n    print(f\"  Final cross-track error: {history['cross_track_error'][-1]:.3f} m\")\n    \n    # Visualization\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Plot 1: Bird's eye view of trajectory\n    ax1 = axes[0, 0]\n    \n    # Plot lane boundaries\n    x_range = np.linspace(0, max(history['x']), 200)\n    left_lane_true = 0.001 * x_range**2 - 2.0\n    right_lane_true = 0.001 * x_range**2 + 1.5\n    center_lane = (left_lane_true + right_lane_true) / 2\n    \n    ax1.plot(x_range, left_lane_true, 'y--', linewidth=2, label='Left Lane', alpha=0.7)\n    ax1.plot(x_range, right_lane_true, 'y--', linewidth=2, label='Right Lane', alpha=0.7)\n    ax1.plot(x_range, center_lane, 'g:', linewidth=1.5, label='Lane Center', alpha=0.5)\n    \n    # Plot vehicle trajectory\n    ax1.plot(history['x'], history['y'], 'b-', linewidth=2, label='Vehicle Path')\n    ax1.plot(history['x'][0], history['y'][0], 'go', markersize=10, label='Start')\n    ax1.plot(history['x'][-1], history['y'][-1], 'ro', markersize=10, label='End')\n    \n    ax1.set_xlabel('X Position (m)', fontsize=11)\n    ax1.set_ylabel('Y Position (m)', fontsize=11)\n    ax1.set_title('Vehicle Trajectory (Bird\\'s Eye View)', fontsize=13, fontweight='bold')\n    ax1.legend(fontsize=9)\n    ax1.grid(True, alpha=0.3)\n    ax1.set_aspect('equal')\n    \n    # Plot 2: Cross-track error over time\n    ax2 = axes[0, 1]\n    time_array = np.arange(len(history['cross_track_error'])) * dt\n    ax2.plot(time_array, history['cross_track_error'], 'b-', linewidth=2)\n    ax2.axhline(y=0, color='g', linestyle='--', linewidth=1.5, alpha=0.7, label='Lane Center')\n    ax2.axhline(y=0.5, color='r', linestyle=':', linewidth=1, alpha=0.5, label='Lane Boundary')\n    ax2.axhline(y=-0.5, color='r', linestyle=':', linewidth=1, alpha=0.5)\n    ax2.set_xlabel('Time (s)', fontsize=11)\n    ax2.set_ylabel('Cross-Track Error (m)', fontsize=11)\n    ax2.set_title('Lane Keeping Performance', fontsize=13, fontweight='bold')\n    ax2.legend(fontsize=9)\n    ax2.grid(True, alpha=0.3)\n    \n    # Plot 3: Steering angle over time\n    ax3 = axes[1, 0]\n    ax3.plot(time_array, np.rad2deg(history['steering']), 'r-', linewidth=2)\n    ax3.set_xlabel('Time (s)', fontsize=11)\n    ax3.set_ylabel('Steering Angle (degrees)', fontsize=11)\n    ax3.set_title('Control Commands', fontsize=13, fontweight='bold')\n    ax3.grid(True, alpha=0.3)\n    \n    # Plot 4: System architecture\n    ax4 = axes[1, 1]\n    ax4.axis('off')\n    \n    # Draw system architecture\n    components = [\n        ('Camera', (0.5, 0.9)),\n        ('Lane Detector', (0.5, 0.75)),\n        ('Path Planner', (0.5, 0.6)),\n        ('Stanley Controller', (0.5, 0.45)),\n        ('Vehicle Dynamics', (0.5, 0.3)),\n        ('Actuators', (0.5, 0.15))\n    ]\n    \n    for i, (name, pos) in enumerate(components):\n        bbox = dict(boxstyle='round,pad=0.5', facecolor='lightblue', edgecolor='black', linewidth=2)\n        ax4.text(pos[0], pos[1], name, ha='center', va='center', fontsize=11,\n                fontweight='bold', bbox=bbox, transform=ax4.transAxes)\n        \n        # Draw arrows\n        if i < len(components) - 1:\n            ax4.annotate('', xy=(pos[0], pos[1] - 0.05), xytext=(pos[0], pos[1] - 0.10),\n                        arrowprops=dict(arrowstyle='->', lw=2, color='black'),\n                        transform=ax4.transAxes)\n    \n    ax4.set_title('System Architecture', fontsize=13, fontweight='bold')\n    ax4.set_xlim(0, 1)\n    ax4.set_ylim(0, 1)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"✓ Lane keeping system successfully demonstrated\")\n    print(\"=\"*50)\n\n# Run the demonstration\ndemonstrate_lane_keeping()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## References\n\n### Textbooks & Comprehensive Resources\n\n1. **Thrun, S., Burgard, W., & Fox, D. (2005)**. *Probabilistic Robotics*. MIT Press.\n   - Foundational text for localization, mapping, and probabilistic methods\n\n2. **Lavalle, S. M. (2006)**. *Planning Algorithms*. Cambridge University Press.\n   - Comprehensive coverage of path planning and motion planning\n\n3. **Rajamani, R. (2011)**. *Vehicle Dynamics and Control*. Springer.\n   - Authoritative reference on vehicle dynamics and control systems\n\n4. **Paden, B., Čáp, M., Yong, S. Z., Yershov, D., & Frazzoli, E. (2016)**. \"A survey of motion planning and control techniques for self-driving urban vehicles.\" *IEEE Transactions on Intelligent Vehicles, 1*(1), 33-55.\n   - Excellent survey of autonomous driving techniques\n\n5. **Badue, C., Guidolini, R., Carneiro, R. V., Azevedo, P., Cardoso, V. B., Forechi, A., ... & De Souza, A. F. (2021)**. \"Self-driving cars: A survey.\" *Expert Systems with Applications, 165*, 113816.\n   - Comprehensive recent survey of self-driving car technologies\n\n### Datasets for Projects\n\n6. **Geiger, A., Lenz, P., & Urtasun, R. (2012)**. \"Are we ready for autonomous driving? The KITTI vision benchmark suite.\" *CVPR*.\n   - KITTI: Standard benchmark for perception tasks\n\n7. **Caesar, H., Bankiti, V., Lang, A. H., Vora, S., Liong, V. E., Xu, Q., ... & Beijbom, O. (2020)**. \"nuScenes: A multimodal dataset for autonomous driving.\" *CVPR*.\n   - nuScenes: Full sensor suite dataset with rich annotations\n\n8. **Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., ... & Anguelov, D. (2020)**. \"Scalability in perception for autonomous driving: Waymo open dataset.\" *CVPR*.\n   - Waymo: Large-scale diverse dataset\n\n9. **Udacity (2016)**. \"Self-Driving Car Dataset.\"\n   - Open dataset with camera, LiDAR, and radar data\n\n### Perception & Computer Vision\n\n10. **Ren, S., He, K., Girshick, R., & Sun, J. (2015)**. \"Faster R-CNN: Towards real-time object detection with region proposal networks.\" *NeurIPS*.\n\n11. **Redmon, J., & Farhadi, A. (2018)**. \"YOLOv3: An incremental improvement.\" *arXiv preprint arXiv:1804.02767*.\n\n12. **Zhou, Y., & Tuzel, O. (2018)**. \"VoxelNet: End-to-end learning for point cloud based 3D object detection.\" *CVPR*.\n\n13. **Bewley, A., Ge, Z., Ott, L., Ramos, F., & Upcroft, B. (2016)**. \"Simple online and realtime tracking.\" *ICIP*.\n    - SORT: Simple multi-object tracking\n\n### Localization & Mapping\n\n14. **Durrant-Whyte, H., & Bailey, T. (2006)**. \"Simultaneous localization and mapping: part I.\" *IEEE Robotics & Automation Magazine, 13*(2), 99-110.\n\n15. **Kummerle, R., Grisetti, G., Strasdat, H., Konolige, K., & Burgard, W. (2011)**. \"g2o: A general framework for graph optimization.\" *ICRA*.\n\n16. **Levinson, J., Montemerlo, M., & Thrun, S. (2007)**. \"Map-based precision vehicle localization in urban environments.\" *RSS*.\n\n### Path Planning\n\n17. **LaValle, S. M., & Kuffner Jr, J. J. (2001)**. \"Randomized kinodynamic planning.\" *The International Journal of Robotics Research, 20*(5), 378-400.\n    - RRT: Rapidly-exploring Random Trees\n\n18. **Karaman, S., & Frazzoli, E. (2011)**. \"Sampling-based algorithms for optimal motion planning.\" *The International Journal of Robotics Research, 30*(7), 846-894.\n    - RRT*: Optimal variant of RRT\n\n19. **Dolgov, D., Thrun, S., Montemerlo, M., & Diebel, J. (2010)**. \"Path planning for autonomous vehicles in unknown semi-structured environments.\" *The International Journal of Robotics Research, 29*(5), 485-501.\n\n20. **Pivtoraiko, M., Knepper, R. A., & Kelly, A. (2009)**. \"Differentially constrained mobile robot motion planning in state lattices.\" *Journal of Field Robotics, 26*(3), 308-333.\n\n### Control\n\n21. **Snider, J. M. (2009)**. \"Automatic steering methods for autonomous automobile path tracking.\" *Robotics Institute, Pittsburgh, PA, Tech. Rep. CMU-RITR-09-08*.\n    - Comparison of Pure Pursuit, Stanley, and MPC\n\n22. **Falcone, P., Borrelli, F., Asgari, J., Tseng, H. E., & Hrovat, D. (2007)**. \"Predictive active steering control for autonomous vehicle systems.\" *IEEE Transactions on Control Systems Technology, 15*(3), 566-580.\n\n23. **Laurense, V. A., Goh, J. Y., & Gerdes, J. C. (2017)**. \"Path-tracking for autonomous vehicles at the limit of friction.\" *ACC*.\n\n### Behavior Planning & Decision Making\n\n24. **Ulbrich, S., & Maurer, M. (2013)**. \"Probabilistic online POMDP decision making for lane changes in fully automated driving.\" *ITSC*.\n\n25. **Schwarting, W., Alonso-Mora, J., & Rus, D. (2018)**. \"Planning and decision-making for autonomous vehicles.\" *Annual Review of Control, Robotics, and Autonomous Systems, 1*, 187-210.\n\n26. **Althoff, M., Stursberg, O., & Buss, M. (2009)**. \"Model-based probabilistic collision detection in autonomous driving.\" *IEEE Transactions on Intelligent Transportation Systems, 10*(2), 299-310.\n\n### Prediction & Interaction\n\n27. **Alahi, A., Goel, K., Ramanathan, V., Robicquet, A., Fei-Fei, L., & Savarese, S. (2016)**. \"Social LSTM: Human trajectory prediction in crowded spaces.\" *CVPR*.\n\n28. **Schmerling, E., Leung, K., Vollprecht, W., & Pavone, M. (2018)**. \"Multimodal probabilistic model-based planning for human-robot interaction.\" *ICRA*.\n\n29. **Fisac, J. F., Bronstein, E., Stefansson, E., Sadigh, D., Sastry, S. S., & Dragan, A. D. (2019)**. \"Hierarchical game-theoretic planning for autonomous vehicles.\" *ICRA*.\n\n### Safety & Verification\n\n30. **Koopman, P., & Wagner, M. (2016)**. \"Challenges in autonomous vehicle testing and validation.\" *SAE International Journal of Transportation Safety, 4*(1), 15-24.\n\n31. **ISO 26262 (2018)**. \"Road vehicles — Functional safety.\" *International Organization for Standardization*.\n\n32. **Kalra, N., & Paddock, S. M. (2016)**. \"Driving to safety: How many miles of driving would it take to demonstrate autonomous vehicle reliability?\" *Transportation Research Part A: Policy and Practice, 94*, 182-193.\n\n33. **Tian, Y., Pei, K., Jana, S., & Ray, B. (2018)**. \"DeepTest: Automated testing of deep-neural-network-driven autonomous cars.\" *ICSE*.\n\n### Simulation & Tools\n\n34. **Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., & Koltun, V. (2017)**. \"CARLA: An open urban driving simulator.\" *CoRL*.\n\n35. **Shah, S., Dey, D., Lovett, C., & Kapoor, A. (2018)**. \"AirSim: High-fidelity visual and physical simulation for autonomous vehicles.\" *Field and Service Robotics*.\n\n36. **Koenig, N., & Howard, A. (2004)**. \"Design and use paradigms for Gazebo, an open-source multi-robot simulator.\" *IROS*.\n\n37. **Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., ... & Ng, A. Y. (2009)**. \"ROS: an open-source Robot Operating System.\" *ICRA Workshop*.\n\n### End-to-End Learning\n\n38. **Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., ... & Zieba, K. (2016)**. \"End to end learning for self-driving cars.\" *arXiv preprint arXiv:1604.07316*.\n\n39. **Chen, D., Zhou, B., Koltun, V., & Krähenbühl, P. (2020)**. \"Learning by cheating.\" *CoRL*.\n\n40. **Codevilla, F., Müller, M., López, A., Koltun, V., & Dosovitskiy, A. (2018)**. \"End-to-end driving via conditional imitation learning.\" *ICRA*.\n\n### Project Management & Software Engineering\n\n41. **Spinellis, D. (2005)**. *Code Quality: The Open Source Perspective*. Addison-Wesley.\n\n42. **Martin, R. C. (2008)**. *Clean Code: A Handbook of Agile Software Craftsmanship*. Prentice Hall.\n\n43. **Hunt, A., & Thomas, D. (1999)**. *The Pragmatic Programmer: From Journeyman to Master*. Addison-Wesley.\n\n### Online Resources\n\n44. **Udacity Self-Driving Car Nanodegree**: https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013\n\n45. **MIT 6.S094: Deep Learning for Self-Driving Cars**: https://selfdrivingcars.mit.edu/\n\n46. **Coursera: Self-Driving Cars Specialization**: https://www.coursera.org/specializations/self-driving-cars\n\n47. **Apollo Auto Open Platform**: https://apollo.auto/\n\n48. **Autoware Foundation**: https://www.autoware.org/\n\n49. **Papers with Code - Autonomous Driving**: https://paperswithcode.com/task/autonomous-driving\n\n50. **arXiv.org (cs.RO - Robotics)**: https://arxiv.org/list/cs.RO/recent\n\n---\n\n## Course Conclusion\n\nCongratulations on completing this autonomous driving course! Over the past 15 weeks, you've explored:\n\n**Technical Foundations:**\n- Perception: Computer vision, deep learning, sensor fusion\n- Localization: SLAM, particle filters, Kalman filters\n- Planning: Path planning (A*, RRT), behavior planning, motion planning\n- Control: PID, MPC, LQR, vehicle dynamics\n\n**Advanced Topics:**\n- Prediction and multi-agent interaction\n- Functional safety and redundancy\n- Testing, validation, and ethics\n- Connected vehicles and cybersecurity\n\n**Practical Skills:**\n- Implementing algorithms from scratch\n- Working with simulations and visualizations\n- Evaluating system performance\n- Integrating multiple components\n\n### Next Steps\n\n**Continue Learning:**\n- Implement projects using real datasets (KITTI, nuScenes)\n- Explore deep learning for perception (YOLO, PointNet, transformers)\n- Study advanced planning (POMDP, game theory)\n- Learn ROS for robot system integration\n\n**Stay Current:**\n- Follow top conferences: CVPR, ICRA, IROS, CoRL, ITSC\n- Read company blogs: Waymo, Tesla, Cruise, Zoox\n- Join communities: r/SelfDrivingCars, autonomous vehicle meetups\n\n**Build Projects:**\n- Contribute to open source (Autoware, Apollo)\n- Participate in competitions (CARLA Challenge, nuScenes Challenge)\n- Build your own autonomous RC car\n- Develop perception/planning tools\n\n**Career Paths:**\n- Perception Engineer\n- Motion Planning Engineer  \n- Controls Engineer\n- Safety/Validation Engineer\n- Research Scientist\n- Robotics Software Engineer\n\n### Final Thoughts\n\nAutonomous driving is one of the most challenging and rewarding fields in robotics and AI. It requires deep technical knowledge across perception, planning, control, machine learning, and systems engineering.\n\nThe technology is rapidly evolving, but the fundamentals you've learned—probabilistic reasoning, optimization, control theory, and system integration—will remain relevant.\n\n**Remember:**\n- Safety is paramount\n- Test thoroughly and rigorously  \n- Consider edge cases and failure modes\n- Communicate clearly about limitations\n- Stay humble and keep learning\n\nGood luck with your final projects and future endeavors in autonomous systems!\n\n---"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}