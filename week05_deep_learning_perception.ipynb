{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 5: Deep Learning for Perception\n\n## Module II: Perception & Localization\n\n### Topics Covered\n\n- Neural Networks Fundamentals (CNNs)\n- Object Detection (YOLO, SSD, Faster R-CNN)\n- Semantic Segmentation (FCN, U-Net, DeepLab)\n- Instance Segmentation\n- Model Training and Evaluation\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. Understand convolutional neural network (CNN) architectures for computer vision\n2. Implement and evaluate object detection models for autonomous driving\n3. Apply semantic segmentation for scene understanding\n4. Distinguish between classification, detection, and segmentation tasks\n5. Evaluate perception models using standard metrics (mAP, IoU, etc.)\n6. Understand trade-offs between accuracy and inference speed\n7. Apply data augmentation and training strategies for robust models\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\nImport required libraries for deep learning and visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle, Circle, Polygon\nfrom matplotlib.colors import ListedColormap\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Plotting configuration\nplt.rcParams['figure.figsize'] = (14, 8)\nplt.rcParams['font.size'] = 10\n\nprint(\"Libraries loaded successfully!\")\nprint(\"NumPy version:\", np.__version__)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Neural Networks Fundamentals\n\n**Deep Learning** has revolutionized perception for autonomous vehicles, enabling robust detection and classification of objects, lanes, and road signs.\n\n### Why Deep Learning for Autonomous Driving?\n\n**Traditional Computer Vision** (hand-crafted features):\n- HOG (Histogram of Oriented Gradients)\n- SIFT (Scale-Invariant Feature Transform)\n- Haar cascades\n\n**Limitations**:\n- ❌ Brittle to lighting/weather variations\n- ❌ Requires manual feature engineering\n- ❌ Poor generalization to novel scenarios\n\n**Deep Learning**:\n- ✅ Learns features automatically from data\n- ✅ Robust to variations (lighting, occlusion, etc.)\n- ✅ State-of-the-art performance on all perception tasks\n\n---\n\n### Convolutional Neural Networks (CNNs)\n\n**CNNs** are specialized neural networks for processing grid-like data (images).\n\n#### **Key Components**\n\n##### **1. Convolutional Layer**\n\nApplies learnable filters to extract features:\n\n$$y[i, j] = \\sum_{m} \\sum_{n} x[i+m, j+n] \\cdot w[m, n] + b$$\n\nWhere:\n- **x**: Input feature map\n- **w**: Convolutional kernel (weights)\n- **b**: Bias term\n- **y**: Output feature map\n\n**Example**: 3×3 kernel convolving over a 5×5 image\n\n**Key properties**:\n- **Local connectivity**: Each neuron sees only a small region\n- **Parameter sharing**: Same kernel applied across entire image\n- **Translation invariance**: Detects features regardless of position\n\n##### **2. Pooling Layer**\n\nDownsamples feature maps to reduce computation and improve invariance.\n\n**Max Pooling** (2×2 with stride 2):\n$$y[i, j] = \\max(x[2i:2i+2, 2j:2j+2])$$\n\n**Average Pooling**:\n$$y[i, j] = \\frac{1}{4} \\sum_{m=0}^{1} \\sum_{n=0}^{1} x[2i+m, 2j+n]$$\n\n**Effect**: Reduces spatial dimensions by factor of 2\n\n##### **3. Activation Functions**\n\nIntroduce non-linearity to learn complex patterns.\n\n**ReLU (Rectified Linear Unit)** - Most common:\n$$f(x) = \\max(0, x)$$\n\n**Sigmoid** - Output layer for binary classification:\n$$f(x) = \\frac{1}{1 + e^{-x}}$$\n\n**Softmax** - Output layer for multi-class:\n$$f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n\n##### **4. Fully Connected Layer**\n\nStandard dense layer connecting all inputs to all outputs:\n$$y = Wx + b$$\n\n---\n\n### CNN Architecture Example: LeNet-5\n\n```\nInput (32×32×1)\n    ↓\nConv1 (5×5, 6 filters) → 28×28×6\n    ↓\nMaxPool (2×2) → 14×14×6\n    ↓\nConv2 (5×5, 16 filters) → 10×10×16\n    ↓\nMaxPool (2×2) → 5×5×16\n    ↓\nFlatten → 400\n    ↓\nFC1 → 120\n    ↓\nFC2 → 84\n    ↓\nOutput → 10 classes\n```\n\n---\n\n### Modern CNN Architectures\n\n| Architecture | Year | Key Innovation | Parameters | ImageNet Top-5 |\n|--------------|------|----------------|------------|-----------------|\n| **AlexNet** | 2012 | Deep CNN + ReLU + Dropout | 61M | 84.6% |\n| **VGG-16** | 2014 | Deeper (16 layers), 3×3 kernels | 138M | 92.7% |\n| **ResNet-50** | 2015 | Residual connections (skip) | 25M | 96.4% |\n| **MobileNetV2** | 2018 | Efficient for mobile/edge | 3.5M | 94.3% |\n| **EfficientNet** | 2019 | Compound scaling | 5-66M | 97.1% |\n\n**ResNet Residual Block**:\n```\nx → Conv → BN → ReLU → Conv → BN → (+) → ReLU\n↓                                    ↑\n└────────────────────────────────────┘ (skip connection)\n```\n\n**Key insight**: Skip connections allow training very deep networks (100+ layers) by addressing vanishing gradients.\n\n---\n\n### Transfer Learning\n\n**Problem**: Training from scratch requires millions of labeled images.\n\n**Solution**: Use pre-trained weights from ImageNet (1.2M images, 1000 classes).\n\n**Approach**:\n1. **Feature extraction**: Freeze early layers, train only final layers\n2. **Fine-tuning**: Unfreeze all layers, train with small learning rate\n\n**Benefits**:\n- ✅ Faster convergence (10-100× fewer iterations)\n- ✅ Better performance with limited data\n- ✅ Reduced computational cost\n\n---\n\n### Training Deep Networks\n\n#### **Loss Functions**\n\n**Classification** (Cross-Entropy Loss):\n$$L = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$\n\nWhere:\n- **y**: True label (one-hot encoded)\n- **ŷ**: Predicted probability\n- **C**: Number of classes\n\n**Regression** (Mean Squared Error):\n$$L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n\n#### **Optimization**\n\n**Stochastic Gradient Descent (SGD)**:\n$$\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)$$\n\n**Adam** (Adaptive Moment Estimation) - Most popular:\n- Combines momentum and adaptive learning rates\n- Robust to hyperparameter choices\n\n**Learning rate scheduling**:\n- **Step decay**: Reduce LR by factor every N epochs\n- **Cosine annealing**: Smooth reduction following cosine curve\n\n#### **Regularization**\n\nPrevents overfitting:\n\n1. **Dropout**: Randomly drop neurons during training (p = 0.5)\n2. **L2 regularization**: Add penalty $\\lambda \\|\\theta\\|^2$ to loss\n3. **Data augmentation**: Artificially increase dataset size\n4. **Batch normalization**: Normalize activations per mini-batch\n\n#### **Data Augmentation for Autonomous Driving**\n\nCritical for robustness:\n\n- **Geometric**: Flip, rotate, crop, scale\n- **Photometric**: Brightness, contrast, saturation, hue\n- **Weather simulation**: Rain, fog, snow overlays\n- **Occlusion**: Random erasing/cutout\n\n---\n\n### Evaluation Metrics\n\n#### **Classification Metrics**\n\n**Confusion Matrix**:\n```\n                Predicted\n              Pos    Neg\nActual Pos    TP     FN\n       Neg    FP     TN\n```\n\n**Precision**: Of predicted positives, how many are correct?\n$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n\n**Recall**: Of actual positives, how many did we find?\n$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n\n**F1 Score**: Harmonic mean of precision and recall:\n$$F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n\n**Accuracy**: Overall correctness:\n$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n\n---\n\n### Computational Requirements\n\n| Model | Parameters | FLOPs | Inference (ms) | Use Case |\n|-------|------------|-------|----------------|----------|\n| MobileNetV2 | 3.5M | 300M | 5-10 | Mobile/embedded |\n| ResNet-50 | 25M | 4B | 20-30 | Edge compute |\n| EfficientNet-B7 | 66M | 37B | 100+ | Cloud/datacenter |\n\n**Real-time constraint**: Autonomous vehicles need **<50ms** inference for 20 Hz perception.\n\n**Hardware accelerators**:\n- **GPU**: NVIDIA Drive AGX (200+ TOPS)\n- **TPU**: Google Edge TPU\n- **ASIC**: Tesla FSD chip (144 TOPS)\n\n---\n\n### Advantages & Limitations\n\n**Advantages**:\n- ✅ State-of-the-art accuracy\n- ✅ End-to-end learning from raw pixels\n- ✅ Robust to variations\n- ✅ Continual improvement with more data\n\n**Limitations**:\n- ❌ Requires large labeled datasets (100k+ images)\n- ❌ Computationally expensive\n- ❌ \"Black box\" - hard to interpret decisions\n- ❌ Vulnerable to adversarial attacks\n- ❌ Distribution shift (train vs. deployment)\n\n**Mitigation**:\n- Active learning and data curation\n- Model compression (quantization, pruning)\n- Uncertainty estimation\n- Multi-sensor fusion for redundancy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CNN Operations Visualization\n\n# Simulate convolution operation\ndef conv2d(image, kernel):\n    \"\"\"\n    Simple 2D convolution (without padding).\n    \n    Args:\n        image: Input 2D array\n        kernel: Convolution kernel\n    \n    Returns:\n        Feature map after convolution\n    \"\"\"\n    h_out = image.shape[0] - kernel.shape[0] + 1\n    w_out = image.shape[1] - kernel.shape[1] + 1\n    output = np.zeros((h_out, w_out))\n    \n    for i in range(h_out):\n        for j in range(w_out):\n            output[i, j] = np.sum(image[i:i+kernel.shape[0], j:j+kernel.shape[1]] * kernel)\n    \n    return output\n\ndef relu(x):\n    \"\"\"ReLU activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef max_pool2d(image, pool_size=2):\n    \"\"\"\n    Max pooling with stride = pool_size.\n    \n    Args:\n        image: Input 2D array\n        pool_size: Size of pooling window\n    \n    Returns:\n        Downsampled feature map\n    \"\"\"\n    h_out = image.shape[0] // pool_size\n    w_out = image.shape[1] // pool_size\n    output = np.zeros((h_out, w_out))\n    \n    for i in range(h_out):\n        for j in range(w_out):\n            output[i, j] = np.max(image[i*pool_size:(i+1)*pool_size, \n                                       j*pool_size:(j+1)*pool_size])\n    \n    return output\n\n# Create synthetic input image (8x8)\ninput_image = np.array([\n    [0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 1, 1, 1, 1, 1, 1, 0],\n    [0, 1, 0, 0, 0, 0, 1, 0],\n    [0, 1, 0, 0, 0, 0, 1, 0],\n    [0, 1, 0, 0, 0, 0, 1, 0],\n    [0, 1, 0, 0, 0, 0, 1, 0],\n    [0, 1, 1, 1, 1, 1, 1, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0]\n])\n\n# Define edge detection kernels\nvertical_edge_kernel = np.array([\n    [-1, 0, 1],\n    [-1, 0, 1],\n    [-1, 0, 1]\n])\n\nhorizontal_edge_kernel = np.array([\n    [-1, -1, -1],\n    [ 0,  0,  0],\n    [ 1,  1,  1]\n])\n\n# Apply convolution\nvertical_edges = conv2d(input_image, vertical_edge_kernel)\nhorizontal_edges = conv2d(input_image, horizontal_edge_kernel)\n\n# Apply ReLU\nvertical_edges_relu = relu(vertical_edges)\nhorizontal_edges_relu = relu(horizontal_edges)\n\n# Apply max pooling\nvertical_pooled = max_pool2d(vertical_edges_relu)\nhorizontal_pooled = max_pool2d(horizontal_edges_relu)\n\n# Visualization\nfig = plt.figure(figsize=(16, 10))\n\n# Row 1: Input and Kernels\nax1 = fig.add_subplot(3, 4, 1)\nim1 = ax1.imshow(input_image, cmap='gray', vmin=0, vmax=1)\nax1.set_title('Input Image (8×8)', fontsize=12, fontweight='bold')\nax1.axis('off')\nplt.colorbar(im1, ax=ax1, fraction=0.046)\n\nax2 = fig.add_subplot(3, 4, 2)\nim2 = ax2.imshow(vertical_edge_kernel, cmap='RdBu_r', vmin=-1, vmax=1)\nax2.set_title('Vertical Edge Kernel (3×3)', fontsize=12, fontweight='bold')\nax2.axis('off')\nfor i in range(3):\n    for j in range(3):\n        ax2.text(j, i, f'{vertical_edge_kernel[i,j]:.0f}', \n                ha='center', va='center', color='white', fontweight='bold')\nplt.colorbar(im2, ax=ax2, fraction=0.046)\n\nax3 = fig.add_subplot(3, 4, 3)\nim3 = ax3.imshow(horizontal_edge_kernel, cmap='RdBu_r', vmin=-1, vmax=1)\nax3.set_title('Horizontal Edge Kernel (3×3)', fontsize=12, fontweight='bold')\nax3.axis('off')\nfor i in range(3):\n    for j in range(3):\n        ax3.text(j, i, f'{horizontal_edge_kernel[i,j]:.0f}', \n                ha='center', va='center', color='white', fontweight='bold')\nplt.colorbar(im3, ax=ax3, fraction=0.046)\n\n# Row 2: After Convolution\nax4 = fig.add_subplot(3, 4, 5)\nim4 = ax4.imshow(vertical_edges, cmap='RdBu_r')\nax4.set_title('After Conv (Vertical)\\n(6×6)', fontsize=12, fontweight='bold')\nax4.axis('off')\nplt.colorbar(im4, ax=ax4, fraction=0.046)\n\nax5 = fig.add_subplot(3, 4, 6)\nim5 = ax5.imshow(horizontal_edges, cmap='RdBu_r')\nax5.set_title('After Conv (Horizontal)\\n(6×6)', fontsize=12, fontweight='bold')\nax5.axis('off')\nplt.colorbar(im5, ax=ax5, fraction=0.046)\n\n# Row 3: After ReLU\nax6 = fig.add_subplot(3, 4, 9)\nim6 = ax6.imshow(vertical_edges_relu, cmap='viridis')\nax6.set_title('After ReLU (Vertical)\\n(6×6)', fontsize=12, fontweight='bold')\nax6.axis('off')\nplt.colorbar(im6, ax=ax6, fraction=0.046)\n\nax7 = fig.add_subplot(3, 4, 10)\nim7 = ax7.imshow(horizontal_edges_relu, cmap='viridis')\nax7.set_title('After ReLU (Horizontal)\\n(6×6)', fontsize=12, fontweight='bold')\nax7.axis('off')\nplt.colorbar(im7, ax=ax7, fraction=0.046)\n\n# Row 3: After Max Pooling\nax8 = fig.add_subplot(3, 4, 11)\nim8 = ax8.imshow(vertical_pooled, cmap='viridis')\nax8.set_title('After MaxPool (Vertical)\\n(3×3)', fontsize=12, fontweight='bold')\nax8.axis('off')\nplt.colorbar(im8, ax=ax8, fraction=0.046)\n\nax9 = fig.add_subplot(3, 4, 12)\nim9 = ax9.imshow(horizontal_pooled, cmap='viridis')\nax9.set_title('After MaxPool (Horizontal)\\n(3×3)', fontsize=12, fontweight='bold')\nax9.axis('off')\nplt.colorbar(im9, ax=ax9, fraction=0.046)\n\n# Add pipeline diagram\nax10 = fig.add_subplot(3, 4, 4)\nax10.axis('off')\nax10.text(0.5, 0.8, 'CNN Pipeline:', ha='center', fontsize=14, fontweight='bold')\nax10.text(0.5, 0.6, '1. Convolution\\n   (Feature extraction)', ha='center', fontsize=10)\nax10.text(0.5, 0.4, '2. ReLU\\n   (Non-linearity)', ha='center', fontsize=10)\nax10.text(0.5, 0.2, '3. Pooling\\n   (Downsampling)', ha='center', fontsize=10)\nax10.set_xlim([0, 1])\nax10.set_ylim([0, 1])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"=\" * 70)\nprint(\"CNN OPERATION SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"Input shape:           {input_image.shape}\")\nprint(f\"Kernel shape:          {vertical_edge_kernel.shape}\")\nprint(f\"After Conv:            {vertical_edges.shape}\")\nprint(f\"After ReLU:            {vertical_edges_relu.shape}\")\nprint(f\"After MaxPool(2×2):    {vertical_pooled.shape}\")\nprint(f\"\\nDimensionality reduction: {input_image.shape} → {vertical_pooled.shape}\")\nprint(f\"Parameters in kernel: {vertical_edge_kernel.size}\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Object Detection\n\n**Object Detection** combines **classification** (what is it?) with **localization** (where is it?). Essential for autonomous driving to detect vehicles, pedestrians, cyclists, etc.\n\n### Task Definition\n\n**Input**: Image (H × W × 3)  \n**Output**: List of detections, each with:\n- **Bounding box**: (x, y, w, h) or (x₁, y₁, x₂, y₂)\n- **Class label**: Car, Pedestrian, Cyclist, etc.\n- **Confidence score**: Probability [0, 1]\n\n---\n\n### Evolution of Object Detection\n\n#### **Two-Stage Detectors** (Accuracy-focused)\n\n**R-CNN Family** (2014-2017):\n```\nR-CNN → Fast R-CNN → Faster R-CNN → Mask R-CNN\n```\n\n**Faster R-CNN Pipeline**:\n1. **Backbone CNN**: Extract feature maps (e.g., ResNet-50)\n2. **Region Proposal Network (RPN)**: Propose ~2000 candidate boxes\n3. **ROI Pooling**: Extract fixed-size features for each proposal\n4. **Detection Head**: Classify + refine box coordinates\n\n**Advantages**:\n- ✅ High accuracy (mAP ~40% on COCO)\n- ✅ Precise localization\n\n**Disadvantages**:\n- ❌ Slow (5-7 FPS)\n- ❌ Complex training (multi-stage)\n\n---\n\n#### **One-Stage Detectors** (Speed-focused)\n\n**Key Innovation**: Predict bounding boxes and classes directly from feature maps, skip region proposals.\n\n##### **YOLO (You Only Look Once)**\n\n**Philosophy**: \"Look at the image once\" - single forward pass.\n\n**YOLOv1 (2016)**:\n1. Divide image into S×S grid (7×7)\n2. Each cell predicts B bounding boxes (2) + confidence\n3. Each cell predicts C class probabilities (20 for PASCAL VOC)\n4. Output: S×S×(B×5 + C) = 7×7×30 tensor\n\n**Loss Function** (Multi-part):\n$$L = \\lambda_{coord} L_{box} + L_{obj} + \\lambda_{noobj} L_{noobj} + L_{class}$$\n\n**Evolution**:\n- **YOLOv2** (2017): Batch norm, anchor boxes, multi-scale\n- **YOLOv3** (2018): 3 scales, residual blocks, 53 layers\n- **YOLOv4** (2020): CSPDarknet, PANet, Mish activation\n- **YOLOv5** (2020): PyTorch, optimized for production\n- **YOLOv8** (2023): SOTA accuracy + speed\n\n**Performance** (YOLOv5):\n- **YOLOv5n** (nano): 1.9M params, 4.5 GFLOPs, **45 FPS**\n- **YOLOv5s** (small): 7.2M params, 16.5 GFLOPs, **30 FPS**\n- **YOLOv5m** (medium): 21.2M params, 49.0 GFLOPs, **20 FPS**\n- **YOLOv5x** (extra-large): 86.7M params, 205.7 GFLOPs, **10 FPS**\n\n---\n\n##### **SSD (Single Shot MultiBox Detector)**\n\n**Key Idea**: Multi-scale feature maps for detecting objects of different sizes.\n\n**Architecture**:\n```\nInput (300×300×3)\n    ↓\nBackbone (VGG-16 / ResNet)\n    ↓\nFeature Pyramid: [38×38, 19×19, 10×10, 5×5, 3×3, 1×1]\n    ↓\nPredictions at each scale\n```\n\n**Multi-scale Detection**:\n- **Large feature maps** (38×38): Detect small objects\n- **Small feature maps** (3×3): Detect large objects\n\n**Anchor Boxes**:\n- Pre-defined boxes of various aspect ratios: [1:1, 2:1, 1:2, 3:1, 1:3]\n- 8732 total anchors across all scales\n\n**Performance**:\n- **SSD300**: 300×300 input, **59 FPS**, 77.2% mAP\n- **SSD512**: 512×512 input, **22 FPS**, 79.8% mAP\n\n---\n\n### Bounding Box Representation\n\n**Two common formats**:\n\n1. **YOLO format**: (x_center, y_center, width, height) - normalized [0, 1]\n2. **Pascal VOC format**: (x_min, y_min, x_max, y_max) - absolute pixels\n\n**Conversion**:\n```python\n# YOLO → Pascal VOC\nx1 = (x_center - width/2) * image_width\ny1 = (y_center - height/2) * image_height\nx2 = (x_center + width/2) * image_width\ny2 = (y_center + height/2) * image_height\n```\n\n---\n\n### Evaluation Metrics\n\n#### **Intersection over Union (IoU)**\n\nMeasures overlap between predicted and ground truth boxes:\n\n$$\\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}$$\n\n**Thresholds**:\n- IoU > 0.5: Typically considered a correct detection\n- IoU > 0.7: Stricter threshold\n\n#### **Precision-Recall Curve**\n\n- **Precision**: What fraction of detections are correct?\n- **Recall**: What fraction of ground truth objects were detected?\n\nPlot Precision vs. Recall as confidence threshold varies.\n\n#### **Average Precision (AP)**\n\nArea under the Precision-Recall curve for a single class.\n\n$$\\text{AP} = \\int_0^1 P(r) \\, dr$$\n\n#### **mean Average Precision (mAP)**\n\nAverage AP across all classes:\n\n$$\\text{mAP} = \\frac{1}{C} \\sum_{c=1}^{C} \\text{AP}_c$$\n\n**Variants**:\n- **mAP@0.5**: IoU threshold = 0.5\n- **mAP@0.5:0.95**: Average over IoU ∈ [0.5, 0.55, ..., 0.95]\n\n---\n\n### Non-Maximum Suppression (NMS)\n\n**Problem**: Multiple overlapping detections for same object.\n\n**Solution**: Keep only the highest confidence detection, suppress others.\n\n**Algorithm**:\n1. Sort detections by confidence score (descending)\n2. Select detection with highest confidence\n3. Remove all detections with IoU > threshold (e.g., 0.5) with selected box\n4. Repeat until no detections remain\n\n**Code**:\n```python\ndef nms(boxes, scores, iou_threshold=0.5):\n    keep = []\n    indices = np.argsort(scores)[::-1]  # Sort by score descending\n    \n    while len(indices) > 0:\n        current = indices[0]\n        keep.append(current)\n        \n        # Compute IoU of current box with all remaining boxes\n        ious = compute_iou(boxes[current], boxes[indices[1:]])\n        \n        # Keep only boxes with IoU < threshold\n        indices = indices[1:][ious < iou_threshold]\n    \n    return keep\n```\n\n---\n\n### Automotive-Specific Datasets\n\n| Dataset | Year | Images | Boxes | Classes | Notes |\n|---------|------|--------|-------|---------|-------|\n| **KITTI** | 2012 | 7,481 | 80k | 8 | Benchmark for autonomous driving |\n| **nuScenes** | 2019 | 40k | 1.4M | 23 | 360° coverage, Boston/Singapore |\n| **Waymo Open** | 2019 | 200k | 12M | 4 | Largest AV dataset |\n| **BDD100K** | 2018 | 100k | 1.8M | 10 | Diverse weather/time of day |\n| **Argoverse** | 2019 | 30k | - | 15 | HD maps + trajectories |\n\n**Class distribution challenges**:\n- Vehicles: 70-80% of instances\n- Pedestrians: 15-20%\n- Cyclists: 2-5%\n- Other: <1% (motorcycles, animals, etc.)\n\n**Solution**: Class-balanced sampling, focal loss\n\n---\n\n### Challenges in Autonomous Driving\n\n1. **Occlusion**: Pedestrians behind cars\n2. **Scale variation**: Close vs. distant objects\n3. **Class imbalance**: Many cars, few cyclists\n4. **Domain shift**: Train on sunny, test in rain/snow\n5. **Real-time constraints**: Must run at 20-30 FPS\n6. **Safety-critical**: False negatives (misses) are catastrophic\n\n**Solutions**:\n- Multi-scale training\n- Data augmentation (weather, occlusion)\n- Ensemble models\n- Temporal consistency (tracking across frames)\n- Sensor fusion (camera + LiDAR + radar)\n\n---\n\n### Model Selection for Autonomous Driving\n\n| Use Case | Model | Rationale |\n|----------|-------|-----------|\n| **Production AV** | YOLOv5m / YOLOv8 | Balance of speed + accuracy |\n| **Safety-critical** | Faster R-CNN | Highest accuracy, redundancy |\n| **Embedded systems** | YOLOv5n / MobileNet-SSD | Low latency, low power |\n| **Research** | Transformer-based (DETR) | SOTA accuracy |\n\n**Typical pipeline**: YOLOv5s (30 FPS) + Multi-object tracking (DeepSORT)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Object Detection Implementation\n\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\nfrom collections import defaultdict\n\n@dataclass\nclass BoundingBox:\n    \"\"\"Bounding box representation.\"\"\"\n    x_min: float\n    y_min: float\n    x_max: float\n    y_max: float\n    confidence: float\n    class_id: int\n    class_name: str = \"\"\n\ndef compute_iou(box1: BoundingBox, box2: BoundingBox) -> float:\n    \"\"\"\n    Compute Intersection over Union (IoU) between two bounding boxes.\n\n    Args:\n        box1: First bounding box\n        box2: Second bounding box\n\n    Returns:\n        IoU value [0, 1]\n    \"\"\"\n    # Compute intersection area\n    x_left = max(box1.x_min, box2.x_min)\n    y_top = max(box1.y_min, box2.y_min)\n    x_right = min(box1.x_max, box2.x_max)\n    y_bottom = min(box1.y_max, box2.y_max)\n\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n\n    # Compute union area\n    box1_area = (box1.x_max - box1.x_min) * (box1.y_max - box1.y_min)\n    box2_area = (box2.x_max - box2.x_min) * (box2.y_max - box2.y_min)\n    union_area = box1_area + box2_area - intersection_area\n\n    return intersection_area / union_area if union_area > 0 else 0.0\n\ndef non_max_suppression(boxes: List[BoundingBox], iou_threshold: float = 0.5) -> List[BoundingBox]:\n    \"\"\"\n    Apply Non-Maximum Suppression (NMS) to remove overlapping detections.\n\n    Args:\n        boxes: List of bounding boxes\n        iou_threshold: IoU threshold for suppression\n\n    Returns:\n        Filtered list of bounding boxes\n    \"\"\"\n    if len(boxes) == 0:\n        return []\n\n    # Sort boxes by confidence (descending)\n    boxes_sorted = sorted(boxes, key=lambda x: x.confidence, reverse=True)\n\n    keep = []\n    while len(boxes_sorted) > 0:\n        # Keep the highest confidence box\n        current = boxes_sorted[0]\n        keep.append(current)\n        boxes_sorted = boxes_sorted[1:]\n\n        # Remove all boxes with high IoU with current box\n        filtered = []\n        for box in boxes_sorted:\n            if compute_iou(current, box) < iou_threshold:\n                filtered.append(box)\n        boxes_sorted = filtered\n\n    return keep\n\ndef compute_average_precision(detections: List[BoundingBox],\n                               ground_truths: List[BoundingBox],\n                               iou_threshold: float = 0.5) -> Tuple[float, np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute Average Precision (AP) for a single class.\n\n    Args:\n        detections: Predicted bounding boxes (sorted by confidence descending)\n        ground_truths: Ground truth bounding boxes\n        iou_threshold: IoU threshold for considering a detection as correct\n\n    Returns:\n        Tuple of (AP, precision array, recall array)\n    \"\"\"\n    if len(ground_truths) == 0:\n        return 0.0, np.array([]), np.array([])\n\n    # Sort detections by confidence\n    detections = sorted(detections, key=lambda x: x.confidence, reverse=True)\n\n    # Track which ground truths have been matched\n    gt_matched = [False] * len(ground_truths)\n\n    true_positives = np.zeros(len(detections))\n    false_positives = np.zeros(len(detections))\n\n    for i, det in enumerate(detections):\n        # Find best matching ground truth\n        best_iou = 0.0\n        best_gt_idx = -1\n\n        for j, gt in enumerate(ground_truths):\n            if not gt_matched[j]:\n                iou = compute_iou(det, gt)\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = j\n\n        # Check if detection is correct\n        if best_iou >= iou_threshold and best_gt_idx >= 0:\n            if not gt_matched[best_gt_idx]:\n                true_positives[i] = 1\n                gt_matched[best_gt_idx] = True\n            else:\n                false_positives[i] = 1\n        else:\n            false_positives[i] = 1\n\n    # Compute cumulative sums\n    tp_cumsum = np.cumsum(true_positives)\n    fp_cumsum = np.cumsum(false_positives)\n\n    # Compute precision and recall\n    recalls = tp_cumsum / len(ground_truths)\n    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-10)\n\n    # Compute AP using 11-point interpolation\n    ap = 0.0\n    for t in np.linspace(0, 1, 11):\n        precision_at_recall = precisions[recalls >= t]\n        if len(precision_at_recall) > 0:\n            ap += np.max(precision_at_recall) / 11\n\n    return ap, precisions, recalls\n\n# Simulate object detection on a driving scene\n\n# Create synthetic driving scene\nnp.random.seed(42)\nscene_width, scene_height = 640, 480\n\n# Define classes\nclasses = ['Car', 'Pedestrian', 'Cyclist', 'Traffic Light']\nclass_colors = {\n    'Car': 'blue',\n    'Pedestrian': 'green',\n    'Cyclist': 'orange',\n    'Traffic Light': 'red'\n}\n\n# Generate ground truth objects\nground_truth_boxes = [\n    BoundingBox(100, 200, 200, 300, 1.0, 0, 'Car'),\n    BoundingBox(250, 180, 350, 320, 1.0, 0, 'Car'),\n    BoundingBox(400, 220, 480, 350, 1.0, 0, 'Car'),\n    BoundingBox(150, 150, 180, 220, 1.0, 1, 'Pedestrian'),\n    BoundingBox(320, 160, 350, 240, 1.0, 1, 'Pedestrian'),\n    BoundingBox(520, 140, 560, 230, 1.0, 2, 'Cyclist'),\n    BoundingBox(580, 50, 600, 100, 1.0, 3, 'Traffic Light'),\n]\n\n# Generate predicted detections (with some errors and noise)\npredicted_boxes = [\n    # True positives (good detections)\n    BoundingBox(105, 205, 205, 305, 0.95, 0, 'Car'),\n    BoundingBox(255, 185, 355, 325, 0.92, 0, 'Car'),\n    BoundingBox(405, 225, 485, 355, 0.88, 0, 'Car'),\n    BoundingBox(152, 152, 182, 222, 0.85, 1, 'Pedestrian'),\n    BoundingBox(522, 142, 562, 232, 0.78, 2, 'Cyclist'),\n    BoundingBox(582, 52, 602, 102, 0.91, 3, 'Traffic Light'),\n\n    # False positive (wrong detection)\n    BoundingBox(450, 100, 500, 150, 0.65, 0, 'Car'),\n\n    # Duplicate detection (should be suppressed by NMS)\n    BoundingBox(110, 210, 210, 310, 0.72, 0, 'Car'),\n\n    # False negative: pedestrian at (320, 160) is missing\n\n    # Low confidence detection (borderline)\n    BoundingBox(560, 300, 620, 400, 0.45, 0, 'Car'),\n]\n\n# Apply NMS\nnms_boxes = non_max_suppression(predicted_boxes, iou_threshold=0.5)\n\n# Filter by confidence threshold\nconfidence_threshold = 0.5\nfiltered_boxes = [box for box in nms_boxes if box.confidence >= confidence_threshold]\n\n# Compute metrics per class\nmetrics = {}\nfor class_id, class_name in enumerate(classes):\n    gt_class = [box for box in ground_truth_boxes if box.class_id == class_id]\n    det_class = [box for box in filtered_boxes if box.class_id == class_id]\n\n    if len(gt_class) > 0:\n        ap, precisions, recalls = compute_average_precision(det_class, gt_class)\n        metrics[class_name] = {\n            'AP': ap,\n            'Precision': precisions,\n            'Recall': recalls,\n            'GT_count': len(gt_class),\n            'Det_count': len(det_class)\n        }\n\n# Compute mAP\nmAP = np.mean([m['AP'] for m in metrics.values()])\n\n# Visualization\nfig = plt.figure(figsize=(18, 12))\n\n# Plot 1: Raw detections (before NMS)\nax1 = fig.add_subplot(2, 3, 1)\nax1.set_xlim([0, scene_width])\nax1.set_ylim([scene_height, 0])\nax1.set_aspect('equal')\nax1.set_title('Raw Detections (Before NMS)\\n{} boxes'.format(len(predicted_boxes)),\n              fontsize=12, fontweight='bold')\nax1.set_xlabel('X (pixels)')\nax1.set_ylabel('Y (pixels)')\nax1.grid(True, alpha=0.3)\n\nfor box in predicted_boxes:\n    rect = Rectangle((box.x_min, box.y_min), box.x_max - box.x_min, box.y_max - box.y_min,\n                      linewidth=2, edgecolor=class_colors[box.class_name], facecolor='none', alpha=0.7)\n    ax1.add_patch(rect)\n    ax1.text(box.x_min, box.y_min - 5, f'{box.class_name}\\n{box.confidence:.2f}',\n             fontsize=8, color=class_colors[box.class_name], fontweight='bold',\n             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n\n# Plot 2: After NMS\nax2 = fig.add_subplot(2, 3, 2)\nax2.set_xlim([0, scene_width])\nax2.set_ylim([scene_height, 0])\nax2.set_aspect('equal')\nax2.set_title('After NMS\\n{} boxes (removed {} duplicates)'.format(len(nms_boxes), len(predicted_boxes) - len(nms_boxes)),\n              fontsize=12, fontweight='bold')\nax2.set_xlabel('X (pixels)')\nax2.set_ylabel('Y (pixels)')\nax2.grid(True, alpha=0.3)\n\nfor box in nms_boxes:\n    rect = Rectangle((box.x_min, box.y_min), box.x_max - box.x_min, box.y_max - box.y_min,\n                      linewidth=2, edgecolor=class_colors[box.class_name], facecolor='none', alpha=0.7)\n    ax2.add_patch(rect)\n    ax2.text(box.x_min, box.y_min - 5, f'{box.class_name}\\n{box.confidence:.2f}',\n             fontsize=8, color=class_colors[box.class_name], fontweight='bold',\n             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n\n# Plot 3: Final detections (after confidence filtering)\nax3 = fig.add_subplot(2, 3, 3)\nax3.set_xlim([0, scene_width])\nax3.set_ylim([scene_height, 0])\nax3.set_aspect('equal')\nax3.set_title('Final Detections (Conf > {:.2f})\\n{} boxes'.format(confidence_threshold, len(filtered_boxes)),\n              fontsize=12, fontweight='bold')\nax3.set_xlabel('X (pixels)')\nax3.set_ylabel('Y (pixels)')\nax3.grid(True, alpha=0.3)\n\nfor box in filtered_boxes:\n    rect = Rectangle((box.x_min, box.y_min), box.x_max - box.x_min, box.y_max - box.y_min,\n                      linewidth=3, edgecolor=class_colors[box.class_name], facecolor='none')\n    ax3.add_patch(rect)\n    ax3.text(box.x_min, box.y_min - 5, f'{box.class_name}\\n{box.confidence:.2f}',\n             fontsize=9, color=class_colors[box.class_name], fontweight='bold',\n             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n\n# Plot 4: Ground truth\nax4 = fig.add_subplot(2, 3, 4)\nax4.set_xlim([0, scene_width])\nax4.set_ylim([scene_height, 0])\nax4.set_aspect('equal')\nax4.set_title('Ground Truth\\n{} objects'.format(len(ground_truth_boxes)),\n              fontsize=12, fontweight='bold')\nax4.set_xlabel('X (pixels)')\nax4.set_ylabel('Y (pixels)')\nax4.grid(True, alpha=0.3)\n\nfor box in ground_truth_boxes:\n    rect = Rectangle((box.x_min, box.y_min), box.x_max - box.x_min, box.y_max - box.y_min,\n                      linewidth=3, edgecolor=class_colors[box.class_name], facecolor='none', linestyle='--')\n    ax4.add_patch(rect)\n    ax4.text(box.x_min, box.y_min - 5, box.class_name,\n             fontsize=9, color=class_colors[box.class_name], fontweight='bold',\n             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n\n# Plot 5: Precision-Recall curves\nax5 = fig.add_subplot(2, 3, 5)\nfor class_name, data in metrics.items():\n    if len(data['Recall']) > 0:\n        ax5.plot(data['Recall'], data['Precision'], marker='o', label=f\"{class_name} (AP={data['AP']:.3f})\", linewidth=2)\nax5.set_xlabel('Recall', fontsize=11)\nax5.set_ylabel('Precision', fontsize=11)\nax5.set_title('Precision-Recall Curves', fontsize=12, fontweight='bold')\nax5.grid(True, alpha=0.3)\nax5.legend()\nax5.set_xlim([0, 1])\nax5.set_ylim([0, 1.05])\n\n# Plot 6: Detection statistics\nax6 = fig.add_subplot(2, 3, 6)\nax6.axis('off')\nax6.text(0.5, 0.95, 'Detection Performance Summary', ha='center', fontsize=14, fontweight='bold')\nax6.text(0.5, 0.85, f'mAP@0.5: {mAP:.3f}', ha='center', fontsize=12,\n         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))\n\ny_pos = 0.72\nfor class_name, data in metrics.items():\n    text = f\"{class_name}: AP={data['AP']:.3f}, GT={data['GT_count']}, Det={data['Det_count']}\"\n    ax6.text(0.1, y_pos, text, fontsize=10, family='monospace')\n    y_pos -= 0.08\n\n# Add statistics\nstats_text = f\"\\nNMS IoU threshold: {0.5:.2f}\\n\"\nstats_text += f\"Confidence threshold: {confidence_threshold:.2f}\\n\"\nstats_text += f\"True Positives: {len([b for b in filtered_boxes if any(compute_iou(b, gt) > 0.5 for gt in ground_truth_boxes)])}\\n\"\nstats_text += f\"False Positives: {len([b for b in filtered_boxes if all(compute_iou(b, gt) < 0.5 for gt in ground_truth_boxes)])}\\n\"\nstats_text += f\"False Negatives: {len(ground_truth_boxes) - len([gt for gt in ground_truth_boxes if any(compute_iou(det, gt) > 0.5 for det in filtered_boxes)])}\"\n\nax6.text(0.1, y_pos - 0.1, stats_text, fontsize=9, family='monospace',\n         bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"=\" * 80)\nprint(\"OBJECT DETECTION METRICS\")\nprint(\"=\" * 80)\nprint(f\"{'Class':<15} {'AP@0.5':<10} {'Ground Truth':<15} {'Detections':<15}\")\nprint(\"-\" * 80)\nfor class_name, data in metrics.items():\n    print(f\"{class_name:<15} {data['AP']:<10.3f} {data['GT_count']:<15} {data['Det_count']:<15}\")\nprint(\"-\" * 80)\nprint(f\"{'mAP@0.5':<15} {mAP:<10.3f}\")\nprint(\"=\" * 80)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- References to be added"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}