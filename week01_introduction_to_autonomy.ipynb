{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to Autonomy\n",
    "\n",
    "## Module I: Introduction & Foundations\n",
    "\n",
    "### Topics Covered\n",
    "\n",
    "- SAE J3016 Levels of Driving Automation\n",
    "- History and Landscape of Autonomous Vehicles\n",
    "- The Sense-Plan-Act Paradigm\n",
    "- System Architecture (hardware/software)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain the six levels of driving automation (SAE J3016)\n",
    "2. Understand the evolution of autonomous vehicles\n",
    "3. Describe the Sense-Plan-Act paradigm\n",
    "4. Identify key components in AV system architecture\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. SAE J3016 Levels of Driving Automation\n\nThe **SAE J3016** standard defines six levels of driving automation, from Level 0 (no automation) to Level 5 (full automation). Understanding these levels is crucial for discussing autonomous vehicle capabilities and limitations.\n\n### The Six Levels\n\n| Level | Name | Driver Role | System Role | Example |\n|-------|------|-------------|-------------|---------|\n| **0** | No Automation | Full control at all times | Warnings only (e.g., blind spot alerts) | Traditional cars with warning systems |\n| **1** | Driver Assistance | Controls steering OR speed | Assists with steering OR speed | Adaptive cruise control, lane keeping |\n| **2** | Partial Automation | Monitors and ready to take over | Controls steering AND speed | Tesla Autopilot, GM Super Cruise |\n| **3** | Conditional Automation | Takes over when system requests | Drives in specific conditions | Audi Traffic Jam Pilot |\n| **4** | High Automation | Not required in defined areas | Drives independently in geo-fenced areas | Waymo robotaxis, cruise origin |\n| **5** | Full Automation | Never required | Drives everywhere, all conditions | Not yet achieved |\n\n### Key Distinctions\n\n**Levels 0-2: Driver is responsible**\n- Driver must monitor the environment at all times\n- Driver is liable for accidents\n- Hands must be ready to take control\n\n**Levels 3-5: System is responsible (in its domain)**\n- System monitors the environment\n- System is liable during automated operation\n- Driver can disengage (Levels 4-5)\n\n### Critical Concept: Operational Design Domain (ODD)\n\nThe **ODD** defines where and when an autonomous system can operate safely:\n- **Geographic area**: Specific cities, highways, or global\n- **Environmental conditions**: Weather (sunny, rain, snow), lighting (day/night)\n- **Road types**: Highways, urban streets, unmarked roads\n- **Speed range**: 0-25 mph (parking), 0-70 mph (highway)\n\n**Example**: A Level 4 robotaxi might have an ODD of:\n- ‚úÖ Downtown San Francisco\n- ‚úÖ Dry weather, daylight hours\n- ‚úÖ Speed under 35 mph\n- ‚ùå Highways\n- ‚ùå Heavy rain or fog"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Visualize SAE Levels with responsibility handoff\nfig, ax = plt.subplots(figsize=(14, 8))\n\nlevels = ['Level 0\\nNo\\nAutomation', 'Level 1\\nDriver\\nAssistance', \n          'Level 2\\nPartial\\nAutomation', 'Level 3\\nConditional\\nAutomation',\n          'Level 4\\nHigh\\nAutomation', 'Level 5\\nFull\\nAutomation']\n\n# Responsibility percentages (Driver vs System)\ndriver_resp = [100, 80, 70, 30, 5, 0]\nsystem_resp = [0, 20, 30, 70, 95, 100]\n\nx_pos = np.arange(len(levels))\nwidth = 0.6\n\n# Create stacked bars\np1 = ax.bar(x_pos, driver_resp, width, label='Driver Responsible', color='#FF6B6B')\np2 = ax.bar(x_pos, system_resp, width, bottom=driver_resp, label='System Responsible', color='#4ECDC4')\n\n# Styling\nax.set_ylabel('Responsibility (%)', fontsize=14, fontweight='bold')\nax.set_xlabel('Automation Level', fontsize=14, fontweight='bold')\nax.set_title('SAE J3016: Responsibility Shift from Driver to System', fontsize=16, fontweight='bold', pad=20)\nax.set_xticks(x_pos)\nax.set_xticklabels(levels, fontsize=11)\nax.legend(loc='upper left', fontsize=12)\nax.set_ylim([0, 100])\n\n# Add grid\nax.grid(axis='y', alpha=0.3, linestyle='--')\nax.set_axisbelow(True)\n\n# Add dividing line between driver-centric (0-2) and system-centric (3-5)\nax.axvline(x=2.5, color='black', linestyle='--', linewidth=2, alpha=0.6)\nax.text(1.2, 105, 'Driver Must Monitor', fontsize=12, fontweight='bold', ha='center')\nax.text(4, 105, 'System Monitors', fontsize=12, fontweight='bold', ha='center')\n\n# Add annotations\nannotations = [\n    (0, 50, 'Warnings\\nonly'),\n    (1, 50, 'Steering\\nOR speed'),\n    (2, 50, 'Steering\\nAND speed'),\n    (3, 50, 'System\\nrequests\\ntakeover'),\n    (4, 50, 'Geo-fenced\\noperation'),\n    (5, 50, 'All\\nconditions')\n]\n\nfor x, y, text in annotations:\n    ax.text(x, y, text, ha='center', va='center', fontsize=9, \n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key Insight:\")\nprint(\"=\" * 60)\nprint(\"The jump from Level 2 to Level 3 is the BIGGEST challenge!\")\nprint(\"Level 2: Driver monitors ‚Üí Level 3: System monitors\")\nprint(\"This requires near-perfect perception and decision-making.\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. History and Landscape of Autonomous Vehicles\n\nThe journey toward autonomous vehicles spans decades of innovation, from early experiments to today's commercial deployments.\n\n### Timeline of Key Milestones\n\n| Year | Event | Significance |\n|------|-------|--------------|\n| **1925** | Houdina Radio Control demonstrates remote-controlled car in NYC | First \"driverless\" vehicle demonstration |\n| **1939** | GM Futurama exhibit at World's Fair | Public vision of automated highways |\n| **1980s** | Carnegie Mellon NavLab projects | First university research programs |\n| **1986** | Ernst Dickmanns' VaMoRs drives autonomously on German highways | First vision-based autonomous driving |\n| **1995** | CMU's NavLab 5 drives 2,797 miles across America (98.2% autonomous) | \"No Hands Across America\" |\n| **2004-2007** | DARPA Grand Challenge & Urban Challenge | Accelerated development, Stanford wins with Stanley |\n| **2009** | Google Self-Driving Car Project begins (Waymo) | Tech companies enter the field |\n| **2016** | Uber launches self-driving pilot in Pittsburgh | Commercial ride-hailing integration |\n| **2018** | Waymo launches commercial robotaxi service in Phoenix | First paid autonomous rides |\n| **2020s** | Multiple companies deploy L4 systems in geo-fenced areas | Current state: limited commercial deployment |\n\n### Current Industry Landscape (2024-2025)\n\n#### Major Players by Approach\n\n**1. Robotaxi Services (L4)**\n- **Waymo (Alphabet)**: Operating in Phoenix, San Francisco, Los Angeles\n- **Cruise (GM)**: San Francisco operations (paused in 2023, resuming)\n- **Baidu Apollo Go**: Operating in Beijing, Shanghai, Wuhan\n- **Zoox (Amazon)**: Testing autonomous shuttles\n\n**2. ADAS/Consumer Vehicles (L2/L2+)**\n- **Tesla**: Full Self-Driving (FSD) - supervised L2\n- **Mercedes-Benz**: Drive Pilot - certified L3 in Germany\n- **GM Super Cruise**: Hands-free highway driving\n- **Ford BlueCruise**: Highway assist system\n\n**3. Trucking and Logistics (L4)**\n- **TuSimple**: Autonomous trucking\n- **Embark**: Long-haul freight\n- **Nuro**: Last-mile delivery robots\n\n**4. Technology Suppliers**\n- **Mobileye (Intel)**: Vision-based ADAS and AV systems\n- **NVIDIA**: AI computing platforms (Drive platform)\n- **Luminar, Velodyne**: LiDAR sensors\n- **Aurora**: Full-stack AV technology\n\n### Geographic Distribution\n\nüåç **Leading Regions:**\n- **United States**: California, Arizona, Texas (permissive regulations)\n- **China**: Beijing, Shanghai, Shenzhen (government support)\n- **Europe**: Germany (strong automotive tradition)\n\n### Market Size and Projections\n\n- **Current Market (2024)**: ~$30-40 billion\n- **Projected Market (2030)**: $300-400 billion\n- **Key Growth Drivers**: Safety improvements, mobility services, freight efficiency"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize AV development timeline\nfig, ax = plt.subplots(figsize=(16, 6))\n\n# Timeline data\nyears = [1925, 1986, 1995, 2004, 2009, 2016, 2018, 2024]\nevents = ['Remote\\nControl', 'VaMoRs\\nAutonomous', 'No Hands\\nAcross America', \n          'DARPA\\nChallenge', 'Google\\nStarts', 'Uber\\nPilot', \n          'Waymo\\nCommercial', 'Current\\nDeployments']\nimportance = [20, 40, 50, 80, 90, 70, 95, 100]  # Relative importance\n\n# Create scatter plot with varying sizes\ncolors = plt.cm.viridis(np.linspace(0, 1, len(years)))\nscatter = ax.scatter(years, importance, s=[x*10 for x in importance], \n                     c=colors, alpha=0.7, edgecolors='black', linewidth=2)\n\n# Add event labels\nfor i, (year, event, imp) in enumerate(zip(years, events, importance)):\n    ax.annotate(event, (year, imp), \n                textcoords=\"offset points\", xytext=(0, 15), \n                ha='center', fontsize=10, fontweight='bold',\n                bbox=dict(boxstyle='round,pad=0.5', facecolor=colors[i], alpha=0.3))\n\n# Draw connecting line\nax.plot(years, importance, 'k--', alpha=0.3, linewidth=2)\n\n# Styling\nax.set_xlabel('Year', fontsize=14, fontweight='bold')\nax.set_ylabel('Technology Maturity', fontsize=14, fontweight='bold')\nax.set_title('Evolution of Autonomous Vehicle Technology', fontsize=16, fontweight='bold', pad=20)\nax.set_ylim([0, 110])\nax.grid(True, alpha=0.3)\n\n# Add era annotations\nax.axvspan(1925, 2003, alpha=0.1, color='blue', label='Research Era')\nax.axvspan(2004, 2015, alpha=0.1, color='orange', label='Development Era')\nax.axvspan(2016, 2024, alpha=0.1, color='green', label='Deployment Era')\n\nax.legend(loc='upper left', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey Observations:\")\nprint(\"=\" * 70)\nprint(\"1. DARPA Challenges (2004-2007) marked the acceleration point\")\nprint(\"2. Google's entry (2009) brought massive investment and talent\")\nprint(\"3. Commercial deployments started in 2018, but remain geo-fenced\")\nprint(\"4. The technology curve is exponential, but deployment is gradual\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. The Sense-Plan-Act Paradigm\n\nThe **Sense-Plan-Act** cycle is the fundamental architecture of autonomous systems. This three-step loop runs continuously, typically at 10-100 Hz (10-100 times per second).\n\n### The Three Stages\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  SENSE  ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ  PLAN   ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ   ACT   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n     ‚Üë                                   ‚îÇ\n     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n              (Repeat Loop)\n```\n\n#### 1Ô∏è‚É£ SENSE (Perception)\n**Goal**: Understand the environment\n\n**Inputs**:\n- Camera images (RGB, 30-60 FPS)\n- LiDAR point clouds (10-20 Hz)\n- Radar returns (20 Hz)\n- GPS/IMU (100 Hz)\n- HD Maps\n\n**Outputs**:\n- Detected objects (cars, pedestrians, cyclists)\n- Lane lines and road boundaries\n- Traffic signs and lights\n- Free space (drivable area)\n- Vehicle localization (position + orientation)\n\n**Example**: \"There's a pedestrian 15 meters ahead, walking at 1.2 m/s toward the crosswalk\"\n\n---\n\n#### 2Ô∏è‚É£ PLAN (Decision Making)\n**Goal**: Decide what to do\n\n**Hierarchical Planning**:\n1. **Mission Planning**: Route from A to B (uses A* on road graph)\n2. **Behavioral Planning**: High-level decisions (lane change, stop, merge)\n3. **Motion Planning**: Generate safe, smooth trajectory\n\n**Inputs**:\n- Perception outputs\n- Current vehicle state\n- Mission goal\n- Traffic rules\n\n**Outputs**:\n- Planned trajectory (sequence of positions over time)\n- Target speed profile\n- Maneuver type (follow lane, change left, stop, etc.)\n\n**Example**: \"Slow from 35 mph to 0 mph over the next 20 meters to stop before pedestrian\"\n\n---\n\n#### 3Ô∏è‚É£ ACT (Control)\n**Goal**: Execute the plan\n\n**Control Commands**:\n- **Steering angle**: Turn wheels to follow path\n- **Throttle**: Accelerate to match speed profile\n- **Brake**: Decelerate as needed\n\n**Controllers**:\n- **Lateral Control**: Pure Pursuit, Stanley, or MPC for steering\n- **Longitudinal Control**: PID or MPC for speed/braking\n\n**Inputs**:\n- Planned trajectory\n- Current vehicle state (speed, position, heading)\n\n**Outputs**:\n- Steering wheel angle (e.g., 5¬∞ left)\n- Throttle position (e.g., 30%)\n- Brake pressure (e.g., 0 bar)\n\n**Example**: \"Apply 15¬∞ steering angle left and reduce throttle to 20%\"\n\n---\n\n### Why This Paradigm?\n\n‚úÖ **Modularity**: Each stage can be developed and tested independently  \n‚úÖ **Parallelism**: Sensing can run while planning executes previous plan  \n‚úÖ **Clear interfaces**: Well-defined data contracts between stages  \n‚úÖ **Debugging**: Easy to identify which stage has issues  \n\n‚ö†Ô∏è **Limitations**:\n- Fixed pipeline can be slow (150-300ms total latency)\n- Modern approaches use end-to-end learning (direct sensor ‚Üí control)\n- Hybrid approaches combine both paradigms"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Interactive Sense-Plan-Act Simulation\n# Simplified scenario: Car following a lane and reacting to obstacles\n\nclass SimpleSensePlanActVehicle:\n    def __init__(self, x=0, speed=10):\n        self.x = x\n        self.speed = speed  # m/s\n        self.target_speed = 10  # m/s\n        \n    def sense(self, obstacle_position):\n        \"\"\"SENSE: Detect obstacle and measure distance\"\"\"\n        distance_to_obstacle = obstacle_position - self.x\n        return {\n            'obstacle_detected': distance_to_obstacle < 50,\n            'obstacle_distance': distance_to_obstacle,\n            'current_speed': self.speed\n        }\n    \n    def plan(self, perception_data):\n        \"\"\"PLAN: Decide target speed based on obstacle\"\"\"\n        if perception_data['obstacle_detected']:\n            # Slow down if obstacle is near\n            distance = perception_data['obstacle_distance']\n            if distance < 10:\n                target = 0  # Stop\n            elif distance < 30:\n                target = 5  # Slow\n            else:\n                target = 8  # Cautious\n        else:\n            target = 10  # Normal speed\n        \n        return {'target_speed': target}\n    \n    def act(self, plan_data, dt=0.1):\n        \"\"\"ACT: Apply control to match target speed\"\"\"\n        error = plan_data['target_speed'] - self.speed\n        \n        # Simple proportional control\n        acceleration = 2.0 * error  # Kp = 2.0\n        \n        # Update speed and position\n        self.speed += acceleration * dt\n        self.speed = max(0, min(self.speed, 15))  # Clamp to [0, 15] m/s\n        self.x += self.speed * dt\n        \n        return {'acceleration': acceleration, 'new_speed': self.speed}\n\n# Simulation\nvehicle = SimpleSensePlanActVehicle(x=0, speed=10)\nobstacle_x = 60  # Fixed obstacle at 60m\n\ntime_steps = 150\ndt = 0.1  # 100ms per step\n\n# Storage\npositions = []\nspeeds = []\ntarget_speeds = []\ntimes = []\n\nfor t in range(time_steps):\n    # SENSE\n    perception = vehicle.sense(obstacle_x)\n    \n    # PLAN\n    plan = vehicle.plan(perception)\n    \n    # ACT\n    control = vehicle.act(plan, dt)\n    \n    # Record\n    positions.append(vehicle.x)\n    speeds.append(vehicle.speed)\n    target_speeds.append(plan['target_speed'])\n    times.append(t * dt)\n\n# Visualization\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n\n# Position plot\nax1.plot(times, positions, 'b-', linewidth=2, label='Vehicle Position')\nax1.axhline(y=obstacle_x, color='r', linestyle='--', linewidth=2, label='Obstacle Position')\nax1.fill_between(times, 0, obstacle_x, alpha=0.1, color='red', label='Danger Zone')\nax1.set_xlabel('Time (s)', fontsize=12, fontweight='bold')\nax1.set_ylabel('Position (m)', fontsize=12, fontweight='bold')\nax1.set_title('Sense-Plan-Act: Vehicle Position Over Time', fontsize=14, fontweight='bold')\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3)\n\n# Speed plot\nax2.plot(times, speeds, 'b-', linewidth=2, label='Actual Speed')\nax2.plot(times, target_speeds, 'g--', linewidth=2, label='Target Speed (from Planner)')\nax2.set_xlabel('Time (s)', fontsize=12, fontweight='bold')\nax2.set_ylabel('Speed (m/s)', fontsize=12, fontweight='bold')\nax2.set_title('Sense-Plan-Act: Speed Control', fontsize=14, fontweight='bold')\nax2.legend(fontsize=11)\nax2.grid(True, alpha=0.3)\n\n# Add annotations\ndetect_time = None\nfor i, pos in enumerate(positions):\n    if obstacle_x - pos < 50 and detect_time is None:\n        detect_time = times[i]\n        ax1.axvline(x=detect_time, color='orange', linestyle=':', linewidth=2, alpha=0.7)\n        ax1.text(detect_time, obstacle_x/2, 'Obstacle\\nDetected', \n                ha='right', fontsize=10, fontweight='bold',\n                bbox=dict(boxstyle='round', facecolor='orange', alpha=0.3))\n        ax2.axvline(x=detect_time, color='orange', linestyle=':', linewidth=2, alpha=0.7)\n        break\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nSimulation Results:\")\nprint(\"=\" * 70)\nprint(f\"Initial position: 0 m\")\nprint(f\"Obstacle position: {obstacle_x} m\")\nprint(f\"Final position: {positions[-1]:.2f} m\")\nprint(f\"Final speed: {speeds[-1]:.2f} m/s\")\nprint(f\"Stopped before obstacle: {'‚úì Yes' if positions[-1] < obstacle_x - 2 else '‚úó No'}\")\nprint(f\"Detection time: {detect_time:.2f} s\")\nprint(\"=\" * 70)\nprint(\"\\nThis demonstrates the SENSE-PLAN-ACT loop:\")\nprint(\"‚Ä¢ SENSE: Detected obstacle at 50m range\")\nprint(\"‚Ä¢ PLAN: Decided to slow down based on distance\")\nprint(\"‚Ä¢ ACT: Applied braking control to match target speed\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. System Architecture (Hardware/Software)\n\nAn autonomous vehicle is a complex system integrating sensors, compute platforms, software stacks, and actuators. Understanding this architecture is essential for developing AV systems.\n\n### Hardware Architecture\n\n#### **Sensor Suite**\n\n| Sensor Type | Purpose | Range | Strengths | Weaknesses |\n|-------------|---------|-------|-----------|------------|\n| **Camera** | Visual perception | 50-200m | Color, texture, signs, lane lines | Poor in darkness, glare, weather |\n| **LiDAR** | 3D point clouds | 100-200m | Accurate depth, works in dark | Expensive, struggles in rain/fog |\n| **Radar** | Object detection/tracking | 150-300m | Works in all weather, Doppler velocity | Low resolution, no color |\n| **Ultrasonic** | Close-range detection | 0.5-5m | Cheap, reliable for parking | Very short range |\n| **GPS/GNSS** | Global positioning | Global | Absolute position reference | 1-5m error, poor in urban canyons |\n| **IMU** | Inertial measurement | N/A | High-frequency motion data | Drifts over time |\n\n**Typical Sensor Configuration**:\n- 6-8 cameras (front, sides, rear) ‚Üí 360¬∞ coverage\n- 1-5 LiDAR units (roof-mounted + corners)\n- 4-6 radar units (front, rear, corners)\n- 12 ultrasonic sensors (bumpers)\n- 1 GPS + 1 IMU\n\n---\n\n#### **Compute Platform**\n\nModern AVs require massive computational power:\n\n| Component | Requirement | Example Hardware |\n|-----------|-------------|------------------|\n| **GPU** | Deep learning inference | NVIDIA Drive AGX Orin (254 TOPS) |\n| **CPU** | General processing, planning | ARM or x86 multi-core (8-16 cores) |\n| **Memory** | Data buffering | 32-64 GB RAM |\n| **Storage** | Logging, HD maps | 500GB-2TB SSD |\n| **Power** | Total system power | 500-2000W |\n\n**Processing Pipeline**:\n```\nSensors (GB/s) ‚Üí Compute (TOPS) ‚Üí Actuators (100 Hz)\n   Camera: 8 x 60 FPS = 480 images/sec\n   LiDAR: 1.3 million points/sec\n   ‚Üí Requires 200+ TOPS for real-time processing\n```\n\n---\n\n#### **Actuators (Drive-by-Wire)**\n\n| Actuator | Function | Control Interface |\n|----------|----------|-------------------|\n| **Steering** | Lateral control | Electric motor (angle commands) |\n| **Throttle** | Acceleration | Electronic throttle body (%) |\n| **Brake** | Deceleration | Hydraulic/electric brake system (pressure) |\n| **Transmission** | Gear selection | Electronic shifter |\n\nAll controlled via **CAN bus** (Controller Area Network) at 500 kbit/s - 1 Mbit/s\n\n---\n\n### Software Architecture\n\n#### **Layered Software Stack**\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         APPLICATION LAYER                   ‚îÇ\n‚îÇ  Mission Planning, Fleet Management         ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n           ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         AUTONOMY STACK                      ‚îÇ\n‚îÇ  Perception | Localization | Planning       ‚îÇ\n‚îÇ  Prediction | Control                       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n           ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         MIDDLEWARE LAYER                    ‚îÇ\n‚îÇ  ROS 2 / Apollo / Autoware                  ‚îÇ\n‚îÇ  (Message passing, synchronization)         ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n           ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         HARDWARE ABSTRACTION LAYER          ‚îÇ\n‚îÇ  Sensor drivers, CAN interface              ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n           ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         OPERATING SYSTEM                    ‚îÇ\n‚îÇ  Linux (Ubuntu 20.04/22.04) + Real-time     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n---\n\n#### **Key Software Modules**\n\n**1. Perception**\n- Object detection (YOLO, PointPillars)\n- Semantic segmentation\n- Multi-object tracking (MOT)\n- Traffic light/sign classification\n\n**2. Localization**\n- GPS + IMU fusion\n- LiDAR-based localization (NDT, ICP)\n- Visual odometry\n- HD map matching\n\n**3. Prediction**\n- Trajectory prediction for other vehicles\n- Pedestrian intent estimation\n- Physics-based models + ML\n\n**4. Planning**\n- Route planning (A*, Dijkstra on HD map graph)\n- Behavior planning (FSM, decision trees)\n- Motion planning (RRT*, lattice planners)\n- Trajectory optimization (MPC)\n\n**5. Control**\n- Lateral control (Pure Pursuit, Stanley, MPC)\n- Longitudinal control (PID, adaptive cruise)\n- Actuation commands (steering, throttle, brake)\n\n---\n\n### Common Software Frameworks\n\n| Framework | Developer | Use Case |\n|-----------|-----------|----------|\n| **ROS 2** | Open Robotics | Research, prototyping |\n| **Apollo** | Baidu | Full-stack AV development |\n| **Autoware** | Autoware Foundation | Open-source AV platform |\n| **NVIDIA DriveWorks** | NVIDIA | Production AV systems |\n\n---\n\n### Data Flow Example\n\n```\nCamera ‚Üí Image ‚Üí CNN ‚Üí Objects ‚Üí Tracker ‚Üí Predicted trajectories\n                                              ‚Üì\nLiDAR ‚Üí Points ‚Üí PointNet ‚Üí Objects ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                              ‚Üì\n                                         Behavior Planner\n                                              ‚Üì\n                                         Motion Planner\n                                              ‚Üì\nGPS/IMU ‚Üí Kalman Filter ‚Üí Vehicle Pose ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Controller\n                                              ‚Üì\n                                        Steering + Throttle\n```\n\n**Latency Budget** (Total: ~200-300ms):\n- Sensor acquisition: 16-33ms (30-60 FPS)\n- Perception: 50-100ms\n- Planning: 50-100ms\n- Control: 10-20ms\n- Actuation: 10-20ms"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize AV System Architecture\nfig = plt.figure(figsize=(16, 10))\n\n# Create grid for different visualizations\ngs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\nax1 = fig.add_subplot(gs[0, :])  # Sensor comparison\nax2 = fig.add_subplot(gs[1, 0])  # Compute requirements\nax3 = fig.add_subplot(gs[1, 1])  # Latency budget\nax4 = fig.add_subplot(gs[2, :])  # Software layers\n\n# 1. Sensor Comparison\nsensors = ['Camera', 'LiDAR', 'Radar', 'Ultrasonic', 'GPS', 'IMU']\nranges = [150, 150, 250, 3, 10000, 0]  # Max range in meters (GPS in km converted)\ncosts = [100, 8000, 500, 20, 150, 200]  # Relative cost in USD\n\nx = np.arange(len(sensors))\nwidth = 0.35\n\nbars1 = ax1.bar(x - width/2, ranges, width, label='Range (m)', color='#3498db', alpha=0.8)\nax1_twin = ax1.twinx()\nbars2 = ax1_twin.bar(x + width/2, costs, width, label='Cost ($)', color='#e74c3c', alpha=0.8)\n\nax1.set_xlabel('Sensor Type', fontsize=12, fontweight='bold')\nax1.set_ylabel('Range (m)', fontsize=11, fontweight='bold', color='#3498db')\nax1_twin.set_ylabel('Cost ($)', fontsize=11, fontweight='bold', color='#e74c3c')\nax1.set_title('Sensor Comparison: Range vs. Cost', fontsize=14, fontweight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(sensors)\nax1.tick_params(axis='y', labelcolor='#3498db')\nax1_twin.tick_params(axis='y', labelcolor='#e74c3c')\nax1.legend(loc='upper left')\nax1_twin.legend(loc='upper right')\nax1.grid(axis='y', alpha=0.3)\n\n# 2. Compute Requirements by Module\nmodules = ['Perception', 'Localization', 'Prediction', 'Planning', 'Control']\ntops_required = [150, 20, 30, 40, 10]  # TOPS (Tera Operations Per Second)\ncolors_compute = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n\nwedges, texts, autotexts = ax2.pie(tops_required, labels=modules, autopct='%1.1f%%',\n                                     colors=colors_compute, startangle=90)\nax2.set_title('Compute Distribution by Module\\n(Total: 250 TOPS)', \n              fontsize=12, fontweight='bold')\n\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontweight('bold')\n    autotext.set_fontsize(10)\n\n# 3. Latency Budget\ncomponents = ['Sensor\\nAcq.', 'Perception', 'Planning', 'Control', 'Actuation']\nlatencies = [25, 75, 75, 15, 15]  # milliseconds\ncolors_latency = ['#1abc9c', '#3498db', '#9b59b6', '#e67e22', '#e74c3c']\n\nbars = ax3.barh(components, latencies, color=colors_latency, alpha=0.8, edgecolor='black', linewidth=1.5)\nax3.set_xlabel('Latency (ms)', fontsize=11, fontweight='bold')\nax3.set_title('End-to-End Latency Budget\\n(Total: 205 ms)', fontsize=12, fontweight='bold')\nax3.axvline(x=200, color='red', linestyle='--', linewidth=2, label='Target: 200ms')\nax3.legend()\nax3.grid(axis='x', alpha=0.3)\n\n# Add values on bars\nfor i, (bar, latency) in enumerate(zip(bars, latencies)):\n    ax3.text(latency + 2, i, f'{latency}ms', va='center', fontweight='bold', fontsize=10)\n\n# 4. Software Stack Layers\nlayers = ['OS\\n(Linux)', 'HAL\\n(Drivers)', 'Middleware\\n(ROS 2)', \n          'Autonomy\\n(Perception,\\nPlanning,\\nControl)', 'Application\\n(Fleet Mgmt)']\nlayer_heights = [1, 1, 1.5, 3, 1]\nlayer_colors = ['#34495e', '#7f8c8d', '#95a5a6', '#3498db', '#2ecc71']\n\ny_pos = 0\nfor i, (layer, height, color) in enumerate(zip(layers, layer_heights, layer_colors)):\n    rect = plt.Rectangle((0.1, y_pos), 0.8, height, facecolor=color, \n                         edgecolor='black', linewidth=2, alpha=0.8)\n    ax4.add_patch(rect)\n    ax4.text(0.5, y_pos + height/2, layer, ha='center', va='center', \n            fontsize=12, fontweight='bold', color='white')\n    y_pos += height\n\nax4.set_xlim(0, 1)\nax4.set_ylim(0, sum(layer_heights))\nax4.set_title('Layered Software Architecture', fontsize=14, fontweight='bold')\nax4.axis('off')\n\n# Add arrows between layers\narrow_x = 0.5\nfor i in range(len(layer_heights) - 1):\n    y_start = sum(layer_heights[:i+1])\n    ax4.annotate('', xy=(arrow_x, y_start + 0.1), xytext=(arrow_x, y_start - 0.1),\n                arrowprops=dict(arrowstyle='<->', color='white', lw=2))\n\nplt.suptitle('Autonomous Vehicle System Architecture Overview', \n             fontsize=16, fontweight='bold', y=0.98)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey Architecture Insights:\")\nprint(\"=\" * 70)\nprint(\"1. Sensor Suite: Multi-modal (Camera + LiDAR + Radar) for redundancy\")\nprint(\"2. Compute: Perception dominates (60% of compute budget)\")\nprint(\"3. Latency: Must stay under 200-300ms for safe real-time operation\")\nprint(\"4. Software: Layered architecture enables modularity and testing\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\nTest your understanding of the key concepts from Week 1.\n\n### Exercise 1: SAE Levels Classification\n\nClassify the following scenarios by SAE automation level:\n\n**A.** A car with adaptive cruise control that maintains speed and following distance, but requires the driver to steer.\n\n**B.** A Tesla with Autopilot engaged on a highway, controlling both steering and speed, while the driver monitors with hands on the wheel.\n\n**C.** A Waymo robotaxi operating in downtown Phoenix with no human driver, but restricted to specific mapped areas.\n\n**D.** A Mercedes-Benz Drive Pilot system that allows the driver to watch a movie during traffic jams on approved highways, but requests the driver take over when exiting the highway.\n\n**E.** A theoretical future vehicle that can drive anywhere in any weather condition without any human input.\n\n---\n\n### Exercise 2: Sense-Plan-Act Analysis\n\nConsider a self-driving car approaching an intersection with a yellow traffic light:\n\n**Scenario**: The car is traveling at 50 km/h, 40 meters from the intersection. The yellow light just turned on.\n\nFor each stage of Sense-Plan-Act, describe:\n\n**A. SENSE**: What sensor data is needed? What objects/features must be detected?\n\n**B. PLAN**: What decision should the system make? What factors should it consider?\n\n**C. ACT**: What control commands should be sent to the actuators?\n\n---\n\n### Exercise 3: Sensor Selection\n\nYou're designing the sensor suite for two different autonomous vehicles:\n\n**Vehicle A**: Low-cost urban delivery robot (max speed 25 km/h, operates only in good weather during daytime)\n\n**Vehicle B**: Highway autonomous truck (max speed 110 km/h, must operate 24/7 in various weather)\n\nFor each vehicle:\n1. Select appropriate sensors from: Camera, LiDAR, Radar, Ultrasonic, GPS, IMU\n2. Justify your choices based on cost, performance, and operational requirements\n3. Identify potential failure modes\n\n---\n\n### Exercise 4: System Architecture Trade-offs\n\nCompare two architectural approaches:\n\n**Approach 1**: Modular pipeline (Sense-Plan-Act with clear separation)\n**Approach 2**: End-to-end learning (Neural network directly maps sensor inputs to control outputs)\n\nFor a **Level 4 robotaxi**, analyze:\n1. Which approach is easier to debug when something goes wrong?\n2. Which approach requires more training data?\n3. Which approach is more interpretable (can you explain why it made a decision)?\n4. Which approach would you choose and why?\n\n---\n\n### Exercise 5: Operational Design Domain (ODD)\n\nDesign an ODD for a new autonomous shuttle service at a university campus.\n\nDefine:\n1. **Geographic boundaries**: Where can it operate?\n2. **Environmental conditions**: Weather, lighting restrictions?\n3. **Road types**: What infrastructure is required?\n4. **Speed limits**: Maximum operating speed?\n5. **Edge cases**: What scenarios require human takeover?\n\nJustify your choices based on current technology limitations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise Solutions\n\nprint(\"=\" * 70)\nprint(\"EXERCISE 1: SAE Levels Classification\")\nprint(\"=\" * 70)\n\nsolutions_ex1 = {\n    'A': ('Level 1', 'Driver Assistance - Controls speed OR steering (not both)'),\n    'B': ('Level 2', 'Partial Automation - Controls speed AND steering, but driver must monitor'),\n    'C': ('Level 4', 'High Automation - No driver needed within geo-fenced ODD'),\n    'D': ('Level 3', 'Conditional Automation - Driver can disengage but must take over on request'),\n    'E': ('Level 5', 'Full Automation - No restrictions on where/when it can operate')\n}\n\nfor key, (level, explanation) in solutions_ex1.items():\n    print(f\"\\n{key}. {level}\")\n    print(f\"   Explanation: {explanation}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"EXERCISE 2: Sense-Plan-Act Analysis\")\nprint(\"=\" * 70)\n\nprint(\"\\nA. SENSE (Perception):\")\nprint(\"   ‚Ä¢ Camera: Detect traffic light color (yellow)\")\nprint(\"   ‚Ä¢ Camera: Detect intersection boundaries, crosswalk\")\nprint(\"   ‚Ä¢ LiDAR/Radar: Measure distance to intersection (40m)\")\nprint(\"   ‚Ä¢ GPS + IMU: Current position and speed (50 km/h = 13.9 m/s)\")\nprint(\"   ‚Ä¢ Camera: Detect other vehicles/pedestrians at intersection\")\n\nprint(\"\\nB. PLAN (Decision Making):\")\nprint(\"   ‚Ä¢ Calculate stopping distance: d = v¬≤/(2Œºg)\")\nprint(\"     - At 50 km/h: ~20-25m (varies with road conditions)\")\nprint(\"   ‚Ä¢ Decision: CAN safely stop before intersection\")\nprint(\"   ‚Ä¢ Check: No vehicles closely following (avoid rear-end collision)\")\nprint(\"   ‚Ä¢ Action: Decelerate smoothly to stop at stop line\")\nprint(\"   ‚Ä¢ Alternative: If stopping distance > 40m, proceed through\")\n\nprint(\"\\nC. ACT (Control):\")\nprint(\"   ‚Ä¢ Throttle: Reduce to 0%\")\nprint(\"   ‚Ä¢ Brake: Apply progressive braking (start 20%, increase to 60%)\")\nprint(\"   ‚Ä¢ Steering: Maintain current lane (0¬∞ deviation)\")\nprint(\"   ‚Ä¢ Target: Smooth deceleration at ~3-4 m/s¬≤ (comfortable for passengers)\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"EXERCISE 3: Sensor Selection\")\nprint(\"=\" * 70)\n\nprint(\"\\nVehicle A: Low-cost urban delivery robot\")\nprint(\"-\" * 70)\nprint(\"Selected Sensors:\")\nprint(\"  ‚úì Camera (2-4 units): Primary perception, low cost (~$400 total)\")\nprint(\"  ‚úì Ultrasonic (8 units): Close-range obstacle detection (~$160)\")\nprint(\"  ‚úì GPS + IMU: Localization (~$300)\")\nprint(\"  ‚úó LiDAR: Too expensive for low-cost vehicle\")\nprint(\"  ‚úó Radar: Not needed at low speeds in good weather\")\nprint(\"\\nTotal Cost: ~$1,000\")\nprint(\"\\nJustification:\")\nprint(\"  ‚Ä¢ Cameras sufficient in daytime + good weather\")\nprint(\"  ‚Ä¢ Low speed (25 km/h) allows camera-only perception\")\nprint(\"  ‚Ä¢ Ultrasonic for close-range safety (parking, pedestrians)\")\nprint(\"\\nFailure Modes:\")\nprint(\"  ‚ö† Sun glare blinds cameras\")\nprint(\"  ‚ö† Unexpected weather (light rain) degrades performance\")\nprint(\"  ‚ö† GPS dropout in urban canyons\")\n\nprint(\"\\n\")\nprint(\"Vehicle B: Highway autonomous truck\")\nprint(\"-\" * 70)\nprint(\"Selected Sensors:\")\nprint(\"  ‚úì Camera (6-8 units): Visual perception, lane lines (~$800)\")\nprint(\"  ‚úì LiDAR (2-3 units): Precise 3D mapping (~$15,000)\")\nprint(\"  ‚úì Radar (4-6 units): All-weather detection (~$3,000)\")\nprint(\"  ‚úì Ultrasonic (12 units): Close-range (~$240)\")\nprint(\"  ‚úì GPS + IMU: High-precision localization (~$2,000)\")\nprint(\"\\nTotal Cost: ~$21,000\")\nprint(\"\\nJustification:\")\nprint(\"  ‚Ä¢ Redundancy critical at highway speeds (110 km/h)\")\nprint(\"  ‚Ä¢ Radar essential for rain/fog/night operation\")\nprint(\"  ‚Ä¢ LiDAR for precise distance measurement\")\nprint(\"  ‚Ä¢ Multi-modal fusion increases reliability\")\nprint(\"\\nFailure Modes:\")\nprint(\"  ‚ö† Heavy snow degrades all sensors\")\nprint(\"  ‚ö† Sensor synchronization failures\")\nprint(\"  ‚ö† GPS jamming/spoofing\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"EXERCISE 4: System Architecture Trade-offs\")\nprint(\"=\" * 70)\n\ncomparison = {\n    'Debugging': {\n        'Modular': '‚úì EASIER - Can isolate which module (perception/planning/control) failed',\n        'E2E': '‚úó HARDER - Black box behavior, difficult to identify root cause'\n    },\n    'Training Data': {\n        'Modular': '‚úì LESS - Each module trained independently with smaller datasets',\n        'E2E': '‚úó MORE - Needs millions of miles to learn all scenarios end-to-end'\n    },\n    'Interpretability': {\n        'Modular': '‚úì HIGH - Can visualize detections, planned paths, control commands',\n        'E2E': '‚úó LOW - Cannot explain why network chose specific steering angle'\n    },\n    'Performance': {\n        'Modular': '‚ö† May have sub-optimal decisions due to pipeline losses',\n        'E2E': '‚úì Theoretically optimal if trained on enough data'\n    }\n}\n\nfor aspect, approaches in comparison.items():\n    print(f\"\\n{aspect}:\")\n    for approach, description in approaches.items():\n        print(f\"  {approach}: {description}\")\n\nprint(\"\\n\\nRecommendation for Level 4 Robotaxi:\")\nprint(\"-\" * 70)\nprint(\"Choose MODULAR APPROACH because:\")\nprint(\"  1. Safety certification requires interpretability\")\nprint(\"  2. Easier to update individual modules (e.g., better object detector)\")\nprint(\"  3. Debugging failures is critical for continuous improvement\")\nprint(\"  4. Regulatory approval requires explainable decisions\")\nprint(\"\\nNote: Many production systems use HYBRID approach:\")\nprint(\"  ‚Ä¢ E2E learning for perception (camera ‚Üí objects)\")\nprint(\"  ‚Ä¢ Modular planning and control for interpretability\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"EXERCISE 5: Operational Design Domain (ODD)\")\nprint(\"=\" * 70)\n\nprint(\"\\nCampus Autonomous Shuttle ODD:\")\nprint(\"-\" * 70)\n\nodd_spec = {\n    'Geographic Boundaries': [\n        '‚Ä¢ Main campus roads (pre-mapped HD map)',\n        '‚Ä¢ 3km x 2km area',\n        '‚Ä¢ Excludes highways and public streets',\n        '‚Ä¢ 6 designated shuttle stops'\n    ],\n    'Environmental Conditions': [\n        '‚Ä¢ Daytime only (6 AM - 8 PM)',\n        '‚Ä¢ Dry weather (no operation during rain/snow)',\n        '‚Ä¢ Temperature: 0¬∞C to 40¬∞C',\n        '‚Ä¢ Visibility > 100m'\n    ],\n    'Road Types': [\n        '‚Ä¢ Paved roads with clear lane markings',\n        '‚Ä¢ Maximum 10% grade (hills)',\n        '‚Ä¢ Pedestrian crossings with signage',\n        '‚Ä¢ No unmarked parking zones'\n    ],\n    'Speed Limits': [\n        '‚Ä¢ Maximum: 25 km/h (15 mph)',\n        '‚Ä¢ School zones: 15 km/h (9 mph)',\n        '‚Ä¢ Near stops: 10 km/h (6 mph)'\n    ],\n    'Edge Cases (Require Human/Remote Takeover)': [\n        '‚Ä¢ Construction or road closures',\n        '‚Ä¢ Large crowds blocking road (game day)',\n        '‚Ä¢ Emergency vehicles approaching',\n        '‚Ä¢ Sensor failures or degradation',\n        '‚Ä¢ Unexpected objects (fallen tree, debris)',\n        '‚Ä¢ Aggressive/erratic drivers'\n    ]\n}\n\nfor category, details in odd_spec.items():\n    print(f\"\\n{category}:\")\n    for detail in details:\n        print(f\"  {detail}\")\n\nprint(\"\\n\\nJustification Based on Technology Limitations:\")\nprint(\"-\" * 70)\nprint(\"‚Ä¢ Daytime only: Camera-based perception struggles in darkness\")\nprint(\"‚Ä¢ Dry weather: LiDAR/camera degrade in rain; puddles confuse sensors\")\nprint(\"‚Ä¢ Low speed: Longer reaction time, reduces severity of failures\")\nprint(\"‚Ä¢ Pre-mapped area: Eliminates need for SLAM, uses HD map localization\")\nprint(\"‚Ä¢ Pedestrian-friendly: Campus has mixed traffic, requires conservative ODD\")\nprint(\"‚Ä¢ Remote takeover: Handles edge cases without onboard safety driver\")\n\nprint(\"\\n\" + \"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## References\n\n### SAE J3016 Standard\n- **SAE J3016_202104**: Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles (April 2021 update)\n- [SAE International](https://www.sae.org/standards/content/j3016_202104/)\n\n### Historical References\n- **Dickmanns, E. D.** (2007). *Dynamic Vision for Perception and Control of Motion*. Springer.\n- **Thrun, S., et al.** (2006). \"Stanley: The Robot That Won the DARPA Grand Challenge\". *Journal of Field Robotics*, 23(9), 661-692.\n- **Urmson, C., et al.** (2008). \"Autonomous Driving in Urban Environments: Boss and the Urban Challenge\". *Journal of Field Robotics*, 25(8), 425-466.\n\n### Industry Reports\n- **Waymo Safety Report** (2021). [Waymo Safety Methodology](https://waymo.com/safety/)\n- **NHTSA** (2023). Automated Vehicles for Safety. [NHTSA AV Policy](https://www.nhtsa.gov/technology-innovation/automated-vehicles-safety)\n- **McKinsey & Company** (2023). \"The Future of Autonomous Vehicles\". Mobility & Automotive Practice.\n\n### Technical Books\n- **Paden, B., et al.** (2016). \"A Survey of Motion Planning and Control Techniques for Self-Driving Urban Vehicles\". *IEEE Transactions on Intelligent Vehicles*, 1(1), 33-55.\n- **Thrun, S., Burgard, W., & Fox, D.** (2005). *Probabilistic Robotics*. MIT Press.\n- **Rajamani, R.** (2012). *Vehicle Dynamics and Control* (2nd ed.). Springer.\n\n### Open-Source Frameworks\n- **ROS 2** (Robot Operating System): [https://docs.ros.org/](https://docs.ros.org/)\n- **Baidu Apollo**: [https://github.com/ApolloAuto/apollo](https://github.com/ApolloAuto/apollo)\n- **Autoware Foundation**: [https://www.autoware.org/](https://www.autoware.org/)\n\n### Online Resources\n- **MIT 6.S094: Deep Learning for Self-Driving Cars**: [https://selfdrivingcars.mit.edu/](https://selfdrivingcars.mit.edu/)\n- **Udacity Self-Driving Car Nanodegree**: [https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd0013)\n- **CARLA Simulator** (Open-source simulator for AVs): [https://carla.org/](https://carla.org/)\n\n### Research Papers\n- **Dosovitskiy, A., et al.** (2017). \"CARLA: An Open Urban Driving Simulator\". *Conference on Robot Learning (CoRL)*.\n- **Chen, C., et al.** (2015). \"DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving\". *ICCV 2015*.\n- **Bojarski, M., et al.** (2016). \"End to End Learning for Self-Driving Cars\". *arXiv:1604.07316*.\n\n---\n\n### Additional Reading\n\n- **ISO 26262**: Road Vehicles - Functional Safety Standard\n- **SOTIF (ISO/PAS 21448)**: Safety of the Intended Functionality\n- **ADAS Regulations**: UNECE WP.29 Automated Driving Standards\n\n---\n\n## Course Project Connection\n\nThis notebook provides the foundational understanding for the full course. In upcoming weeks, you'll dive deeper into:\n\n- **Week 3**: Control algorithms (PID, MPC) - See our interactive demos in `pid_ball_chase.ipynb`\n- **Week 6**: State estimation (Kalman Filter) - See our interactive demo in `kalman_ball_chase.ipynb`\n- **Weeks 4-7**: Perception and localization systems\n- **Weeks 8-11**: Planning and decision-making algorithms\n\n**Next Steps**: Proceed to [Week 2: Vehicle Dynamics & Kinematics](week02_vehicle_dynamics_kinematics.ipynb)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}