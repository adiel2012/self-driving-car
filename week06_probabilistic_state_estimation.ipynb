{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 6: Probabilistic State Estimation\n\n## Module II: Perception & Localization\n\n### Topics Covered\n\n- Modeling Uncertainty with Probability Distributions\n- Bayes Filter Framework (Predict + Update)\n- Kalman Filter for Linear Systems\n- Extended Kalman Filter (EKF) for Nonlinear Systems\n- Unscented Kalman Filter (UKF) for Highly Nonlinear Systems\n- Sensor Fusion Applications\n\n---\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. Understand probabilistic representations of uncertainty (Gaussian distributions)\n2. Implement the Bayes filter framework for recursive state estimation\n3. Build and tune a Kalman Filter for linear state estimation problems\n4. Extend to EKF for nonlinear systems using Jacobian linearization\n5. Implement UKF using sigma points for better nonlinear handling\n6. Apply filters to sensor fusion (GPS + IMU, LIDAR + RADAR)\n7. Compare performance of different filter types on various scenarios\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Additional imports as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Modeling Uncertainty\n\nIn autonomous systems, we never have perfect information about the world. Sensors are noisy, models are approximate, and the environment is unpredictable. Probabilistic state estimation provides a principled framework for reasoning under uncertainty.\n\n### Sources of Uncertainty\n\n1. **Measurement Noise**: Sensors provide imperfect observations\n   - GPS accuracy: ±5-10 meters\n   - LIDAR noise: ±2-3 cm\n   - Camera pixel uncertainty\n\n2. **Process Noise**: System dynamics are not perfectly modeled\n   - Wind effects on vehicle motion\n   - Tire slip\n   - Unmodeled disturbances\n\n3. **Model Uncertainty**: Simplified representations of reality\n   - Linearization errors\n   - Parameter uncertainty\n\n### Probability Distributions\n\nWe represent uncertainty using probability distributions:\n\n**Gaussian (Normal) Distribution:**\n$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n\n- Mean $\\mu$: Expected value\n- Variance $\\sigma^2$: Uncertainty\n\n**Multivariate Gaussian:**\n$$p(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^n|\\boldsymbol{\\Sigma}|}} e^{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})}$$\n\n- Mean vector $\\boldsymbol{\\mu}$\n- Covariance matrix $\\boldsymbol{\\Sigma}$ (captures correlations)"
  },
  {
   "cell_type": "code",
   "source": "# Complete Kalman Filter Implementation\n\nclass KalmanFilter:\n    \"\"\"Linear Kalman Filter for state estimation.\"\"\"\n    \n    def __init__(self, F, H, Q, R, x0, P0):\n        \"\"\"\n        Initialize Kalman Filter.\n        \n        Args:\n            F: State transition matrix (n x n)\n            H: Measurement matrix (m x n)\n            Q: Process noise covariance (n x n)\n            R: Measurement noise covariance (m x m)\n            x0: Initial state estimate (n x 1)\n            P0: Initial covariance estimate (n x n)\n        \"\"\"\n        self.F = F  # State transition\n        self.H = H  # Measurement\n        self.Q = Q  # Process noise\n        self.R = R  # Measurement noise\n        \n        self.x = x0  # State estimate\n        self.P = P0  # Covariance estimate\n        \n        self.n = F.shape[0]  # State dimension\n        self.m = H.shape[0]  # Measurement dimension\n    \n    def predict(self, u=None):\n        \"\"\"\n        Prediction step.\n        \n        Args:\n            u: Control input (optional)\n        \"\"\"\n        # State prediction: x̂⁻ = F * x̂\n        self.x = self.F @ self.x\n        if u is not None:\n            self.x += u\n        \n        # Covariance prediction: P⁻ = F * P * F^T + Q\n        self.P = self.F @ self.P @ self.F.T + self.Q\n    \n    def update(self, z):\n        \"\"\"\n        Update step.\n        \n        Args:\n            z: Measurement vector (m x 1)\n        \"\"\"\n        # Innovation: y = z - H * x̂⁻\n        y = z - self.H @ self.x\n        \n        # Innovation covariance: S = H * P⁻ * H^T + R\n        S = self.H @ self.P @ self.H.T + self.R\n        \n        # Kalman gain: K = P⁻ * H^T * S^(-1)\n        K = self.P @ self.H.T @ np.linalg.inv(S)\n        \n        # State update: x̂ = x̂⁻ + K * y\n        self.x = self.x + K @ y\n        \n        # Covariance update: P = (I - K * H) * P⁻\n        I = np.eye(self.n)\n        self.P = (I - K @ self.H) @ self.P\n    \n    def get_state(self):\n        \"\"\"Return current state estimate.\"\"\"\n        return self.x.copy()\n    \n    def get_covariance(self):\n        \"\"\"Return current covariance estimate.\"\"\"\n        return self.P.copy()\n\n\n# Demonstration: 2D Position and Velocity Tracking\n\nnp.random.seed(42)\n\n# Simulation parameters\ndt = 0.1  # Time step\nn_steps = 100\nt = np.arange(n_steps) * dt\n\n# True trajectory (sinusoidal motion)\ntrue_x = 10 * np.sin(0.5 * t)\ntrue_vx = 10 * 0.5 * np.cos(0.5 * t)\ntrue_y = 5 * t\ntrue_vy = 5 * np.ones(n_steps)\n\n# Generate noisy measurements (position only)\nmeasurement_noise_std = 2.0\nmeasured_x = true_x + np.random.randn(n_steps) * measurement_noise_std\nmeasured_y = true_y + np.random.randn(n_steps) * measurement_noise_std\n\n# Kalman Filter setup\n# State: [x, vx, y, vy]\n# Measurement: [x, y]\n\n# State transition matrix (constant velocity model)\nF = np.array([\n    [1, dt, 0, 0],\n    [0, 1, 0, 0],\n    [0, 0, 1, dt],\n    [0, 0, 0, 1]\n])\n\n# Measurement matrix (observe position only)\nH = np.array([\n    [1, 0, 0, 0],\n    [0, 0, 1, 0]\n])\n\n# Process noise covariance\nprocess_noise_std = 0.5\nQ = np.array([\n    [dt**4/4, dt**3/2, 0, 0],\n    [dt**3/2, dt**2, 0, 0],\n    [0, 0, dt**4/4, dt**3/2],\n    [0, 0, dt**3/2, dt**2]\n]) * process_noise_std**2\n\n# Measurement noise covariance\nR = np.eye(2) * measurement_noise_std**2\n\n# Initial state and covariance\nx0 = np.array([0, 0, 0, 5])  # Initial guess\nP0 = np.eye(4) * 10  # High initial uncertainty\n\n# Create filter\nkf = KalmanFilter(F, H, Q, R, x0, P0)\n\n# Run filter\nestimates = []\ncovariances = []\n\nfor i in range(n_steps):\n    # Prediction\n    kf.predict()\n    \n    # Update with measurement\n    z = np.array([measured_x[i], measured_y[i]])\n    kf.update(z)\n    \n    # Store results\n    estimates.append(kf.get_state())\n    covariances.append(kf.get_covariance())\n\nestimates = np.array(estimates)\ncovariances = np.array(covariances)\n\n# Visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# Plot 1: 2D trajectory\nax1 = axes[0, 0]\nax1.plot(true_x, true_y, 'g-', linewidth=3, label='True Trajectory', alpha=0.8)\nax1.scatter(measured_x, measured_y, c='red', s=30, alpha=0.5, label='Measurements', zorder=3)\nax1.plot(estimates[:, 0], estimates[:, 2], 'b-', linewidth=2, label='KF Estimate')\n\n# Plot uncertainty ellipses at selected points\nfor i in range(0, n_steps, 20):\n    # Extract position covariance\n    P_pos = covariances[i][[0, 2]][:, [0, 2]]\n    \n    # Compute eigenvalues/eigenvectors for ellipse\n    eigenvalues, eigenvectors = np.linalg.eigh(P_pos)\n    angle = np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0])\n    \n    # 95% confidence ellipse (chi-square with 2 DOF: 5.991)\n    width, height = 2 * np.sqrt(5.991 * eigenvalues)\n    \n    from matplotlib.patches import Ellipse\n    ellipse = Ellipse((estimates[i, 0], estimates[i, 2]), width, height,\n                      angle=np.degrees(angle), facecolor='blue', \n                      alpha=0.1, edgecolor='blue', linewidth=1)\n    ax1.add_patch(ellipse)\n\nax1.set_xlabel('X Position (m)', fontsize=12)\nax1.set_ylabel('Y Position (m)', fontsize=12)\nax1.set_title('2D Trajectory Tracking', fontsize=14, fontweight='bold')\nax1.legend(fontsize=10)\nax1.grid(True, alpha=0.3)\nax1.axis('equal')\n\n# Plot 2: X position over time\nax2 = axes[0, 1]\nax2.plot(t, true_x, 'g-', linewidth=2, label='True')\nax2.scatter(t, measured_x, c='red', s=20, alpha=0.5, label='Measured')\nax2.plot(t, estimates[:, 0], 'b-', linewidth=2, label='KF Estimate')\n\n# Uncertainty bounds (±2σ)\nstd_x = np.sqrt(covariances[:, 0, 0])\nax2.fill_between(t, estimates[:, 0] - 2*std_x, estimates[:, 0] + 2*std_x,\n                 alpha=0.2, color='blue', label='95% Confidence')\n\nax2.set_xlabel('Time (s)', fontsize=12)\nax2.set_ylabel('X Position (m)', fontsize=12)\nax2.set_title('X Position Tracking', fontsize=14, fontweight='bold')\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\n# Plot 3: X velocity over time\nax3 = axes[0, 2]\nax3.plot(t, true_vx, 'g-', linewidth=2, label='True')\nax3.plot(t, estimates[:, 1], 'b-', linewidth=2, label='KF Estimate')\n\n# Uncertainty bounds\nstd_vx = np.sqrt(covariances[:, 1, 1])\nax3.fill_between(t, estimates[:, 1] - 2*std_vx, estimates[:, 1] + 2*std_vx,\n                 alpha=0.2, color='blue', label='95% Confidence')\n\nax3.set_xlabel('Time (s)', fontsize=12)\nax3.set_ylabel('X Velocity (m/s)', fontsize=12)\nax3.set_title('X Velocity Estimation', fontsize=14, fontweight='bold')\nax3.legend(fontsize=10)\nax3.grid(True, alpha=0.3)\n\n# Plot 4: Y position over time\nax4 = axes[1, 0]\nax4.plot(t, true_y, 'g-', linewidth=2, label='True')\nax4.scatter(t, measured_y, c='red', s=20, alpha=0.5, label='Measured')\nax4.plot(t, estimates[:, 2], 'b-', linewidth=2, label='KF Estimate')\n\n# Uncertainty bounds\nstd_y = np.sqrt(covariances[:, 2, 2])\nax4.fill_between(t, estimates[:, 2] - 2*std_y, estimates[:, 2] + 2*std_y,\n                 alpha=0.2, color='blue', label='95% Confidence')\n\nax4.set_xlabel('Time (s)', fontsize=12)\nax4.set_ylabel('Y Position (m)', fontsize=12)\nax4.set_title('Y Position Tracking', fontsize=14, fontweight='bold')\nax4.legend(fontsize=10)\nax4.grid(True, alpha=0.3)\n\n# Plot 5: Position error over time\nax5 = axes[1, 1]\nposition_error = np.sqrt((estimates[:, 0] - true_x)**2 + (estimates[:, 2] - true_y)**2)\nmeasurement_error = np.sqrt((measured_x - true_x)**2 + (measured_y - true_y)**2)\n\nax5.plot(t, measurement_error, 'r-', linewidth=1.5, alpha=0.7, label='Measurement Error')\nax5.plot(t, position_error, 'b-', linewidth=2, label='KF Error')\nax5.axhline(np.mean(position_error), color='b', linestyle='--', linewidth=1.5,\n           label=f'Mean KF: {np.mean(position_error):.2f}m')\n\nax5.set_xlabel('Time (s)', fontsize=12)\nax5.set_ylabel('Position Error (m)', fontsize=12)\nax5.set_title('Tracking Error Comparison', fontsize=14, fontweight='bold')\nax5.legend(fontsize=10)\nax5.grid(True, alpha=0.3)\n\n# Plot 6: Uncertainty reduction over time\nax6 = axes[1, 2]\ntrace_P = np.array([np.trace(P) for P in covariances])\ndet_P = np.array([np.linalg.det(P) for P in covariances])\n\nax6_twin = ax6.twinx()\nline1 = ax6.plot(t, trace_P, 'b-', linewidth=2, label='Trace(P)')\nline2 = ax6_twin.plot(t, det_P, 'r-', linewidth=2, label='Det(P)')\n\nax6.set_xlabel('Time (s)', fontsize=12)\nax6.set_ylabel('Trace(P)', fontsize=12, color='b')\nax6_twin.set_ylabel('Det(P)', fontsize=12, color='r')\nax6.set_title('Uncertainty Reduction', fontsize=14, fontweight='bold')\nax6.tick_params(axis='y', labelcolor='b')\nax6_twin.tick_params(axis='y', labelcolor='r')\n\nlines = line1 + line2\nlabels = [l.get_label() for l in lines]\nax6.legend(lines, labels, fontsize=10)\nax6.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print statistics\nprint(\"=\" * 70)\nprint(\"KALMAN FILTER PERFORMANCE\")\nprint(\"=\" * 70)\nprint(f\"Simulation time: {n_steps * dt:.1f} seconds\")\nprint(f\"Time step: {dt} seconds\")\nprint(f\"Measurement noise std: {measurement_noise_std:.2f} m\")\nprint(f\"Process noise std: {process_noise_std:.2f}\")\nprint(f\"\\nPosition Error:\")\nprint(f\"  Measurement RMSE: {np.sqrt(np.mean(measurement_error**2)):.3f} m\")\nprint(f\"  KF RMSE: {np.sqrt(np.mean(position_error**2)):.3f} m\")\nprint(f\"  Improvement: {(1 - np.sqrt(np.mean(position_error**2))/np.sqrt(np.mean(measurement_error**2)))*100:.1f}%\")\nprint(f\"\\nVelocity Estimation (not directly measured):\")\nvx_error = np.abs(estimates[:, 1] - true_vx)\nvy_error = np.abs(estimates[:, 3] - true_vy)\nprint(f\"  X velocity MAE: {np.mean(vx_error):.3f} m/s\")\nprint(f\"  Y velocity MAE: {np.mean(vy_error):.3f} m/s\")\nprint(f\"\\nFinal Uncertainty (trace of covariance):\")\nprint(f\"  Initial: {np.trace(P0):.3f}\")\nprint(f\"  Final: {trace_P[-1]:.3f}\")\nprint(f\"  Reduction: {(1 - trace_P[-1]/np.trace(P0))*100:.1f}%\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Bayes Filter Framework\n\nThe **Bayes filter** is a recursive algorithm for estimating the state of a dynamic system from noisy observations.\n\n### Recursive State Estimation\n\n**Goal**: Estimate state $\\mathbf{x}_t$ given:\n- All measurements up to time $t$: $\\mathbf{z}_{1:t}$\n- All controls up to time $t$: $\\mathbf{u}_{1:t}$\n\n**Belief**: Probability distribution over states\n$$bel(\\mathbf{x}_t) = p(\\mathbf{x}_t | \\mathbf{z}_{1:t}, \\mathbf{u}_{1:t})$$\n\n---\n\n### Bayes Filter Algorithm\n\n**Two-step process**:\n\n#### **1. Prediction Step** (Motion Update)\n\nPredict state based on motion model:\n$$\\overline{bel}(\\mathbf{x}_t) = \\int p(\\mathbf{x}_t | \\mathbf{u}_t, \\mathbf{x}_{t-1}) \\, bel(\\mathbf{x}_{t-1}) \\, d\\mathbf{x}_{t-1}$$\n\n**Intuition**: Propagate belief forward using system dynamics.\n\n#### **2. Update Step** (Measurement Update)\n\nCorrect prediction using sensor measurement:\n$$bel(\\mathbf{x}_t) = \\eta \\, p(\\mathbf{z}_t | \\mathbf{x}_t) \\, \\overline{bel}(\\mathbf{x}_t)$$\n\nWhere $\\eta$ is a normalization constant.\n\n**Intuition**: Weight prediction by likelihood of measurement.\n\n---\n\n### Assumptions\n\n1. **Markov Property**: Future states depend only on current state\n   $$p(\\mathbf{x}_t | \\mathbf{x}_{0:t-1}, \\mathbf{u}_{1:t}, \\mathbf{z}_{1:t-1}) = p(\\mathbf{x}_t | \\mathbf{x}_{t-1}, \\mathbf{u}_t)$$\n\n2. **Measurement Independence**: Measurements depend only on current state\n   $$p(\\mathbf{z}_t | \\mathbf{x}_{0:t}, \\mathbf{u}_{1:t}, \\mathbf{z}_{1:t-1}) = p(\\mathbf{z}_t | \\mathbf{x}_t)$$\n\n---\n\n### Kalman Filter: Gaussian Bayes Filter\n\n**Kalman Filter** implements the Bayes filter for linear-Gaussian systems.\n\n**Key Insight**: If:\n- Initial belief is Gaussian\n- Motion and measurement models are linear with Gaussian noise\n- Then belief remains Gaussian at all times!\n\n**Representation**: Only need to track mean $\\boldsymbol{\\mu}_t$ and covariance $\\boldsymbol{\\Sigma}_t$\n\n$$bel(\\mathbf{x}_t) = \\mathcal{N}(\\boldsymbol{\\mu}_t, \\boldsymbol{\\Sigma}_t)$$\n\n---\n\n## 3. Kalman Filter (Linear Systems)\n\n**Kalman Filter** is optimal (minimum mean squared error) for linear systems with Gaussian noise.\n\n### System Model\n\n**State Transition** (Motion Model):\n$$\\mathbf{x}_t = \\mathbf{F}_t \\mathbf{x}_{t-1} + \\mathbf{B}_t \\mathbf{u}_t + \\mathbf{w}_t$$\n\nWhere:\n- $\\mathbf{F}_t$: State transition matrix\n- $\\mathbf{B}_t$: Control input matrix\n- $\\mathbf{w}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q}_t)$: Process noise\n\n**Measurement Model**:\n$$\\mathbf{z}_t = \\mathbf{H}_t \\mathbf{x}_t + \\mathbf{v}_t$$\n\nWhere:\n- $\\mathbf{H}_t$: Measurement matrix\n- $\\mathbf{v}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R}_t)$: Measurement noise\n\n---\n\n### Algorithm\n\n**Prediction Step**:\n\n$$\\boldsymbol{\\mu}_t^- = \\mathbf{F}_t \\boldsymbol{\\mu}_{t-1} + \\mathbf{B}_t \\mathbf{u}_t$$\n$$\\boldsymbol{\\Sigma}_t^- = \\mathbf{F}_t \\boldsymbol{\\Sigma}_{t-1} \\mathbf{F}_t^T + \\mathbf{Q}_t$$\n\n**Update Step**:\n\n**Innovation** (measurement residual):\n$$\\mathbf{y}_t = \\mathbf{z}_t - \\mathbf{H}_t \\boldsymbol{\\mu}_t^-$$\n\n**Innovation covariance**:\n$$\\mathbf{S}_t = \\mathbf{H}_t \\boldsymbol{\\Sigma}_t^- \\mathbf{H}_t^T + \\mathbf{R}_t$$\n\n**Kalman Gain**:\n$$\\mathbf{K}_t = \\boldsymbol{\\Sigma}_t^- \\mathbf{H}_t^T \\mathbf{S}_t^{-1}$$\n\n**State update**:\n$$\\boldsymbol{\\mu}_t = \\boldsymbol{\\mu}_t^- + \\mathbf{K}_t \\mathbf{y}_t$$\n\n**Covariance update**:\n$$\\boldsymbol{\\Sigma}_t = (\\mathbf{I} - \\mathbf{K}_t \\mathbf{H}_t) \\boldsymbol{\\Sigma}_t^-$$\n\n---\n\n### Example: 1D Position Tracking\n\n**State**: $x_t$ = position\n**Measurement**: $z_t$ = noisy position\n\n**Motion model**: $x_t = x_{t-1} + v \\cdot \\Delta t + w_t$ (constant velocity)\n**Measurement model**: $z_t = x_t + v_t$\n\n**Matrices**:\n- $F = 1$ (identity)\n- $H = 1$ (direct measurement)\n- $Q$ = process noise variance\n- $R$ = measurement noise variance\n\n---\n\n### Tuning Parameters\n\n**Process Noise $\\mathbf{Q}$**:\n- **High $\\mathbf{Q}$**: Trust measurements more (filter responsive)\n- **Low $\\mathbf{Q}$**: Trust model more (filter smooth)\n\n**Measurement Noise $\\mathbf{R}$**:\n- **High $\\mathbf{R}$**: Trust model more (ignore noisy sensors)\n- **Low $\\mathbf{R}$**: Trust measurements more (follow sensors)\n\n**Rule of thumb**: Set based on known sensor/model characteristics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Unscented Kalman Filter Implementation\n\nclass UnscentedKalmanFilter:\n    def __init__(self, n_states, initial_state, initial_cov, process_noise, measurement_noise,\n                 alpha=1e-3, beta=2, kappa=0):\n        \"\"\"\n        UKF implementation\n        \n        Parameters:\n        - n_states: dimension of state vector\n        - alpha: spread of sigma points (typically 1e-3 to 1)\n        - beta: incorporates prior knowledge (2 is optimal for Gaussian)\n        - kappa: secondary scaling parameter (typically 0 or 3-n)\n        \"\"\"\n        self.n = n_states\n        self.x = initial_state\n        self.P = initial_cov\n        self.Q = process_noise\n        self.R = measurement_noise\n        \n        # UKF parameters\n        self.alpha = alpha\n        self.beta = beta\n        self.kappa = kappa\n        \n        self.lambda_ = alpha**2 * (n_states + kappa) - n_states\n        \n        # Weights for mean and covariance\n        self.Wm = np.zeros(2 * n_states + 1)\n        self.Wc = np.zeros(2 * n_states + 1)\n        \n        self.Wm[0] = self.lambda_ / (n_states + self.lambda_)\n        self.Wc[0] = self.lambda_ / (n_states + self.lambda_) + (1 - alpha**2 + beta)\n        \n        for i in range(1, 2 * n_states + 1):\n            self.Wm[i] = 1.0 / (2.0 * (n_states + self.lambda_))\n            self.Wc[i] = self.Wm[i]\n    \n    def generate_sigma_points(self, x, P):\n        \"\"\"Generate sigma points around x with covariance P\"\"\"\n        n = len(x)\n        sigma_points = np.zeros((2 * n + 1, n))\n        \n        # Center point\n        sigma_points[0] = x\n        \n        # Matrix square root using Cholesky decomposition\n        try:\n            U = np.linalg.cholesky((n + self.lambda_) * P)\n        except np.linalg.LinAlgError:\n            # If Cholesky fails, use eigenvalue decomposition\n            eigvals, eigvecs = np.linalg.eigh(P)\n            eigvals = np.maximum(eigvals, 0)  # Ensure positive\n            U = eigvecs @ np.diag(np.sqrt((n + self.lambda_) * eigvals))\n        \n        # Positive sigma points\n        for i in range(n):\n            sigma_points[i + 1] = x + U[:, i]\n        \n        # Negative sigma points\n        for i in range(n):\n            sigma_points[n + i + 1] = x - U[:, i]\n        \n        return sigma_points\n    \n    def predict(self, dt, f):\n        \"\"\"\n        Prediction step\n        f: state transition function f(x, dt) -> x_new\n        \"\"\"\n        # Generate sigma points\n        sigma_points = self.generate_sigma_points(self.x, self.P)\n        \n        # Propagate sigma points through motion model\n        n = len(self.x)\n        sigma_points_pred = np.zeros((2 * n + 1, n))\n        \n        for i in range(2 * n + 1):\n            sigma_points_pred[i] = f(sigma_points[i], dt)\n        \n        # Compute predicted mean\n        self.x = np.sum(self.Wm[:, np.newaxis] * sigma_points_pred, axis=0)\n        \n        # Compute predicted covariance\n        self.P = np.zeros((n, n))\n        for i in range(2 * n + 1):\n            diff = sigma_points_pred[i] - self.x\n            self.P += self.Wc[i] * np.outer(diff, diff)\n        \n        self.P += self.Q\n    \n    def update(self, z, h):\n        \"\"\"\n        Update step\n        z: measurement vector\n        h: measurement function h(x) -> z_pred\n        \"\"\"\n        n = len(self.x)\n        m = len(z)\n        \n        # Generate sigma points from predicted state\n        sigma_points = self.generate_sigma_points(self.x, self.P)\n        \n        # Propagate sigma points through measurement model\n        sigma_meas = np.zeros((2 * n + 1, m))\n        for i in range(2 * n + 1):\n            sigma_meas[i] = h(sigma_points[i])\n        \n        # Predicted measurement\n        z_pred = np.sum(self.Wm[:, np.newaxis] * sigma_meas, axis=0)\n        \n        # Innovation covariance\n        S = np.zeros((m, m))\n        for i in range(2 * n + 1):\n            diff = sigma_meas[i] - z_pred\n            S += self.Wc[i] * np.outer(diff, diff)\n        S += self.R\n        \n        # Cross-correlation\n        Pxz = np.zeros((n, m))\n        for i in range(2 * n + 1):\n            dx = sigma_points[i] - self.x\n            dz = sigma_meas[i] - z_pred\n            Pxz += self.Wc[i] * np.outer(dx, dz)\n        \n        # Kalman gain\n        K = Pxz @ np.linalg.inv(S)\n        \n        # State update\n        self.x = self.x + K @ (z - z_pred)\n        \n        # Covariance update\n        self.P = self.P - K @ S @ K.T\n\n\n# Comparison: UKF vs EKF on highly nonlinear system\ndef compare_ukf_ekf():\n    np.random.seed(42)\n    \n    # State transition (nonlinear)\n    def f(x, dt):\n        \"\"\"Constant turn rate and velocity model\"\"\"\n        px, py, v, theta, omega = x\n        if abs(omega) < 1e-6:  # Straight line motion\n            return np.array([\n                px + v * np.cos(theta) * dt,\n                py + v * np.sin(theta) * dt,\n                v,\n                theta,\n                omega\n            ])\n        else:  # Curved motion\n            return np.array([\n                px + v / omega * (np.sin(theta + omega * dt) - np.sin(theta)),\n                py + v / omega * (-np.cos(theta + omega * dt) + np.cos(theta)),\n                v,\n                theta + omega * dt,\n                omega\n            ])\n    \n    # Measurement model (range-bearing)\n    def h(x):\n        px, py, _, _, _ = x\n        r = np.sqrt(px**2 + py**2)\n        bearing = np.arctan2(py, px)\n        return np.array([r, bearing])\n    \n    # Simulation\n    dt = 0.1\n    num_steps = 100\n    \n    # True initial state: [px, py, v, theta, omega]\n    x_true = np.array([0.0, 0.0, 5.0, np.pi/4, 0.1])  # Turning motion\n    \n    true_states = []\n    measurements = []\n    \n    for t in range(num_steps):\n        x_true = f(x_true, dt)\n        true_states.append(x_true.copy())\n        \n        # Noisy measurement\n        z_true = h(x_true)\n        z_meas = z_true + np.array([np.random.randn() * 2.0, np.random.randn() * 0.1])\n        measurements.append(z_meas)\n    \n    true_states = np.array(true_states)\n    measurements = np.array(measurements)\n    \n    # Initialize UKF\n    x0 = np.array([0.0, 0.0, 4.0, np.pi/4, 0.0])  # Initial guess\n    P0 = np.diag([5.0, 5.0, 2.0, 0.5, 0.2])\n    Q = np.diag([0.1, 0.1, 0.1, 0.01, 0.01])\n    R = np.diag([4.0, 0.01])\n    \n    ukf = UnscentedKalmanFilter(5, x0.copy(), P0.copy(), Q, R)\n    \n    ukf_estimates = []\n    \n    for i in range(num_steps):\n        ukf.predict(dt, f)\n        ukf.update(measurements[i], h)\n        ukf_estimates.append(ukf.x.copy())\n    \n    ukf_estimates = np.array(ukf_estimates)\n    \n    # Visualization\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Trajectory\n    axes[0].plot(true_states[:, 0], true_states[:, 1], 'g-', label='True', linewidth=2)\n    axes[0].plot(ukf_estimates[:, 0], ukf_estimates[:, 1], 'b-', label='UKF', linewidth=2)\n    axes[0].plot(0, 0, 'r*', markersize=15, label='Sensor')\n    axes[0].set_xlabel('X Position')\n    axes[0].set_ylabel('Y Position')\n    axes[0].set_title('UKF Tracking (Nonlinear Turning Motion)')\n    axes[0].legend()\n    axes[0].grid(True)\n    axes[0].axis('equal')\n    \n    # Position error\n    position_error = np.sqrt((ukf_estimates[:, 0] - true_states[:, 0])**2 + \n                            (ukf_estimates[:, 1] - true_states[:, 1])**2)\n    time = np.arange(num_steps) * dt\n    \n    axes[1].plot(time, position_error, 'b-', linewidth=2)\n    axes[1].set_xlabel('Time (s)')\n    axes[1].set_ylabel('Position Error (m)')\n    axes[1].set_title('UKF Position Error')\n    axes[1].grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"Mean Position Error: {np.mean(position_error):.2f} m\")\n    print(f\"Max Position Error: {np.max(position_error):.2f} m\")\n\ncompare_ukf_ekf()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercises\n\n### Exercise 1: 1D Kalman Filter for Temperature Tracking\n\nImplement a 1D Kalman filter to estimate room temperature from noisy sensor readings.\n\n**Given:**\n- True temperature: 20°C with small random walk (±0.1°C per step)\n- Sensor noise: ±1.5°C (standard deviation)\n- 100 time steps\n\n**Tasks:**\n1. Implement the 1D Kalman filter\n2. Plot true temperature, measurements, and estimates\n3. Calculate the mean squared error\n4. Experiment with different process and measurement noise values"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Exercise 3: This is left as a challenge for the student\n# Hint: Reuse the implementations from sections 3-5\n# Try different turning rates (omega values) to vary nonlinearity\n\n# Here's a template to get started:\n\ndef compare_all_filters():\n    \"\"\"\n    Compare Linear KF (will perform poorly), EKF, and UKF\n    on a turning vehicle scenario\n    \"\"\"\n    # TODO: Implement comparison\n    # 1. Set up nonlinear motion model (constant turn rate)\n    # 2. Initialize all three filters with same parameters\n    # 3. Run them on the same measurement sequence\n    # 4. Compare errors and computation time\n    # 5. Visualize results\n    \n    pass  # Your implementation here\n\n# Uncomment to run:\n# compare_all_filters()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## References and Additional Resources\n\n### Core Textbooks\n\n1. **Probabilistic Robotics** by Thrun, Burgard, and Fox (2005)\n   - The definitive reference for probabilistic state estimation\n   - Chapters 2-3: Recursive state estimation and Bayes filters\n   - Chapter 3.2: Kalman Filter\n   - Chapter 3.3: Extended Kalman Filter\n   - Chapter 3.4: Unscented Kalman Filter\n\n2. **Optimal State Estimation** by Simon (2006)\n   - Comprehensive treatment of Kalman filtering\n   - Practical implementation considerations\n\n3. **State Estimation for Robotics** by Barfoot (2017)\n   - Modern perspective with robotics applications\n   - Available online: http://asrl.utias.utoronto.ca/~tdb/bib/barfoot_ser17.pdf\n\n### Key Papers\n\n1. **Kalman, R. E. (1960)** - \"A New Approach to Linear Filtering and Prediction Problems\"\n   - Original Kalman filter paper\n   - Transactions of the ASME–Journal of Basic Engineering\n\n2. **Julier, S. J., & Uhlmann, J. K. (1997)** - \"New extension of the Kalman filter to nonlinear systems\"\n   - Introduced the Unscented Kalman Filter\n   - SPIE 3068, Signal Processing, Sensor Fusion, and Target Recognition VI\n\n### Online Resources\n\n1. **Kalman Filter Tutorial by Greg Welch and Gary Bishop**\n   - https://www.cs.unc.edu/~welch/kalman/\n   - Excellent introduction with practical examples\n\n2. **Roger Labbe's Kalman Filter Book (Jupyter Notebooks)**\n   - https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\n   - Interactive Python implementations\n\n3. **Udacity Self-Driving Car Nanodegree**\n   - Extended Kalman Filter project\n   - Sensor fusion with LIDAR and RADAR\n\n### Software Libraries\n\n1. **FilterPy** (Python)\n   - pip install filterpy\n   - Kalman filter implementations\n\n2. **PyKalman** (Python)\n   - pip install pykalman\n   - EM algorithm for parameter estimation\n\n3. **Robot Operating System (ROS)**\n   - robot_localization package\n   - Production-ready EKF/UKF implementations\n\n### Applications in Autonomous Vehicles\n\n1. **Localization**: GPS/IMU fusion for position estimation\n2. **Object Tracking**: Multi-object tracking with RADAR/LIDAR/Camera\n3. **SLAM**: Simultaneous Localization and Mapping\n4. **Sensor Calibration**: Estimating sensor biases and drift\n\n### Related Topics to Explore\n\n1. **Particle Filters**: Non-parametric Bayes filter for multimodal distributions\n2. **Information Filter**: Inverse covariance form of Kalman filter\n3. **Kalman Smoother**: Batch processing for improved estimates\n4. **Multi-Hypothesis Tracking**: Handling data association uncertainty\n5. **Adaptive Filtering**: Online estimation of Q and R matrices\n\n### Visualization Tools\n\n- **Matplotlib** (used in this notebook)\n- **Plotly**: Interactive visualizations\n- **RViz**: Real-time robotics visualization\n\n---\n\n## Summary\n\nThis notebook covered the fundamentals of probabilistic state estimation:\n\n**Key Concepts:**\n- Uncertainty representation with Gaussian distributions\n- Bayes filter framework (predict + update)\n- Kalman Filter for linear systems\n- Extended Kalman Filter for nonlinear systems (Jacobian linearization)\n- Unscented Kalman Filter for highly nonlinear systems (sigma points)\n\n**Practical Skills:**\n- Implementing filters from scratch\n- Tuning process and measurement noise\n- Sensor fusion techniques\n- Performance evaluation\n\n**Next Steps:**\n1. Explore the HTML visualizations in this repository ([kalman_ball_chase.html](kalman_ball_chase.html))\n2. Implement a real-world application (e.g., GPS/IMU fusion)\n3. Study Particle Filters for non-Gaussian distributions\n4. Learn about SLAM and its relation to state estimation"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}