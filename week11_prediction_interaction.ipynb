{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11: Prediction and Interaction\n",
    "\n",
    "### Topics Covered\n",
    "\n",
    "- Modeling driver behavior; Predicting the trajectories of other agents; Game Theory and Reinforcement Learning for complex intersections\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the key concepts\n",
    "2. Implement algorithms\n",
    "3. Apply techniques to real-world problems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle, Circle, Polygon, FancyArrowPatch, Wedge\nfrom matplotlib.animation import FuncAnimation\nfrom scipy.stats import multivariate_normal\nfrom scipy.optimize import minimize\nimport itertools\n\n# Set random seed for reproducibility\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Prediction Fundamentals\n\nPredicting the future behavior of other agents (vehicles, pedestrians, cyclists) is crucial for safe autonomous driving.\n\n### Why Prediction Matters\n\n- **Safety**: Anticipate dangerous situations before they occur\n- **Comfort**: Smooth driving requires anticipating other agents\n- **Efficiency**: Optimize routes based on predicted traffic flow\n- **Planning**: Generate ego trajectories that account for others' future states\n\n### The Prediction Problem\n\n**Given:**\n- Observed history: $H = \\{s_1, s_2, \\ldots, s_t\\}$ where $s_i = (x_i, y_i, v_i, \\theta_i)$\n- Context: Road geometry, traffic rules, map information\n\n**Predict:**\n- Future trajectory: $\\hat{T} = \\{s_{t+1}, s_{t+2}, \\ldots, s_{t+T_h}\\}$ over horizon $T_h$\n\n### Prediction Challenges\n\n1. **Uncertainty**: Multiple plausible futures\n2. **Multimodality**: Agents can take different actions (turn left, go straight, etc.)\n3. **Interaction**: Agents react to each other\n4. **Long horizons**: Uncertainty grows with time\n5. **Rare events**: Hard to learn from data\n\n### Prediction Approaches\n\n**1. Physics-Based Models**\n- Constant Velocity (CV)\n- Constant Acceleration (CA)\n- Kinematic bicycle model\n\n**2. Learning-Based Models**\n- Recurrent Neural Networks (RNN/LSTM)\n- Convolutional Social Pooling\n- Graph Neural Networks\n- Transformers (attention mechanisms)\n\n**3. Hybrid Approaches**\n- Combine physics models with learned intent prediction\n- Physics-informed neural networks\n\n**4. Probabilistic Models**\n- Gaussian Processes\n- Mixture Density Networks\n- Conditional Variational Autoencoders (CVAE)\n\n---\n\n## 2. Constant Velocity and Constant Acceleration Models\n\n### Constant Velocity Model\n\nAssumes the agent maintains current velocity:\n\n$$\n\\begin{align}\nx_{t+\\Delta t} &= x_t + v_x \\Delta t \\\\\ny_{t+\\Delta t} &= y_t + v_y \\Delta t \\\\\nv_{x,t+\\Delta t} &= v_x \\\\\nv_{y,t+\\Delta t} &= v_y\n\\end{align}\n$$\n\n**Pros**: Simple, computationally efficient  \n**Cons**: Doesn't model turning or speed changes\n\n### Constant Acceleration Model\n\nAssumes constant acceleration:\n\n$$\n\\begin{align}\nx_{t+\\Delta t} &= x_t + v_x \\Delta t + \\frac{1}{2} a_x \\Delta t^2 \\\\\ny_{t+\\Delta t} &= y_t + v_y \\Delta t + \\frac{1}{2} a_y \\Delta t^2 \\\\\nv_{x,t+\\Delta t} &= v_x + a_x \\Delta t \\\\\nv_{y,t+\\Delta t} &= v_y + a_y \\Delta t\n\\end{align}\n$$\n\n**Pros**: Models acceleration/braking  \n**Cons**: Still doesn't capture complex maneuvers\n\n### Uncertainty Propagation\n\nBoth models can be extended with uncertainty using Kalman filters:\n\n$$\n\\begin{align}\n\\hat{s}_{t+1} &= F \\hat{s}_t \\\\\n\\Sigma_{t+1} &= F \\Sigma_t F^T + Q\n\\end{align}\n$$\n\nWhere $\\Sigma$ is the covariance matrix representing prediction uncertainty."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Implementation: Multi-Model Trajectory Prediction\n\nclass TrajectoryPredictor:\n    \"\"\"Predicts future trajectories using multiple motion models\"\"\"\n    \n    def __init__(self, dt=0.1):\n        self.dt = dt\n    \n    def constant_velocity(self, state, horizon):\n        \"\"\"\n        Predict using constant velocity model\n        state: [x, y, vx, vy]\n        horizon: number of time steps\n        \"\"\"\n        x, y, vx, vy = state\n        trajectory = [state]\n        \n        for _ in range(horizon):\n            x = x + vx * self.dt\n            y = y + vy * self.dt\n            trajectory.append(np.array([x, y, vx, vy]))\n        \n        return np.array(trajectory)\n    \n    def constant_acceleration(self, state, accel, horizon):\n        \"\"\"\n        Predict using constant acceleration model\n        state: [x, y, vx, vy]\n        accel: [ax, ay]\n        horizon: number of time steps\n        \"\"\"\n        x, y, vx, vy = state\n        ax, ay = accel\n        trajectory = [state]\n        \n        for _ in range(horizon):\n            x = x + vx * self.dt + 0.5 * ax * self.dt**2\n            y = y + vy * self.dt + 0.5 * ay * self.dt**2\n            vx = vx + ax * self.dt\n            vy = vy + ay * self.dt\n            trajectory.append(np.array([x, y, vx, vy]))\n        \n        return np.array(trajectory)\n    \n    def constant_turn_rate(self, state, omega, horizon):\n        \"\"\"\n        Predict using constant turn rate model\n        state: [x, y, v, theta]\n        omega: turn rate (rad/s)\n        horizon: number of time steps\n        \"\"\"\n        x, y, v, theta = state\n        trajectory = [[x, y, v, theta]]\n        \n        for _ in range(horizon):\n            if abs(omega) < 1e-6:  # Straight line\n                x = x + v * np.cos(theta) * self.dt\n                y = y + v * np.sin(theta) * self.dt\n            else:  # Circular arc\n                x = x + (v / omega) * (np.sin(theta + omega * self.dt) - np.sin(theta))\n                y = y + (v / omega) * (-np.cos(theta + omega * self.dt) + np.cos(theta))\n            theta = theta + omega * self.dt\n            trajectory.append([x, y, v, theta])\n        \n        return np.array(trajectory)\n\n\n# Demonstration: Compare prediction models\ndef demonstrate_prediction_models():\n    predictor = TrajectoryPredictor(dt=0.1)\n    horizon = 50\n    \n    # Initial state for CV/CA models: [x, y, vx, vy]\n    state_linear = np.array([0.0, 0.0, 10.0, 5.0])  # Moving diagonally\n    \n    # Initial state for CTR model: [x, y, v, theta]\n    speed = np.sqrt(10**2 + 5**2)\n    heading = np.arctan2(5, 10)\n    state_turn = np.array([0.0, 0.0, speed, heading])\n    \n    # Generate predictions\n    cv_pred = predictor.constant_velocity(state_linear, horizon)\n    ca_pred_accel = predictor.constant_acceleration(state_linear, accel=[1.0, 0.5], horizon=horizon)\n    ca_pred_decel = predictor.constant_acceleration(state_linear, accel=[-1.0, -0.5], horizon=horizon)\n    \n    # Turn predictions\n    ctr_left = predictor.constant_turn_rate(state_turn, omega=0.2, horizon=horizon)\n    ctr_right = predictor.constant_turn_rate(state_turn, omega=-0.2, horizon=horizon)\n    \n    # Visualization\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Linear motion models\n    ax1 = axes[0]\n    ax1.plot(cv_pred[:, 0], cv_pred[:, 1], 'b-', linewidth=2, label='Constant Velocity', marker='o', markevery=10)\n    ax1.plot(ca_pred_accel[:, 0], ca_pred_accel[:, 1], 'g-', linewidth=2, label='Constant Accel (+)', marker='s', markevery=10)\n    ax1.plot(ca_pred_decel[:, 0], ca_pred_decel[:, 1], 'r-', linewidth=2, label='Constant Accel (-)', marker='^', markevery=10)\n    ax1.plot(0, 0, 'ko', markersize=10, label='Start')\n    ax1.set_xlabel('X Position (m)', fontsize=12)\n    ax1.set_ylabel('Y Position (m)', fontsize=12)\n    ax1.set_title('Linear Motion Models Comparison', fontsize=14, fontweight='bold')\n    ax1.legend(fontsize=11)\n    ax1.grid(True, alpha=0.3)\n    ax1.axis('equal')\n    \n    # Turn rate model\n    ax2 = axes[1]\n    ax2.plot(ctr_left[:, 0], ctr_left[:, 1], 'b-', linewidth=2, label='Left Turn (ω=+0.2)', marker='o', markevery=10)\n    ax2.plot(ctr_right[:, 0], ctr_right[:, 1], 'r-', linewidth=2, label='Right Turn (ω=-0.2)', marker='s', markevery=10)\n    ax2.plot(0, 0, 'ko', markersize=10, label='Start')\n    \n    # Add velocity vectors at start\n    ax2.arrow(0, 0, 5*np.cos(heading), 5*np.sin(heading), \n             head_width=2, head_length=1, fc='green', ec='green', alpha=0.6, label='Initial Velocity')\n    \n    ax2.set_xlabel('X Position (m)', fontsize=12)\n    ax2.set_ylabel('Y Position (m)', fontsize=12)\n    ax2.set_title('Constant Turn Rate Model', fontsize=14, fontweight='bold')\n    ax2.legend(fontsize=11)\n    ax2.grid(True, alpha=0.3)\n    ax2.axis('equal')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"Prediction Models Comparison:\")\n    print(f\"  Time horizon: {horizon * predictor.dt:.1f} seconds\")\n    print(f\"  CV final position: ({cv_pred[-1, 0]:.1f}, {cv_pred[-1, 1]:.1f}) m\")\n    print(f\"  CA(+) final position: ({ca_pred_accel[-1, 0]:.1f}, {ca_pred_accel[-1, 1]:.1f}) m\")\n    print(f\"  CA(-) final position: ({ca_pred_decel[-1, 0]:.1f}, {ca_pred_decel[-1, 1]:.1f}) m\")\n\ndemonstrate_prediction_models()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 3. Game Theory for Interaction Modeling\n\nGame theory provides a mathematical framework for modeling strategic interactions between multiple agents.\n\n### Why Game Theory for Autonomous Driving?\n\n- **Strategic Behavior**: Drivers make decisions considering what others will do\n- **Multi-Agent**: Multiple vehicles interact simultaneously\n- **Incomplete Information**: Drivers don't know others' exact intentions\n- **Sequential Decisions**: Actions unfold over time\n\n### Key Game Theory Concepts\n\n**1. Players**: Autonomous vehicle (ego) and other agents\n\n**2. Actions**: Discrete choices (yield, go, merge) or continuous (acceleration, steering)\n\n**3. Payoffs**: Utility functions representing goals:\n- Safety: Avoid collisions\n- Efficiency: Minimize travel time\n- Comfort: Smooth acceleration\n\n**4. Nash Equilibrium**: A state where no player can improve their payoff by unilaterally changing strategy\n\n### Types of Games in Driving\n\n**1. Simultaneous Games** (Matrix Form)\n- Both players act at the same time\n- Example: Intersection crossing\n\n**2. Sequential Games** (Game Trees)\n- Players take turns\n- Example: Merge negotiation\n\n**3. Repeated Games**\n- Same interaction happens multiple times\n- Example: Highway lane changes\n\n**4. Differential Games**\n- Continuous-time, continuous-action\n- Example: Trajectory planning with interaction\n\n---\n\n## 3.1 Intersection Game Example\n\nConsider a 4-way intersection with two vehicles approaching:\n\n**Players**: Vehicle A (ego), Vehicle B (other)\n\n**Actions**: \n- Go (G): Proceed through intersection\n- Yield (Y): Wait for other vehicle\n\n**Payoffs** (Vehicle A, Vehicle B):\n\n$$\n\\begin{array}{c|c|c}\n & \\text{B: Go} & \\text{B: Yield} \\\\\n\\hline\n\\text{A: Go} & (-100, -100) & (10, -5) \\\\\n\\text{A: Yield} & (-5, 10) & (-2, -2)\n\\end{array}\n$$\n\n- Both Go: Collision! Very negative payoff\n- A Go, B Yield: A gets through quickly (+10), B waits (-5)\n- A Yield, B Go: B gets through quickly (+10), A waits (-5)\n- Both Yield: Both waste time (-2 each)\n\n**Nash Equilibria**: (Go, Yield) and (Yield, Go)\n\nThe challenge: How to coordinate which equilibrium to select?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Implementation: Game Theoretic Intersection Model\n\nclass IntersectionGame:\n    \"\"\"Models intersection crossing as a 2-player game\"\"\"\n    \n    def __init__(self):\n        # Payoff matrix: [player_A_action, player_B_action] -> (payoff_A, payoff_B)\n        # Actions: 0 = Yield, 1 = Go\n        self.payoff_matrix = np.array([\n            [(-2, -2), (-5, 10)],   # A: Yield, B: Yield/Go\n            [(10, -5), (-100, -100)] # A: Go, B: Yield/Go\n        ])\n    \n    def get_payoff(self, action_a, action_b):\n        \"\"\"Get payoffs for both players given their actions\"\"\"\n        return self.payoff_matrix[action_a, action_b]\n    \n    def find_nash_equilibria(self):\n        \"\"\"Find all pure strategy Nash equilibria\"\"\"\n        equilibria = []\n        \n        for action_a in range(2):\n            for action_b in range(2):\n                is_equilibrium = True\n                current_payoff = self.get_payoff(action_a, action_b)\n                \n                # Check if A can improve by deviating\n                for alt_action_a in range(2):\n                    if alt_action_a != action_a:\n                        alt_payoff = self.get_payoff(alt_action_a, action_b)\n                        if alt_payoff[0] > current_payoff[0]:\n                            is_equilibrium = False\n                            break\n                \n                # Check if B can improve by deviating\n                for alt_action_b in range(2):\n                    if alt_action_b != action_b:\n                        alt_payoff = self.get_payoff(action_a, alt_action_b)\n                        if alt_payoff[1] > current_payoff[1]:\n                            is_equilibrium = False\n                            break\n                \n                if is_equilibrium:\n                    equilibria.append((action_a, action_b, current_payoff))\n        \n        return equilibria\n    \n    def best_response(self, other_action, player='A'):\n        \"\"\"Find best response to other player's action\"\"\"\n        best_action = None\n        best_payoff = -np.inf\n        \n        if player == 'A':\n            for action in range(2):\n                payoff = self.get_payoff(action, other_action)[0]\n                if payoff > best_payoff:\n                    best_payoff = payoff\n                    best_action = action\n        else:  # player == 'B'\n            for action in range(2):\n                payoff = self.get_payoff(other_action, action)[1]\n                if payoff > best_payoff:\n                    best_payoff = payoff\n                    best_action = action\n        \n        return best_action, best_payoff\n\n\n# Demonstration: Analyze intersection game\ndef demonstrate_intersection_game():\n    game = IntersectionGame()\n    \n    print(\"=== Intersection Game Analysis ===\\n\")\n    \n    # Display payoff matrix\n    print(\"Payoff Matrix (Player A, Player B):\")\n    print(\"           B: Yield    B: Go\")\n    print(f\"A: Yield   {game.payoff_matrix[0,0]}   {game.payoff_matrix[0,1]}\")\n    print(f\"A: Go      {game.payoff_matrix[1,0]}   {game.payoff_matrix[1,1]}\")\n    print()\n    \n    # Find Nash equilibria\n    equilibria = game.find_nash_equilibria()\n    print(f\"Nash Equilibria: {len(equilibria)} found\")\n    action_names = ['Yield', 'Go']\n    for eq in equilibria:\n        action_a, action_b, payoffs = eq\n        print(f\"  ({action_names[action_a]}, {action_names[action_b]}): Payoffs = {payoffs}\")\n    print()\n    \n    # Best responses\n    print(\"Best Response Analysis:\")\n    for action_b in range(2):\n        br_a, payoff_a = game.best_response(action_b, player='A')\n        print(f\"  If B plays {action_names[action_b]}, A's best response: {action_names[br_a]} (payoff: {payoff_a})\")\n    \n    for action_a in range(2):\n        br_b, payoff_b = game.best_response(action_a, player='B')\n        print(f\"  If A plays {action_names[action_a]}, B's best response: {action_names[br_b]} (payoff: {payoff_b})\")\n    \n    # Visualize game tree\n    fig, ax = plt.subplots(figsize=(12, 8))\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    ax.axis('off')\n    \n    # Title\n    ax.text(5, 9.5, 'Intersection Game Tree', fontsize=16, fontweight='bold', ha='center')\n    \n    # Root\n    ax.plot(5, 8, 'ko', markersize=15)\n    ax.text(5, 8.3, 'Start', fontsize=11, ha='center', fontweight='bold')\n    \n    # Player A's decisions\n    ax.plot([5, 2.5], [8, 6], 'k-', linewidth=2)\n    ax.plot([5, 7.5], [8, 6], 'k-', linewidth=2)\n    ax.text(3.5, 7.2, 'A: Yield', fontsize=10, ha='center', style='italic')\n    ax.text(6.5, 7.2, 'A: Go', fontsize=10, ha='center', style='italic')\n    \n    ax.plot(2.5, 6, 'bo', markersize=12)\n    ax.plot(7.5, 6, 'bo', markersize=12)\n    \n    # Player B's responses to A: Yield\n    ax.plot([2.5, 1.5], [6, 4], 'b--', linewidth=1.5)\n    ax.plot([2.5, 3.5], [6, 4], 'b--', linewidth=1.5)\n    ax.text(1.8, 5.1, 'B: Yield', fontsize=9, ha='center', color='blue')\n    ax.text(3.2, 5.1, 'B: Go', fontsize=9, ha='center', color='blue')\n    \n    # Player B's responses to A: Go\n    ax.plot([7.5, 6.5], [6, 4], 'b--', linewidth=1.5)\n    ax.plot([7.5, 8.5], [6, 4], 'b--', linewidth=1.5)\n    ax.text(6.8, 5.1, 'B: Yield', fontsize=9, ha='center', color='blue')\n    ax.text(8.2, 5.1, 'B: Go', fontsize=9, ha='center', color='blue')\n    \n    # Outcomes\n    outcomes = [\n        (1.5, 4, '(-2, -2)', 'Both wait', 'yellow'),\n        (3.5, 4, '(-5, 10)', 'B goes', 'lightgreen'),\n        (6.5, 4, '(10, -5)', 'A goes', 'lightgreen'),\n        (8.5, 4, '(-100, -100)', 'CRASH!', 'red')\n    ]\n    \n    for x, y, payoff, desc, color in outcomes:\n        rect = Rectangle((x-0.6, y-0.3), 1.2, 0.6, facecolor=color, edgecolor='black', linewidth=2, alpha=0.7)\n        ax.add_patch(rect)\n        ax.text(x, y+0.05, payoff, fontsize=10, ha='center', fontweight='bold')\n        ax.text(x, y-0.7, desc, fontsize=9, ha='center', style='italic')\n    \n    # Add Nash equilibria markers\n    ax.text(3.5, 3.2, '★ Nash Eq.', fontsize=9, ha='center', color='green', fontweight='bold')\n    ax.text(6.5, 3.2, '★ Nash Eq.', fontsize=9, ha='center', color='green', fontweight='bold')\n    \n    plt.tight_layout()\n    plt.show()\n\ndemonstrate_intersection_game()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 4. Reinforcement Learning for Interactive Driving\n\nReinforcement Learning (RL) enables agents to learn optimal behavior through trial-and-error interaction with the environment.\n\n### Why RL for Autonomous Driving Interactions?\n\n- **Complex Scenarios**: Hard to manually specify rules for all situations\n- **Learning from Experience**: Improve over time through data\n- **Multi-Agent**: Can model interactions between multiple learners\n- **Delayed Rewards**: Actions have long-term consequences\n\n### RL Formulation\n\n**Markov Decision Process (MDP)**:\n- **State** $s_t$: Vehicle positions, velocities, road geometry\n- **Action** $a_t$: Acceleration, steering angle\n- **Reward** $r_t$: Scalar feedback (positive for good, negative for bad)\n- **Transition** $p(s_{t+1}|s_t, a_t)$: Dynamics of environment\n- **Policy** $\\pi(a|s)$: Strategy mapping states to actions\n\n**Objective**: Learn policy $\\pi^*$ that maximizes expected cumulative reward:\n$$\nJ(\\pi) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\right]\n$$\n\nWhere $\\gamma \\in [0,1]$ is the discount factor.\n\n### Q-Learning Algorithm\n\n**Q-function** represents expected return starting from state $s$, taking action $a$:\n$$\nQ^*(s, a) = \\max_{\\pi} \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0=s, a_0=a, \\pi \\right]\n$$\n\n**Q-Learning Update Rule**:\n$$\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right]\n$$\n\nWhere:\n- $\\alpha$: Learning rate\n- $r_t + \\gamma \\max_{a'} Q(s_{t+1}, a')$: TD target\n- $r_t + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)$: TD error\n\n### Multi-Agent RL for Intersection\n\nIn multi-agent settings, each vehicle learns simultaneously:\n- **Independent Q-Learning**: Each agent learns ignoring others\n- **Joint Action Learning**: Learn Q-values for joint actions\n- **Actor-Critic Methods**: Learn policy and value function\n- **Multi-Agent Deep RL**: Use neural networks for complex states\n\n### Reward Design for Driving\n\nCritical for learning safe and efficient behavior:\n\n**Safety Rewards**:\n- Large negative reward for collision: $r_{collision} = -1000$\n- Penalty for getting too close: $r_{proximity} = -50 \\cdot \\max(0, d_{safe} - d)$\n\n**Efficiency Rewards**:\n- Reward for progress toward goal: $r_{progress} = v \\cdot \\cos(\\theta_{error})$\n- Penalty for time: $r_{time} = -1$ per step\n\n**Comfort Rewards**:\n- Penalty for high acceleration: $r_{comfort} = -0.1 \\cdot |a|$\n\n**Total Reward**:\n$$\nr_t = r_{progress} + r_{proximity} + r_{comfort} + r_{collision} + r_{time}\n$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Implementation: Q-Learning for Simple Merging Scenario\n\nclass MergeEnvironment:\n    \"\"\"Simple 1D highway merge environment\"\"\"\n    \n    def __init__(self):\n        self.length = 100  # Highway length\n        self.goal = 90     # Goal position\n        self.safe_distance = 5.0\n        self.reset()\n    \n    def reset(self):\n        \"\"\"Reset environment to initial state\"\"\"\n        self.ego_pos = 10.0\n        self.ego_vel = 5.0\n        self.other_pos = 40.0\n        self.other_vel = 6.0\n        self.time_step = 0\n        self.max_steps = 100\n        return self._get_state()\n    \n    def _get_state(self):\n        \"\"\"Discretize continuous state\"\"\"\n        # Relative position bins\n        rel_pos = self.other_pos - self.ego_pos\n        rel_pos_bin = min(max(int(rel_pos / 10), 0), 9)  # 10 bins\n        \n        # Relative velocity bins\n        rel_vel = self.other_vel - self.ego_vel\n        rel_vel_bin = min(max(int((rel_vel + 5) / 2), 0), 4)  # 5 bins\n        \n        # Distance to goal bins\n        dist_to_goal = self.goal - self.ego_pos\n        goal_bin = min(max(int(dist_to_goal / 20), 0), 4)  # 5 bins\n        \n        return (rel_pos_bin, rel_vel_bin, goal_bin)\n    \n    def step(self, action):\n        \"\"\"\n        Execute action and return (next_state, reward, done)\n        Actions: 0 = decelerate, 1 = maintain, 2 = accelerate\n        \"\"\"\n        # Map action to acceleration\n        accel_map = {0: -2.0, 1: 0.0, 2: 2.0}\n        accel = accel_map[action]\n        \n        # Update ego vehicle\n        self.ego_vel = np.clip(self.ego_vel + accel * 0.1, 0, 15)\n        self.ego_pos += self.ego_vel * 0.1\n        \n        # Update other vehicle (constant velocity)\n        self.other_pos += self.other_vel * 0.1\n        \n        self.time_step += 1\n        \n        # Calculate reward\n        reward = self._calculate_reward()\n        \n        # Check termination conditions\n        done = False\n        if self.ego_pos >= self.goal:\n            reward += 100  # Bonus for reaching goal\n            done = True\n        elif self.time_step >= self.max_steps:\n            done = True\n        elif abs(self.ego_pos - self.other_pos) < self.safe_distance:\n            reward = -1000  # Collision penalty\n            done = True\n        \n        next_state = self._get_state()\n        \n        return next_state, reward, done\n    \n    def _calculate_reward(self):\n        \"\"\"Calculate reward for current state\"\"\"\n        reward = 0.0\n        \n        # Progress reward\n        reward += self.ego_vel * 0.1\n        \n        # Time penalty\n        reward -= 1.0\n        \n        # Proximity penalty\n        distance = abs(self.ego_pos - self.other_pos)\n        if distance < self.safe_distance * 2:\n            reward -= 10 * (self.safe_distance * 2 - distance)\n        \n        # Comfort penalty (high acceleration)\n        # (already implicitly handled by velocity changes)\n        \n        return reward\n\n\nclass QLearningAgent:\n    \"\"\"Q-Learning agent for discrete state-action spaces\"\"\"\n    \n    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.95, epsilon=0.1):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.alpha = alpha        # Learning rate\n        self.gamma = gamma        # Discount factor\n        self.epsilon = epsilon    # Exploration rate\n        \n        # Initialize Q-table\n        self.Q = np.zeros(n_states + (n_actions,))\n    \n    def get_action(self, state, training=True):\n        \"\"\"Epsilon-greedy action selection\"\"\"\n        if training and np.random.random() < self.epsilon:\n            return np.random.randint(self.n_actions)  # Explore\n        else:\n            return np.argmax(self.Q[state])  # Exploit\n    \n    def update(self, state, action, reward, next_state, done):\n        \"\"\"Q-learning update\"\"\"\n        if done:\n            td_target = reward\n        else:\n            td_target = reward + self.gamma * np.max(self.Q[next_state])\n        \n        td_error = td_target - self.Q[state + (action,)]\n        self.Q[state + (action,)] += self.alpha * td_error\n\n\n# Training loop\ndef train_q_learning(episodes=500):\n    env = MergeEnvironment()\n    agent = QLearningAgent(\n        n_states=(10, 5, 5),  # State space dimensions\n        n_actions=3,           # Action space size\n        alpha=0.1,\n        gamma=0.95,\n        epsilon=0.2\n    )\n    \n    episode_rewards = []\n    episode_lengths = []\n    \n    print(\"Training Q-Learning agent...\")\n    \n    for episode in range(episodes):\n        state = env.reset()\n        total_reward = 0\n        done = False\n        steps = 0\n        \n        while not done:\n            action = agent.get_action(state, training=True)\n            next_state, reward, done = env.step(action)\n            agent.update(state, action, reward, next_state, done)\n            \n            state = next_state\n            total_reward += reward\n            steps += 1\n        \n        episode_rewards.append(total_reward)\n        episode_lengths.append(steps)\n        \n        if (episode + 1) % 100 == 0:\n            avg_reward = np.mean(episode_rewards[-100:])\n            avg_length = np.mean(episode_lengths[-100:])\n            print(f\"  Episode {episode+1}/{episodes}: Avg Reward = {avg_reward:.2f}, Avg Length = {avg_length:.1f}\")\n    \n    # Visualize learning progress\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Plot rewards\n    ax1 = axes[0]\n    window = 50\n    smoothed_rewards = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n    ax1.plot(smoothed_rewards, linewidth=2, color='blue')\n    ax1.set_xlabel('Episode', fontsize=12)\n    ax1.set_ylabel('Total Reward (smoothed)', fontsize=12)\n    ax1.set_title('Learning Curve: Cumulative Reward', fontsize=14, fontweight='bold')\n    ax1.grid(True, alpha=0.3)\n    \n    # Plot episode lengths\n    ax2 = axes[1]\n    smoothed_lengths = np.convolve(episode_lengths, np.ones(window)/window, mode='valid')\n    ax2.plot(smoothed_lengths, linewidth=2, color='green')\n    ax2.set_xlabel('Episode', fontsize=12)\n    ax2.set_ylabel('Episode Length (smoothed)', fontsize=12)\n    ax2.set_title('Learning Curve: Episode Duration', fontsize=14, fontweight='bold')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return agent, env\n\n# Train and visualize\ntrained_agent, env = train_q_learning(episodes=500)\n\nprint(\"\\nFinal Q-Learning Performance:\")\nprint(f\"  Q-table shape: {trained_agent.Q.shape}\")\nprint(f\"  Non-zero Q-values: {np.count_nonzero(trained_agent.Q)}/{trained_agent.Q.size}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n### Exercise 1: Multi-Modal Trajectory Prediction\n\n**Objective:** Implement a probabilistic prediction system that outputs multiple trajectory hypotheses.\n\n**Task:** Create a predictor that generates multiple plausible future trajectories for a vehicle at an intersection.\n\n**Instructions:**\n- Use the constant velocity and constant turn rate models to generate multiple hypotheses\n- Assign probabilities to each hypothesis based on context (e.g., lane geometry, turn signals)\n- Implement a maneuver classifier that predicts: {go straight, turn left, turn right}\n- Visualize the predicted trajectory distribution as a probability cone\n\n**Bonus:** Add uncertainty quantification using covariance ellipses at future time steps.\n\n### Exercise 2: Mixed Strategy Nash Equilibrium\n\n**Objective:** Find and analyze mixed strategy equilibria for the intersection game.\n\n**Task:** Extend the `IntersectionGame` class to compute mixed strategy Nash equilibria.\n\n**Background:** In a mixed strategy, players randomize their actions. For the intersection game, each player chooses a probability distribution over {Yield, Go}.\n\n**Instructions:**\n- Implement a function to find mixed strategy equilibria\n- For each player, the expected payoff from both pure strategies must be equal when mixing optimally\n- Compute the equilibrium probabilities $(p_A, p_B)$ where:\n  - $p_A$ = probability that A plays \"Go\"\n  - $p_B$ = probability that B plays \"Go\"\n- Visualize the payoff landscape as a heatmap\n- Compare pure vs. mixed equilibria in terms of expected payoffs\n\n### Exercise 3: Deep Q-Learning for Lane Changing\n\n**Objective:** Implement Deep Q-Network (DQN) for a more complex highway lane changing scenario.\n\n**Task:** Extend the Q-learning implementation to use a neural network function approximator instead of a Q-table.\n\n**Instructions:**\n- Create a 2D highway environment with multiple lanes\n- Use a neural network to approximate Q(s,a) for continuous state spaces\n- Implement experience replay buffer\n- Add target network for stable learning\n- Train the agent to perform safe lane changes\n- Compare performance with tabular Q-learning\n\n**State representation:** [ego_position, ego_velocity, lane_id, nearby_vehicles_positions, nearby_vehicles_velocities]\n\n**Actions:** {lane_left, maintain_lane, lane_right, accelerate, decelerate}"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Exercise Solutions\n\n# Exercise 1: Multi-Modal Trajectory Prediction\n# TODO: Implement multi-modal trajectory prediction\n#\n# Suggested approach:\n# 1. Define maneuver classes (straight, left turn, right turn)\n# 2. For each maneuver, generate trajectory using appropriate motion model\n# 3. Assign prior probabilities based on context\n# 4. Visualize all hypotheses with transparency proportional to probability\n#\n# Example structure:\n# class MultiModalPredictor:\n#     def __init__(self, predictor):\n#         self.predictor = predictor\n#         self.maneuvers = ['straight', 'left', 'right']\n#     \n#     def predict_multimodal(self, state, context):\n#         hypotheses = []\n#         probabilities = []\n#         \n#         # Generate hypothesis for going straight\n#         traj_straight = self.predictor.constant_velocity(state, horizon=50)\n#         hypotheses.append(('straight', traj_straight))\n#         \n#         # Generate hypothesis for left turn\n#         traj_left = self.predictor.constant_turn_rate(\n#             convert_to_polar(state), omega=0.3, horizon=50\n#         )\n#         hypotheses.append(('left', traj_left))\n#         \n#         # Generate hypothesis for right turn\n#         traj_right = self.predictor.constant_turn_rate(\n#             convert_to_polar(state), omega=-0.3, horizon=50\n#         )\n#         hypotheses.append(('right', traj_right))\n#         \n#         # Compute probabilities based on context\n#         probabilities = self.compute_maneuver_probabilities(state, context)\n#         \n#         return hypotheses, probabilities\n#     \n#     def compute_maneuver_probabilities(self, state, context):\n#         # Use lane geometry, turn signals, traffic rules\n#         # Simple example: uniform prior if no context\n#         return [0.6, 0.2, 0.2]  # [straight, left, right]\n\n\n# Exercise 2: Mixed Strategy Nash Equilibrium\n# TODO: Implement mixed strategy equilibrium finder\n#\n# Suggested approach:\n# 1. Extend IntersectionGame class\n# 2. Set up indifference equations for each player\n# 3. Solve for mixing probabilities\n# 4. Verify equilibrium conditions\n#\n# Example structure:\n# class IntersectionGameExtended(IntersectionGame):\n#     def find_mixed_strategy_equilibrium(self):\n#         \"\"\"\n#         For 2x2 game, player A is indifferent when:\n#         p_B * U_A(Go, Go) + (1-p_B) * U_A(Go, Yield) = \n#         p_B * U_A(Yield, Go) + (1-p_B) * U_A(Yield, Yield)\n#         \n#         Solve for p_B, then similarly for p_A\n#         \"\"\"\n#         # Player A's payoffs\n#         u_a_go_go = self.payoff_matrix[1, 1][0]\n#         u_a_go_yield = self.payoff_matrix[1, 0][0]\n#         u_a_yield_go = self.payoff_matrix[0, 1][0]\n#         u_a_yield_yield = self.payoff_matrix[0, 0][0]\n#         \n#         # Solve for p_B (probability B plays Go)\n#         # p_B * u_a_go_go + (1-p_B) * u_a_go_yield = \n#         # p_B * u_a_yield_go + (1-p_B) * u_a_yield_yield\n#         numerator = u_a_yield_yield - u_a_go_yield\n#         denominator = (u_a_go_go - u_a_yield_go) - (u_a_go_yield - u_a_yield_yield)\n#         p_B = numerator / denominator if denominator != 0 else 0.5\n#         \n#         # Similarly solve for p_A\n#         u_b_go_go = self.payoff_matrix[1, 1][1]\n#         u_b_yield_go = self.payoff_matrix[0, 1][1]\n#         u_b_go_yield = self.payoff_matrix[1, 0][1]\n#         u_b_yield_yield = self.payoff_matrix[0, 0][1]\n#         \n#         numerator = u_b_yield_yield - u_b_yield_go\n#         denominator = (u_b_go_go - u_b_go_yield) - (u_b_yield_go - u_b_yield_yield)\n#         p_A = numerator / denominator if denominator != 0 else 0.5\n#         \n#         return p_A, p_B\n#     \n#     def expected_payoff_mixed(self, p_A, p_B):\n#         \"\"\"Compute expected payoffs under mixed strategy\"\"\"\n#         # E[U_A] = sum over all action pairs: p(a_A) * p(a_B) * U_A(a_A, a_B)\n#         exp_payoff_A = 0\n#         exp_payoff_B = 0\n#         \n#         for a_A in range(2):\n#             for a_B in range(2):\n#                 prob_joint = (p_A if a_A == 1 else 1-p_A) * (p_B if a_B == 1 else 1-p_B)\n#                 payoffs = self.get_payoff(a_A, a_B)\n#                 exp_payoff_A += prob_joint * payoffs[0]\n#                 exp_payoff_B += prob_joint * payoffs[1]\n#         \n#         return exp_payoff_A, exp_payoff_B\n\n\n# Exercise 3: Deep Q-Learning for Lane Changing\n# TODO: Implement DQN with neural network\n#\n# Suggested approach:\n# 1. Define neural network architecture (e.g., 2 hidden layers)\n# 2. Implement experience replay buffer\n# 3. Create target network (copy of Q-network, updated periodically)\n# 4. Implement DQN training loop\n# 5. Compare with tabular Q-learning\n#\n# Example structure (requires PyTorch or TensorFlow):\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from collections import deque\n# import random\n#\n# class QNetwork(nn.Module):\n#     def __init__(self, state_dim, action_dim, hidden_dim=64):\n#         super(QNetwork, self).__init__()\n#         self.fc1 = nn.Linear(state_dim, hidden_dim)\n#         self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n#         self.fc3 = nn.Linear(hidden_dim, action_dim)\n#     \n#     def forward(self, x):\n#         x = torch.relu(self.fc1(x))\n#         x = torch.relu(self.fc2(x))\n#         return self.fc3(x)\n#\n# class ReplayBuffer:\n#     def __init__(self, capacity=10000):\n#         self.buffer = deque(maxlen=capacity)\n#     \n#     def push(self, state, action, reward, next_state, done):\n#         self.buffer.append((state, action, reward, next_state, done))\n#     \n#     def sample(self, batch_size):\n#         return random.sample(self.buffer, batch_size)\n#     \n#     def __len__(self):\n#         return len(self.buffer)\n#\n# class DQNAgent:\n#     def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):\n#         self.q_network = QNetwork(state_dim, action_dim)\n#         self.target_network = QNetwork(state_dim, action_dim)\n#         self.target_network.load_state_dict(self.q_network.state_dict())\n#         \n#         self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n#         self.gamma = gamma\n#         self.replay_buffer = ReplayBuffer()\n#         self.batch_size = 64\n#         self.epsilon = 1.0\n#         self.epsilon_decay = 0.995\n#         self.epsilon_min = 0.01\n#     \n#     def get_action(self, state, training=True):\n#         if training and random.random() < self.epsilon:\n#             return random.randint(0, self.action_dim - 1)\n#         else:\n#             with torch.no_grad():\n#                 state_tensor = torch.FloatTensor(state).unsqueeze(0)\n#                 q_values = self.q_network(state_tensor)\n#                 return q_values.argmax().item()\n#     \n#     def update(self):\n#         if len(self.replay_buffer) < self.batch_size:\n#             return\n#         \n#         # Sample batch\n#         batch = self.replay_buffer.sample(self.batch_size)\n#         states, actions, rewards, next_states, dones = zip(*batch)\n#         \n#         # Convert to tensors\n#         states = torch.FloatTensor(states)\n#         actions = torch.LongTensor(actions)\n#         rewards = torch.FloatTensor(rewards)\n#         next_states = torch.FloatTensor(next_states)\n#         dones = torch.FloatTensor(dones)\n#         \n#         # Compute Q(s,a)\n#         q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n#         \n#         # Compute target: r + gamma * max_a' Q_target(s', a')\n#         with torch.no_grad():\n#             next_q_values = self.target_network(next_states).max(1)[0]\n#             targets = rewards + self.gamma * next_q_values * (1 - dones)\n#         \n#         # Loss and backprop\n#         loss = nn.MSELoss()(q_values.squeeze(), targets)\n#         self.optimizer.zero_grad()\n#         loss.backward()\n#         self.optimizer.step()\n#         \n#         # Decay epsilon\n#         self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n#     \n#     def update_target_network(self):\n#         self.target_network.load_state_dict(self.q_network.state_dict())",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## References\n\n### Books\n\n1. **Shoham, Y., & Leyton-Brown, K.** (2008). *Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations*. Cambridge University Press.\n   - Comprehensive coverage of game theory for multi-agent systems\n   - Chapter 3: Strategic games and Nash equilibria\n\n2. **Sutton, R. S., & Barto, A. G.** (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.\n   - Definitive textbook on RL fundamentals\n   - Free online: http://incompleteideas.net/book/the-book-2nd.html\n\n3. **Busoniu, L., Babuska, R., & De Schutter, B.** (2008). *A Comprehensive Survey of Multiagent Reinforcement Learning*. IEEE Transactions on Systems, Man, and Cybernetics.\n   - Survey of multi-agent RL techniques\n\n### Papers - Prediction\n\n4. **Lefèvre, S., Vasquez, D., & Laugier, C.** (2014). \"A survey on motion prediction and risk assessment for intelligent vehicles.\" *ROBOMECH Journal*, 1(1), 1-14.\n   - Excellent overview of prediction methods\n   - Comparison of physics-based vs. learning-based approaches\n\n5. **Deo, N., & Trivedi, M. M.** (2018). \"Convolutional social pooling for vehicle trajectory prediction.\" *IEEE Conference on Computer Vision and Pattern Recognition Workshops*.\n   - CNN-based approach for trajectory prediction\n   - Captures spatial interactions between agents\n\n6. **Alahi, A., et al.** (2016). \"Social LSTM: Human trajectory prediction in crowded spaces.\" *IEEE Conference on Computer Vision and Pattern Recognition*.\n   - LSTM with social pooling layer\n   - Foundational paper for learning-based prediction\n\n7. **Chai, Y., et al.** (2019). \"MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction.\" *Conference on Robot Learning*.\n   - Multi-modal prediction with anchor trajectories\n   - Used in Waymo's production system\n\n8. **Cui, H., et al.** (2019). \"Multimodal trajectory predictions for autonomous driving using deep convolutional networks.\" *IEEE International Conference on Robotics and Automation*.\n   - Deep learning for multi-modal prediction\n   - Includes uncertainty quantification\n\n### Papers - Game Theory\n\n9. **Schwarting, W., Alonso-Mora, J., & Rus, D.** (2018). \"Planning and decision-making for autonomous vehicles.\" *Annual Review of Control, Robotics, and Autonomous Systems*, 1, 187-210.\n   - Section on game-theoretic planning\n   - Overview of interaction-aware decision making\n\n10. **Fisac, J. F., et al.** (2019). \"Hierarchical game-theoretic planning for autonomous vehicles.\" *IEEE International Conference on Robotics and Automation*.\n    - Combines game theory with hierarchical planning\n    - Addresses computational complexity\n\n11. **Sadigh, D., et al.** (2016). \"Planning for autonomous cars that leverage effects on human actions.\" *Robotics: Science and Systems*.\n    - How AV actions influence human behavior\n    - Game-theoretic framework for interaction\n\n12. **Tian, R., et al.** (2020). \"Game-theoretic modeling of traffic in unsignalized intersection network for autonomous vehicle control verification and validation.\" *IEEE Transactions on Intelligent Transportation Systems*.\n    - Application to intersections\n    - Validation methodology\n\n### Papers - Reinforcement Learning\n\n13. **Sallab, A. E., et al.** (2017). \"Deep reinforcement learning framework for autonomous driving.\" *Electronic Imaging*, 2017(19), 70-76.\n    - DQN for lane keeping and obstacle avoidance\n    - Comparison with traditional methods\n\n14. **Isele, D., Rahimi, R., Cosgun, A., Subramanian, K., & Fujimura, K.** (2018). \"Navigating occluded intersections with autonomous vehicles using deep reinforcement learning.\" *IEEE International Conference on Robotics and Automation*.\n    - RL for intersection navigation\n    - Handles partial observability\n\n15. **Leurent, E., & Mercat, J.** (2019). \"Social attention for autonomous decision-making in dense traffic.\" *arXiv preprint arXiv:1911.12250*.\n    - Attention mechanisms for multi-agent RL\n    - Open-source implementation: highway-env\n\n16. **Mirchevska, B., et al.** (2018). \"High-level decision making for safe and reasonable autonomous lane changing using reinforcement learning.\" *21st International Conference on Intelligent Transportation Systems*.\n    - Safe RL for lane changing\n    - Incorporates safety constraints\n\n17. **Tram, T., et al.** (2018). \"Learning to communicate and correct pose errors.\" *Conference on Robot Learning*.\n    - Multi-agent communication for coordination\n    - Emergent communication protocols\n\n### Papers - Integrated Approaches\n\n18. **Hubmann, C., et al.** (2018). \"Decision making for autonomous driving considering interaction and uncertain prediction of surrounding vehicles.\" *IEEE Intelligent Vehicles Symposium*.\n    - Combines prediction and planning\n    - POMDP formulation with interaction\n\n19. **Sun, L., et al.** (2020). \"Courteous autonomous cars.\" *IEEE/RSJ International Conference on Intelligent Robots and Systems*.\n    - Social norms in autonomous driving\n    - Politeness and assertiveness trade-offs\n\n20. **Bouton, M., et al.** (2020). \"Reinforcement learning with probabilistic guarantees for autonomous driving.\" *Conference on Robot Learning*.\n    - Safe RL with formal guarantees\n    - Shielding approach\n\n### Datasets\n\n21. **NGSIM** - Next Generation Simulation\n    - https://ops.fhwa.dot.gov/trafficanalysistools/ngsim.htm\n    - Highway trajectory data\n    - Used for learning driver behavior models\n\n22. **highD Dataset**\n    - https://www.highd-dataset.com/\n    - Drone-recorded highway trajectories\n    - 110,000+ vehicle trajectories\n\n23. **Argoverse**\n    - https://www.argoverse.org/\n    - Motion forecasting benchmark\n    - HD maps + trajectory data\n\n24. **nuScenes**\n    - https://www.nuscenes.org/\n    - Prediction challenge\n    - Multi-modal sensor data\n\n### Software & Tools\n\n25. **SUMO** - Simulation of Urban MObility\n    - https://www.eclipse.org/sumo/\n    - Open-source traffic simulator\n    - Supports multi-agent scenarios\n\n26. **CARLA** - Open-source simulator for autonomous driving\n    - http://carla.org/\n    - Realistic environments\n    - RL integration with OpenAI Gym\n\n27. **Highway-Env**\n    - https://github.com/eleurent/highway-env\n    - Lightweight driving environments for RL\n    - Fast prototyping\n\n28. **SMARTS** - Scalable Multi-Agent RL Training School\n    - https://github.com/huawei-noah/SMARTS\n    - Multi-agent driving scenarios\n    - Diverse agent policies\n\n### Courses\n\n29. **Stanford CS238: Decision Making under Uncertainty**\n    - https://web.stanford.edu/class/aa228/\n    - MDPs, POMDPs, game theory\n\n30. **Berkeley CS285: Deep Reinforcement Learning**\n    - http://rail.eecs.berkeley.edu/deeprlcourse/\n    - Modern deep RL techniques\n    - Policy gradients, Q-learning, actor-critic"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}